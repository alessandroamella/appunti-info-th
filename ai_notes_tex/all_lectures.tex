% =========================================================================
% ===                COMMON PREAMBLE FOR ALL LECTURES                   ===
% =========================================================================

\documentclass[a4paper, 11pt]{book} % Use 'book' class for chapters

% --- Core Packages ---
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\usepackage[a4paper, total={6in, 8in}]{geometry}

% --- Fonts and Appearance ---
\usepackage{mathpazo} % For a nicer font
\usepackage{fancyhdr} % For custom headers and footers

% --- Special Content Packages ---
\usepackage{minted} % For pseudocode. Requires --shell-escape compiler flag
\usepackage{amsthm}   % For theorem environments
\usepackage{tikz}     % For diagrams
\usepackage[shortlabels]{enumitem} % For customized lists with backward compatibility
\usepackage{subcaption} % For subfigures
\usepackage{float}    % For figure placement [H]

% --- Enumitem Configuration ---
% Set up enumitem to handle standard enumerate environments properly
\setlist[enumerate]{label=\arabic*.}

% --- Hyperlinking ---
\usepackage{hyperref}

% --- TikZ Libraries (superset of all used libraries) ---
\usetikzlibrary{
    automata,
    positioning,
    arrows,
    arrows.meta,
    calc,
    fit,
    patterns,
    graphs,
    graphdrawing,
    quotes,
    shapes.geometric
}

% --- Theorem-like Environments (numbered within sections for consistency) ---
\newtheorem{theorem}{Teorema}[section]
\newtheorem{definition}[theorem]{Definizione}
\newtheorem{example}[theorem]{Esempio}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposizione}
\newtheorem{corollary}[theorem]{Corollario}
\newtheorem{remark}[theorem]{Osservazione}
\newtheorem{proof_sketch}[theorem]{Sketch di Dimostrazione}

% --- Custom Proof Environment ---
\theoremstyle{definition}
\newtheorem*{proof*}{Dimostrazione}

% --- Custom Commands (from various lectures) ---
% For Turing Machine symbols (from lecture 9/10)
\newcommand{\B}{\text{B}} % Blank symbol
\newcommand{\SigmaI}{\Sigma_I} % Input Alphabet
\newcommand{\GammaT}{\Gamma} % Tape Alphabet
\newcommand{\alphaSym}{\alpha} % Generic symbol from input alphabet (non-#)

% For TikZ diagrams (from lecture 27)
\tikzset{
    green_arrow/.style={-Latex, thick, draw=green!70!black},
    red_arrow/.style={-Latex, thick, draw=red!70!black},
    black_arrow/.style={-Latex, thick, draw=black},
    double_black_arrow/.style={-Latex, thick, draw=black, double},
    node_style/.style={circle, draw, fill=white, inner sep=1pt},
    small_node_style/.style={circle, draw, fill=lightgray, inner sep=0.5pt},
    diamond_node_style/.style={diamond, draw, fill=white, inner sep=1pt},
    clause_node_style/.style={rectangle, draw, fill=white, inner sep=2pt},
}

% --- PDF Metadata ---
\hypersetup{
    pdftitle={Appunti Completi di Informatica Teorica},
    pdfauthor={Appunti da Trascrizione AI}
}

% --- Header and Footer Configuration ---
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{\textit{Informatica Teorica}} % Right on Odd, Left on Even
\fancyhead[RE,LO]{\nouppercase{\leftmark}} % Chapter name
\fancyfoot[C]{\thepage}

% =========================================================================
% ===                  END OF COMMON PREAMBLE                           ===
% =========================================================================

\title{Appunti Completi di Informatica Teorica}
\author{Consolidato da Script Python}
\date{\today}

\begin{document}
\frontmatter
\maketitle
\tableofcontents
\mainmatter

% =====================================================
% --- START LECTURE 02 ---
% =====================================================

\chapter{Il Problema dell'Arresto e Introduzione agli Automi}



\section{Introduzione e Riepilogo}

Questa lezione mira a consolidare i concetti di base della Teoria della Computazione e introdurre gli strumenti formali per la loro analisi. In particolare, approfondiremo il concetto di problema, dimostreremo l'indecidibilità del Problema dell'Arresto e introdurremo gli Automi a Stati Finiti Determinici (DFA) come modello per lo studio dei linguaggi.

\subsection{Richiamo: Definizione di Problema}

Per noi, un problema è una relazione tra un input e un output. Più precisamente:

\begin{definition}[Problema]
Un problema è un sottoinsieme di tutte le possibili coppie formate da una stringa di input e una stringa di output.
Formalmente, un problema è una relazione $R \subseteq \Sigma^* \times \Gamma^*$, dove $\Sigma$ è l'alfabeto degli input e $\Gamma$ è l'alfabeto degli output.
\end{definition}

Quando si analizza un problema, è fondamentale chiarire tre elementi:
\begin{enumerate}
    \item \textbf{Input}: Cosa è l'input? Come viene rappresentato? Qual è il suo significato?
    \item \textbf{Output}: Cosa è l'output? Come viene rappresentato? Qual è il suo significato?
    \item \textbf{Relazione}: Qual è la relazione che lega l'output all'input? Data una specifica istanza di input, quale output ci si aspetta?
\end{enumerate}
Questo processo di chiarificazione è cruciale per evitare ambiguità e per una corretta formulazione del problema.

\section{Il Problema dell'Arresto (Halting Problem)}

\subsection{Definizione Informale}
Il Problema dell'Arresto chiede: "Dato un programma e un input per quel programma, il programma si arresterà (terminerà) o continuerà a girare indefinitamente su quell'input?"

\subsection{Definizione Formale}
Applichiamo la struttura di analisi del problema:

\begin{definition}[Problema dell'Arresto (HALT)]
\textbf{Input}: Una coppia $\langle P, I \rangle$, dove:
\begin{itemize}
    \item $P$ è una stringa che codifica un programma (e.g., codice Python).
    \item $I$ è una stringa che rappresenta l'input per il programma $P$.
\end{itemize}
\textbf{Output}: Una risposta booleana: "sì" o "no".
\textbf{Relazione}: L'output è "sì" se il programma $P$, eseguito con l'input $I$, si arresta. L'output è "no" se il programma $P$, eseguito con l'input $I$, non si arresta (cioè, entra in un loop infinito o non termina in tempo finito).
\end{definition}

Questo problema ha un'enorme utilità pratica per i programmatori, in quanto un compilatore o un ambiente di sviluppo che potesse prevedere la terminazione di un programma sarebbe uno strumento diagnostico potentissimo.

\subsection{Dimostrazione dell'Indecidibilità}
Dimostreremo che il Problema dell'Arresto è \textbf{indecidibile}, ovvero non esiste un algoritmo universale che possa risolvere correttamente il problema per \emph{ogni} possibile coppia $\langle P, I \rangle$. La dimostrazione è per assurdo.

\begin{theorem}[Indecidibilità del Problema dell'Arresto]
Non esiste un algoritmo che, per ogni data coppia $\langle P, I \rangle$ (programma $P$ e input $I$), sia in grado di determinare correttamente se $P$ si arresta su $I$.
\end{theorem}

\begin{proof}[Dimostrazione per Assurdo]
Assumiamo per assurdo che esista una procedura (un algoritmo) perfetta per risolvere il Problema dell'Arresto. Chiamiamo questa procedura \texttt{halt\_checker}.
Questa procedura \texttt{halt\_checker} prende in input due stringhe: una che rappresenta un programma $P$ e un'altra che rappresenta un input $I$ per $P$. Restituisce un valore booleano (\texttt{True} se $P$ si arresta su $I$, \texttt{False} altrimenti).

\begin{minted}[frame=lines,framesep=2mm,linenos]{python}
# Assunzione: questa funzione "magica" esiste
def halt_checker(P_code: str, I_input: str) -> bool:
    """
    Restituisce True se il programma P_code si arresta con I_input,
    False altrimenti.
    """
    # La sua implementazione è sconosciuta, ma si assume che funzioni perfettamente
    pass
\end{minted}

Ora, useremo questa \texttt{halt\_checker} per costruire un nuovo programma, che chiameremo \texttt{reverse}. Il programma \texttt{reverse} prende in input una singola stringa, che rappresenta il codice di un programma $P$.

\begin{minted}[frame=lines,framesep=2mm,linenos]{python}
def reverse(P_code: str):
    """
    Questo programma prende in input il codice di un altro programma P.
    """
    # Chiama halt_checker passando P_code sia come programma
    # che come input per sé stesso
    halts = halt_checker(P_code, P_code)

    if halts:
        # Se halt_checker ha detto che P_code si arresta su P_code,
        # allora reverse entra in un loop infinito.
        while True:
            pass # Loop infinito
    else:
        # Se halt_checker ha detto che P_code non si arresta su P_code,
        # allora reverse si arresta immediatamente.
        pass # Termina
\end{minted}

Questo programma \texttt{reverse} è un programma Python legittimo. Può essere compilato ed eseguito.
Sia \texttt{code\_reverse} la stringa che rappresenta il codice del programma \texttt{reverse} stesso.

Consideriamo ora l'esecuzione del programma \texttt{reverse} avendo come input il proprio codice, cioè \texttt{reverse(code\_reverse)}.
Ci sono due possibilità per l'esecuzione di \texttt{reverse(code\_reverse)}:

\begin{enumerate}
    \item \textbf{Caso 1: \texttt{reverse(code\_reverse)} si arresta.}
    \begin{itemize}
        \item Se \texttt{reverse(code\_reverse)} si arresta, significa che l'esecuzione è giunta alla linea \texttt{pass} (termina).
        \item Questo accade se e solo se la condizione \texttt{if halts:} era falsa, cioè \texttt{halts} era \texttt{False}.
        \item Il valore di \texttt{halts} è determinato dalla chiamata \texttt{halt\_checker(code\_reverse, code\_reverse)}.
        \item Quindi, \texttt{halt\_checker(code\_reverse, code\_reverse)} deve aver restituito \texttt{False}.
        \item Ma per definizione di \texttt{halt\_checker}, se restituisce \texttt{False}, significa che il programma \texttt{code\_reverse} (cioè \texttt{reverse}) \textbf{non si arresta} quando esegue con input \texttt{code\_reverse}.
        \item Abbiamo una contraddizione: abbiamo assunto che \texttt{reverse(code\_reverse)} si arresta, ma la logica interna del programma e l'assunzione di \texttt{halt\_checker} implicano che \textbf{non si arresta}.
    \end{itemize}

    \item \textbf{Caso 2: \texttt{reverse(code\_reverse)} non si arresta (entra in loop).}
    \begin{itemize}
        \item Se \texttt{reverse(code\_reverse)} non si arresta, significa che l'esecuzione è giunta al \texttt{while True: pass} (loop infinito).
        \item Questo accade se e solo se la condizione \texttt{if halts:} era vera, cioè \texttt{halts} era \texttt{True}.
        \item Il valore di \texttt{halts} è determinato dalla chiamata \texttt{halt\_checker(code\_reverse, code\_reverse)}.
        \item Quindi, \texttt{halt\_checker(code\_reverse, code\_reverse)} deve aver restituito \texttt{True}.
        \item Ma per definizione di \texttt{halt\_checker}, se restituisce \texttt{True}, significa che il programma \texttt{code\_reverse} (cioè \texttt{reverse}) \textbf{si arresta} quando esegue con input \texttt{code\_reverse}.
        \item Abbiamo una contraddizione: abbiamo assunto che \texttt{reverse(code\_reverse)} non si arresta, ma la logica interna del programma e l'assunzione di \texttt{halt\_checker} implicano che \textbf{si arresta}.
    \end{itemize}
\end{enumerate}

In entrambi i casi possibili (si arresta o non si arresta), giungiamo a una contraddizione logica. Poiché la costruzione del programma \texttt{reverse} e la sua esecuzione sono perfettamente lecite secondo le regole della computazione, l'unica conclusione possibile è che l'assunzione iniziale fosse falsa.

Pertanto, la procedura \texttt{halt\_checker} \textbf{non può esistere}.
\end{proof}

\subsubsection{Implicazioni della Dimostrazione}
\begin{itemize}
    \item \textbf{Indipendenza dalla Tecnologia}: La dimostrazione non fa alcuna assunzione specifica sul linguaggio di programmazione utilizzato (Python è solo un esempio) o sull'architettura hardware su cui il programma viene eseguito. L'unico requisito è che il programma $P$ possa essere codificato come una stringa e che possa prendere un input, e che la procedura \texttt{halt\_checker} possa essere invocata. Questo rende il risultato universale: il Problema dell'Arresto è indecidibile in qualsiasi modello di computazione "abbastanza potente" da poter implementare un \texttt{halt\_checker} e un \texttt{reverse} (come la Macchina di Turing, che vedremo in seguito).
    \item \textbf{Non Esistenza di Soluzioni Perfette}: Ciò non significa che non si possano scrivere programmi che, in alcuni casi specifici o per determinate classi di input, riescano a determinare la terminazione. Significa che non esiste un algoritmo \emph{generale} e \emph{perfetto} che per \emph{tutti} gli input dia sempre la risposta corretta in un tempo finito. Possiamo avere soluzioni approssimate o euristiche, ma non una soluzione infallibile per ogni caso.
\end{itemize}

\section{Classificazione dei Problemi: Decisione vs. Ricerca}

Abbiamo visto che i problemi possono essere di diversa natura. È utile categorizzarli per facilitarne lo studio.

\begin{example}[Problemi di Grafo]
\begin{enumerate}
    \item \textbf{Problema del Percorso (PATH)}:
    \begin{itemize}
        \item \textbf{Input}: Un grafo $G$, un nodo sorgente $S$, un nodo destinazione $T$.
        \item \textbf{Output}: Un percorso da $S$ a $T$ in $G$.
    \end{itemize}
    \item \textbf{Problema del Ciclo Hamiltoniano (HAMILTONIAN CYCLE)}:
    \begin{itemize}
        \item \textbf{Input}: Un grafo $G$.
        \item \textbf{Output}: "Sì" se esiste un ciclo Hamiltoniano (un ciclo che visita ogni nodo esattamente una volta) in $G$, "No" altrimenti.
    \end{itemize}
\end{enumerate}
\end{example}

Notiamo una differenza fondamentale nell'output:

\begin{definition}[Problema di Ricerca]
Un problema di ricerca è un problema il cui output può essere una stringa arbitraria, come un percorso, un numero, una derivata, una matrice risultante, ecc. L'obiettivo è trovare o calcolare qualcosa.
\end{definition}

\begin{definition}[Problema di Decisione]
Un problema di decisione è un problema il cui output è sempre una risposta booleana: "sì" o "no". L'obiettivo è decidere se una certa proprietà è vera o falsa per l'input dato.
\end{definition}

\subsubsection{Relazione tra Problemi di Ricerca e Problemi di Decisione}
Queste due classi non sono disgiunte. Per ogni problema di ricerca, possiamo definire un problema di decisione correlato.

\begin{example}[PATH come Problema di Decisione]
Consideriamo il problema del percorso come problema di decisione:
\begin{itemize}
    \item \textbf{Input}: Un grafo $G$, un nodo sorgente $S$, un nodo destinazione $T$.
    \item \textbf{Output}: "Sì" se esiste un percorso da $S$ a $T$ in $G$, "No" altrimenti.
\end{itemize}
\end{example}

È evidente che se siamo in grado di risolvere il problema di ricerca (trovare un percorso), siamo automaticamente in grado di risolvere il problema di decisione (sapere se un percorso esiste). Spesso, risolvere il problema di decisione è concettualmente più semplice, poiché richiede solo una risposta binaria. Per questa ragione, nella teoria della computazione, ci si concentra principalmente sui problemi di decisione.

\section{Problemi di Decisione e Linguaggi Formali}

Sebbene i problemi di decisione siano più semplici dei problemi di ricerca, possono comunque avere input di forme molto diverse (grafi, matrici, numeri, ecc.). Per standardizzare lo studio e semplificare l'analisi degli algoritmi, si ricorre al concetto di \textbf{linguaggio formale}.

\subsection{Definizioni Fondamentali}

\begin{definition}[Alfabeto ($\Sigma$)]
Un alfabeto $\Sigma$ è un insieme finito e non vuoto di simboli.
\end{definition}
\begin{example}
$\Sigma = \{a, b, c\}$ \\
$\Sigma = \{0, 1\}$ \\
$\Sigma = \{\text{quadratino}, \text{cerchietto}\}$
\end{example}

\begin{definition}[Parola o Stringa]
Una parola (o stringa) su un alfabeto $\Sigma$ è una sequenza finita di zero o più simboli tratti da $\Sigma$.
\end{definition}
\begin{example}
Se $\Sigma = \{a, b\}$, allora $ababa$, $a$, $b$, $\epsilon$ (stringa vuota) sono parole su $\Sigma$.
\end{example}

\begin{definition}[$\Sigma^*$]
$\Sigma^*$ denota l'insieme di tutte le possibili parole (di qualsiasi lunghezza, inclusa la stringa vuota $\epsilon$) che possono essere formate usando i simboli dell'alfabeto $\Sigma$.
\end{definition}

\begin{definition}[Linguaggio ($L$)]
Un linguaggio $L$ su un alfabeto $\Sigma$ è un qualsiasi sottoinsieme di $\Sigma^*$.
Formalmente, $L \subseteq \Sigma^*$.
\end{definition}
\begin{example}
Se $\Sigma = \{0, 1\}$, il linguaggio di tutte le stringhe binarie che iniziano con '0' potrebbe essere $L = \{0, 00, 01, 000, 001, \ldots\}$.
\end{example}

\subsection{Decidere un Linguaggio}

\begin{definition}[Problema di Decisione di un Linguaggio]
Decidere un linguaggio $L$ significa risolvere il seguente problema:
\textbf{Input}: Una stringa $w \in \Sigma^*$.
\textbf{Output}: "Sì" o "No".
\textbf{Relazione}: L'output è "Sì" se $w \in L$. L'output è "No" se $w \notin L$.
\end{definition}
È importante notare che il linguaggio $L$ fa parte della definizione del problema, non dell'input. L'input è solo la stringa $w$.

\subsection{Conversione di Problemi di Decisione in Problemi di Linguaggio}
Ogni problema di decisione può essere ricondotto (o codificato) come un problema di decisione di un linguaggio. Questo si fa codificando le istanze del problema come stringhe.

\begin{example}[Codifica del Problema PATH come Linguaggio]
Riprendiamo il problema di decisione PATH: "Esiste un percorso da $S$ a $T$ in $G$?".
Possiamo definire un linguaggio $L_{PATH}$ su un opportuno alfabeto $\Sigma$ (ad esempio, simboli per nodi, archi, parentesi, virgole, ecc.) tale che:
\[L_{PATH} = \{ \langle G, S, T \rangle \mid G \text{ è un grafo, } S \text{ e } T \text{ sono nodi in } G, \text{ ed esiste un percorso da } S \text{ a } T \text{ in } G \}.\]
Dove $\langle G, S, T \rangle$ è una stringa che codifica la tripla $(G, S, T)$.
Decidere se una stringa $w$ appartiene a $L_{PATH}$ è equivalente a risolvere il problema di decisione PATH originale.
\end{example}
Questa riduzione ci permette di studiare tutti i problemi di decisione concentrandoci unicamente sulla classe dei problemi di riconoscere linguaggi. Questo è il passo che ci permette di definire modelli di calcolo astratti (gli automi) che non dipendono da specifici linguaggi di programmazione o architetture hardware.

\section{Introduzione agli Automi}

\subsection{Concetto Intuitivo di Automa}
Un automa è un dispositivo astratto (o una macchina) che ha un numero finito di "stati" o "modalità di funzionamento". L'automa riceve segnali o input e, in base al suo stato corrente e al segnale ricevuto, cambia stato e/o produce un output.

\subsection{Esempio: Automa di un Lettore CD}
Consideriamo il comportamento di un lettore CD come un automa per capire il concetto.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
        node distance=2.5cm,
        every state/.style={minimum size=1cm},
        initial text={},
        shorten >=1pt,
        auto
    ]
        % Stati
        \node[state, initial] (standby) {StandBy};
        \node[state, right of=standby, xshift=2cm] (testcd) {Test CD};
        \node[state, above right of=testcd, yshift=0.5cm] (empty_stopped) {Empty Stopped};
        \node[state, below right of=testcd, yshift=-0.5cm] (cd_in_stopped) {CD in Stopped};
        \node[state, above right of=empty_stopped, xshift=2cm] (opened) {Opened};
        \node[state, below right of=cd_in_stopped, yshift=-0.5cm] (playing) {Playing};
        \node[state, right of=playing, xshift=2cm] (paused) {Paused};

        % Transizioni
        \path (standby) edge[bend left=10] node {PWR} (testcd);
        \path (testcd) edge node {No} (empty_stopped);
        \path (testcd) edge node {Yes} (cd_in_stopped);

        % Power off from various states
        \path (empty_stopped) edge[bend left=10] node {PWR} (standby);
        \path (cd_in_stopped) edge[bend left=10] node {PWR} (standby);
        \path (testcd) edge[bend left=20] node {PWR} (standby); % Assuming PWR can turn it off from test state too

        % Eject
        \path (empty_stopped) edge node {EJECT} (opened);
        \path (cd_in_stopped) edge node {EJECT} (opened);
        \path (opened) edge node {EJECT} (testcd); % Closing tray leads to re-test

        % Play, Pause, Stop
        \path (cd_in_stopped) edge[bend left=10] node {PLAY} (playing);
        \path (playing) edge node {PAUSE} (paused);
        \path (paused) edge node {PAUSE} (playing);
        \path (playing) edge[bend right=10] node {STOP} (cd_in_stopped);
        \path (paused) edge[bend left=10] node {STOP} (cd_in_stopped);
    \end{tikzpicture}
    \caption{Automa di un Lettore CD}
\end{figure}

\subsubsection{Spiegazione dell'Automa del Lettore CD}
\begin{itemize}
    \item \textbf{Stati (nodi)}: Rappresentano le diverse modalità operative del lettore (e.g., \texttt{StandBy}, \texttt{Playing}, \texttt{Paused}). Il pallino pieno indica lo stato iniziale (\texttt{StandBy}).
    \item \textbf{Transizioni (frecce)}: Rappresentano il passaggio da uno stato all'altro in risposta a un input (e.g., \texttt{PWR}, \texttt{EJECT}, \texttt{PLAY}). Le etichette sulle frecce indicano l'input che provoca la transizione.
    \item \textbf{Input/Segnali}: Azioni dell'utente (pressione di tasti) o sensori (rilevamento CD).
\end{itemize}
Questo esempio illustra come un automa modellizzi un sistema: lo stato in cui si trova il sistema influenzerà come reagirà al prossimo input.

\subsection{Automi a Stati Finiti Determinici (DFA)}
Gli automi vengono usati in informatica teorica per riconoscere linguaggi formali. Un Automaton a Stati Finiti Determinico (DFA) è il modello più semplice.

\begin{definition}[Automa a Stati Finiti Determinico (DFA)]
Un DFA $M$ è una quintupla $(Q, \Sigma, \delta, q_0, F)$, dove:
\begin{itemize}
    \item $Q$ è un insieme finito di stati.
    \item $\Sigma$ è l'alfabeto di input.
    \item $\delta: Q \times \Sigma \to Q$ è la funzione di transizione, che per ogni stato e ogni simbolo di input determina in modo unico il prossimo stato.
    \item $q_0 \in Q$ è lo stato iniziale.
    \item $F \subseteq Q$ è l'insieme degli stati finali (o accettanti).
\end{itemize}
\end{definition}

Un DFA \textbf{accetta} una stringa se, partendo dallo stato iniziale e processando la stringa simbolo per simbolo, l'automa termina in uno stato accettante. Altrimenti, la stringa viene \textbf{rifiutata}.

\begin{example}[Linguaggio dei Numeri Binari Dispari]
Consideriamo il linguaggio $L_{dispari}$ delle stringhe binarie che rappresentano numeri dispari.
\begin{itemize}
    \item $\Sigma = \{0, 1\}$
    \item Una stringa binaria rappresenta un numero dispari se e solo se il suo ultimo simbolo è '1'. (Es: $1_2=1$, $11_2=3$, $101_2=5$; $0_2=0$, $10_2=2$, $100_2=4$). La stringa vuota $\epsilon$ non rappresenta un numero e non deve essere accettata.
\end{itemize}

Costruiamo un DFA per $L_{dispari}$:
\begin{itemize}
    \item $Q = \{Q_0, Q_1\}$
        \begin{itemize}
            \item $Q_0$: Rappresenta lo stato in cui la stringa letta finora indica un numero pari (o la stringa vuota, o un numero che termina con 0).
            \item $Q_1$: Rappresenta lo stato in cui la stringa letta finora indica un numero dispari (termina con 1).
        \end{itemize}
    \item $q_0 = Q_0$ (Inizialmente, il numero è considerato pari, o non abbiamo ancora letto nulla).
    \item $F = \{Q_1\}$ (Se la stringa termina mentre siamo in questo stato, è un numero dispari).
    \item Funzione di transizione $\delta$:
        \begin{itemize}
            \item $\delta(Q_0, 0) = Q_0$ (Se eravamo in stato 'pari' e leggiamo '0', rimaniamo 'pari').
            \item $\delta(Q_0, 1) = Q_1$ (Se eravamo in stato 'pari' e leggiamo '1', diventiamo 'dispari').
            \item $\delta(Q_1, 0) = Q_0$ (Se eravamo in stato 'dispari' e leggiamo '0', diventiamo 'pari').
            \item $\delta(Q_1, 1) = Q_1$ (Se eravamo in stato 'dispari' e leggiamo '1', rimaniamo 'dispari').
        \end{itemize}
\end{itemize}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
        node distance=2.5cm,
        every state/.style={minimum size=1cm},
        initial text={},
        accepting,
        auto
    ]
        % Stati
        \node[state, initial] (q0) {$Q_0$};
        \node[state, accepting, right of=q0] (q1) {$Q_1$};

        % Transizioni
        \path (q0) edge[loop above] node {0} (q0);
        \path (q0) edge node {1} (q1);
        \path (q1) edge node {0} (q0);
        \path (q1) edge[loop above] node {1} (q1);
    \end{tikzpicture}
    \caption{DFA per il Linguaggio dei Numeri Binari Dispari}
\end{figure}

\subsubsection{Esempio di Tracciamento}
Stringa di input: \texttt{101}
\begin{enumerate}
    \item Inizia in $Q_0$.
    \item Legge '1': $\delta(Q_0, 1) = Q_1$. Automa si sposta in $Q_1$.
    \item Legge '0': $\delta(Q_1, 0) = Q_0$. Automa si sposta in $Q_0$.
    \item Legge '1': $\delta(Q_0, 1) = Q_1$. Automa si sposta in $Q_1$.
    \item Stringa terminata. L'automa si trova in $Q_1$, che è uno stato accettante.
\end{enumerate}
Quindi, la stringa \texttt{101} (che rappresenta il numero 5, dispari) è accettata.

Stringa di input: \texttt{110}
\begin{enumerate}
    \item Inizia in $Q_0$.
    \item Legge '1': $\delta(Q_0, 1) = Q_1$. Automa si sposta in $Q_1$.
    \item Legge '1': $\delta(Q_1, 1) = Q_1$. Automa si sposta in $Q_1$.
    \item Legge '0': $\delta(Q_1, 0) = Q_0$. Automa si sposta in $Q_0$.
    \item Stringa terminata. L'automa si trova in $Q_0$, che non è uno stato accettante.
\end{enumerate}
Quindi, la stringa \texttt{110} (che rappresenta il numero 6, pari) è rifiutata.

Questo modello dimostra come un DFA possa "decidere" se una data stringa appartiene a un linguaggio specifico. I DFA sono un modello di calcolo molto semplice ma potente per la loro classe di problemi.
\end{example}


% =====================================================
% --- START LECTURE 03 ---
% =====================================================

\chapter{Automi a Stati Finiti}



\section{Introduzione e Recap}
Questa lezione riprende i concetti introdotti precedentemente, approfondendo la definizione formale di automi a stati finiti (deterministi e non-deterministici) e le loro capacità computazionali.

\subsection{Problemi di Ricerca e Problemi di Decisione}
In informatica teorica, siamo principalmente interessati ai \emph{problemi di ricerca}, che sono problemi generici in cui la risposta può essere qualsiasi valore valido (es. "Qual è il prodotto di $A \times B$?", "Qual è il percorso minimo tra due punti?").
Un sottoinsieme importante sono i \emph{problemi di decisione}, in cui la risposta possibile è solo "sì" o "no".
La relazione tra i due tipi è significativa: spesso, un problema di ricerca può essere trasformato in un problema di decisione correlato. Per l'analisi, ci focalizziamo sui problemi di decisione per la loro maggiore semplicità.

\subsection{Linguaggi e Decidibilità}
I problemi di decisione sono spesso legati alla \emph{decidibilità dei linguaggi}.
\begin{definition}[Linguaggio]
Un \emph{linguaggio} è semplicemente un insieme di stringhe su un dato alfabeto $\Sigma$.
\end{definition}

\begin{definition}[Decidere un Linguaggio]
\emph{Decidere un linguaggio $L$} significa, data una stringa $w$, stabilire se $w$ appartiene a $L$ o meno. Le risposte possibili sono "sì" (se $w \in L$) o "no" (se $w \notin L$). Questo è un problema di decisione.
\end{definition}

\noindent \textbf{Nota Importante}: Quando si decide un linguaggio $L$, l'input al problema è la \textbf{stringa $w$}, non il linguaggio $L$ stesso. Il linguaggio $L$ fa parte della definizione del problema.

\section{Automi a Stati Finiti Deterministici (DFA)}
Per formalizzare la decisione dei linguaggi, utilizziamo il concetto di \emph{automa}. Gli automi sono modelli di calcolo semplici e formalmente definibili.

\subsection{Intuizione e Funzionamento}
Un automa a stati finiti può essere immaginato come un computer molto semplice che riceve l'input su un "nastro" (come uno scontrino). La macchina legge un simbolo per volta dal nastro, si sposta in un nuovo stato basandosi sul simbolo letto e sul suo stato corrente, e il simbolo letto viene "consumato" (non può essere riletto). Non si torna mai indietro sul nastro.

\begin{example}[Riconoscimento di Numeri Dispari Binari]
Consideriamo il linguaggio delle stringhe binarie che codificano numeri dispari. Un automa per questo linguaggio necessita solo di guardare l'ultimo bit. Possiamo usare due stati: $Q_0$ (l'ultimo simbolo visto era 0, o il numero parziale è pari) e $Q_1$ (l'ultimo simbolo visto era 1, o il numero parziale è dispari). Lo stato iniziale è $Q_0$. $Q_1$ sarà lo stato accettante.
\begin{itemize}
    \item Se siamo in $Q_0$ e leggiamo $0$, rimaniamo in $Q_0$.
    \item Se siamo in $Q_0$ e leggiamo $1$, andiamo in $Q_1$.
    \item Se siamo in $Q_1$ e leggiamo $0$, andiamo in $Q_0$.
    \item Se siamo in $Q_1$ e leggiamo $1$, rimaniamo in $Q_1$.
\end{itemize}
\begin{center}
\begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
    \node[state,initial] (q0) {$Q_0$};
    \node[state,right of=q0] (q1) {$Q_1$};
    \path[->] (q0) edge [loop above] node {0} (q0)
              (q0) edge node {1} (q1)
              (q1) edge node {0} (q0)
              (q1) edge [loop above] node {1} (q1);
\end{tikzpicture}
\end{center}
\end{example}

\subsection{Definizione Formale di DFA}
\begin{definition}[Automa a Stati Finiti Deterministico (DFA)]
Un automa a stati finiti deterministico $D$ è una quintupla $( \Sigma, Q, q_0, F, \delta )$, dove:
\begin{itemize}
    \item $\Sigma$ è l'alfabeto di input (un insieme finito non vuoto di simboli).
    \item $Q$ è un insieme finito di stati.
    \item $q_0 \in Q$ è lo stato iniziale.
    \item $F \subseteq Q$ è l'insieme degli stati finali (o accettanti).
    \item $\delta: Q \times \Sigma \to Q$ è la funzione di transizione. Per ogni coppia (stato corrente, simbolo letto), $\delta$ determina \textbf{un solo} prossimo stato.
\end{itemize}
\end{definition}
Il numero di stati in $Q$ è fisso e non può cambiare durante la computazione.

\subsection{Computazione di un DFA}
Per capire il funzionamento di un automa, modelliamo i suoi passi attraverso il concetto di "configurazione".

\begin{definition}[Configurazione]
Una \emph{configurazione} di un DFA è una coppia $(q, w)$, dove:
\begin{itemize}
    \item $q \in Q$ è lo stato corrente in cui si trova l'automa.
    \item $w \in \Sigma^*$ è la porzione di stringa di input che deve ancora essere letta.
\end{itemize}
Una configurazione rappresenta uno "snapshot" dello stato di avanzamento della computazione.
\end{definition}

\begin{definition}[Computazione Parziale]
Una \emph{computazione parziale} di un DFA $D$ su una stringa $w = c_1 c_2 \dots c_n$ è una sequenza di $m+1$ configurazioni:
\[ (R_0, c_1 \dots c_n) \xrightarrow{} (R_1, c_2 \dots c_n) \xrightarrow{} \dots \xrightarrow{} (R_m, c_{m+1} \dots c_n) \]
tale che:
\begin{itemize}
    \item $R_0 = q_0$ (la computazione inizia dallo stato iniziale con l'intera stringa in input).
    \item Per ogni $0 \leq i < m$, si ha $R_{i+1} = \delta(R_i, c_{i+1})$.
\end{itemize}
Il simbolo $c_{i+1}$ viene letto e consumato, e l'automa si sposta nello stato $R_{i+1}$.
\end{definition}

\begin{example}[Traccia di Computazione per $001$]
Usiamo il DFA per numeri dispari e la stringa $w = 001$.
\begin{itemize}
    \item Passo 0: $(Q_0, 001)$ \quad (Configurazione iniziale)
    \item Passo 1: $(Q_0, 01)$ \quad ($\delta(Q_0, 0) = Q_0$)
    \item Passo 2: $(Q_0, 1)$ \quad ($\delta(Q_0, 0) = Q_0$)
    \item Passo 3: $(Q_1, \epsilon)$ \quad ($\delta(Q_0, 1) = Q_1$)
\end{itemize}
L'ultima configurazione è $(Q_1, \epsilon)$.
\end{example}

\begin{definition}[Computazione Completa]
Una \emph{computazione completa} (o semplicemente \emph{computazione}) è una computazione parziale che è massimale, cioè non può essere estesa ulteriormente. Questo si verifica in due casi:
\begin{itemize}
    \item La stringa di input è stata completamente consumata (la porzione residua è $\epsilon$).
    \item Non esiste una transizione definita per lo stato corrente e il simbolo da leggere (l'automa si "blocca").
\end{itemize}
\end{definition}

\begin{definition}[Accettazione di una Stringa (DFA)]
Un DFA $D$ \emph{accetta} una stringa $w$ se la sua unica computazione completa, che inizia dalla configurazione $(q_0, w)$, termina in una configurazione $(q_f, \epsilon)$ tale che $q_f \in F$.
In altre parole, due condizioni devono essere soddisfatte:
\begin{enumerate}
    \item L'intera stringa di input $w$ deve essere consumata.
    \item Lo stato finale raggiunto dall'automa deve essere uno stato accettante ($q_f \in F$).
\end{enumerate}
\end{definition}

\begin{definition}[Rifiuto di una Stringa (DFA)]
Un DFA $D$ \emph{rifiuta} una stringa $w$ se la computazione su $w$ non è accettante. Ciò può avvenire se:
\begin{itemize}
    \item L'automa termina in uno stato $q \notin F$ pur avendo consumato tutto l'input.
    \item L'automa si blocca (non ha transizioni definite) prima di aver consumato tutto l'input.
\end{itemize}
\end{definition}

\begin{definition}[DFA che Decide un Linguaggio]
Un DFA $D$ \emph{decide} un linguaggio $L$ se per ogni stringa $w \in L$, $D$ accetta $w$, e per ogni stringa $w \notin L$, $D$ rifiuta $w$.
\end{definition}

\section{Automi a Stati Finiti Non-Deterministici (NFA)}
Gli automi non-deterministici introducono il concetto di scelte multiple durante la computazione.

\subsection{Introduzione al Non-Determinismo}
Consideriamo il linguaggio delle stringhe binarie che terminano con $000$, espresso dalla espressione regolare $(0|1)^*000$.

\begin{example}[NFA per $(0|1)^*000$]
Un tentativo intuitivo di DFA potrebbe essere:
\begin{itemize}
    \item $Q_0$: Stato iniziale e per leggere $(0|1)^*$.
    \item $Q_1$: Dopo aver letto il primo $0$ dei $000$ finali.
    \item $Q_2$: Dopo aver letto il secondo $0$ dei $000$ finali.
    \item $Q_3$: Stato finale, dopo aver letto il terzo $0$ dei $000$ finali.
\end{itemize}
Le transizioni sarebbero:
\begin{itemize}
    \item Da $Q_0$:
        \begin{itemize}
            \item Se legge $0$: può rimanere in $Q_0$ (per $(0|1)^*$) \emph{oppure} andare in $Q_1$ (se è il primo $0$ della sequenza finale). Questo è un punto di non-determinismo.
            \item Se legge $1$: rimane in $Q_0$.
        \end{itemize}
    \item Da $Q_1$:
        \begin{itemize}
            \item Se legge $0$: va in $Q_2$.
            \item Se legge $1$: torna in $Q_0$ (la sequenza $000$ è interrotta).
        \end{itemize}
    \item Da $Q_2$:
        \begin{itemize}
            \item Se legge $0$: va in $Q_3$.
            \item Se legge $1$: torna in $Q_0$.
        \end{itemize}
    \item Da $Q_3$:
        \begin{itemize}
            \item Se legge $0$: rimane in $Q_3$ (ma in realtà non è così semplice per $(0|1)^*000$ per via dei $000$ finali). Per la semplicità dell'esempio del professore, si può immaginare che da $Q_3$ si accettino solo stringhe che finiscono esattamente con $000$ e nessun altro carattere dopo. Se la stringa è $010001$ dopo $Q_3$ dovrebbe andare in $Q_0$.
        \end{itemize}
\end{itemize}
Il problema evidenziato dal professore è il \emph{non-determinismo} in $Q_0$ quando legge $0$: può andare in $Q_0$ o $Q_1$.
\end{example}

\begin{definition}[Automa a Stati Finiti Non-Deterministico (NFA)]
Un automa a stati finiti non-deterministico $N$ è una quintupla $( \Sigma, Q, q_0, F, \delta )$, dove:
\begin{itemize}
    \item $\Sigma$, $Q$, $q_0$, $F$ sono definiti come per i DFA.
    \item $\delta: Q \times \Sigma \to \mathcal{P}(Q)$ è la funzione di transizione (o relazione di transizione). Per ogni coppia (stato corrente, simbolo letto), $\delta$ determina un \textbf{insieme} di possibili prossimi stati ($\mathcal{P}(Q)$ denota l'insieme delle parti di $Q$).
\end{itemize}
\end{definition}

\subsection{Computazione di un NFA}
A causa delle transizioni che possono portare a più stati, per un dato input un NFA può avere \emph{più computazioni differenti}.

\begin{definition}[Computazione Parziale per NFA]
Una \emph{computazione parziale} di un NFA $N$ su una stringa $w = c_1 c_2 \dots c_n$ è una sequenza di $m+1$ configurazioni:
\[ (R_0, c_1 \dots c_n) \xrightarrow{} (R_1, c_2 \dots c_n) \xrightarrow{} \dots \xrightarrow{} (R_m, c_{m+1} \dots c_n) \]
tale che:
\begin{itemize}
    \item $R_0 = q_0$ (la computazione inizia dallo stato iniziale con l'intera stringa in input).
    \item Per ogni $0 \leq i < m$, si ha $R_{i+1} \in \delta(R_i, c_{i+1})$. (Il prossimo stato è uno qualsiasi tra quelli ammessi dalla relazione di transizione).
\end{itemize}
\end{definition}

\begin{example}[Traccia di Computazione per $0100$ su NFA $(0|1)^*000$]
Consideriamo l'NFA precedente e la stringa $w = 0100$.
\begin{itemize}
    \item \textbf{Percorso Accettante (se esistesse Q3 come accettante):}
    \item $(Q_0, 0100)$
    \item $\xrightarrow{\text{leggi 0, transisci in } Q_0}$ $(Q_0, 100)$
    \item $\xrightarrow{\text{leggi 1, transisci in } Q_0}$ $(Q_0, 00)$
    \item $\xrightarrow{\text{leggi 0, transisci in } Q_1}$ $(Q_1, 0)$
    \item $\xrightarrow{\text{leggi 0, transisci in } Q_2}$ $(Q_2, \epsilon)$
    \item $\xrightarrow{\text{leggi } \epsilon \text{, transisci in } Q_3}$ $(Q_3, \epsilon)$
    Questo percorso, se $Q_3$ è accettante, porta all'accettazione.

    \item \textbf{Percorso Rifiutante:}
    \item $(Q_0, 0100)$
    \item $\xrightarrow{\text{leggi 0, transisci in } Q_1}$ $(Q_1, 100)$
    \item Ora, da $Q_1$ leggendo $1$, si torna in $Q_0$.
    \item $\xrightarrow{\text{leggi 1, transisci in } Q_0}$ $(Q_0, 00)$
    *A questo punto l'automa non è riuscito a finire la sequenza $000$ nel modo sperato, e dovrebbe continuare a processare per vedere se trova altri $000$.*
    *Esempio più semplice: se da $Q_1$ non ci fosse una transizione per '1', la macchina si bloccherebbe.*
\end{itemize}
\end{example}

\begin{definition}[Accettazione di una Stringa (NFA)]
Un NFA $N$ \emph{accetta} una stringa $w$ se \textbf{esiste almeno una} computazione completa per $w$ che sia accettante (ovvero, termina in uno stato finale $q_f \in F$ con input esaurito $\epsilon$).
\end{definition}
\noindent \textbf{Nota}: Un NFA rifiuta una stringa $w$ se \emph{tutte} le possibili computazioni per $w$ sono rifiutanti.

\subsubsection*{La Natura del Non-Determinismo}
È cruciale comprendere che gli NFA sono \emph{modelli astratti} di calcolo. Non implicano che una macchina fisica "provi tutte le strade contemporaneamente", "tiri a indovinare" o "faccia backtracking". La loro utilità risiede nella \textbf{semplicità di progettazione} e analisi concettuale.

\begin{theorem}[Equivalenza DFA-NFA]
Per ogni Automa a Stati Finiti Non-Deterministico (NFA), esiste un Automa a Stati Finiti Deterministico (DFA) equivalente che riconosce lo stesso linguaggio.
\end{theorem}
Questo teorema, sebbene non dimostrato qui, è fondamentale: significa che DFAs e NFAs hanno la stessa \emph{potenza espressiva}. Qualsiasi linguaggio riconoscibile da un NFA è riconoscibile anche da un DFA, e viceversa. Questo giustifica l'uso degli NFA per la loro semplicità di programmazione, sapendo che possono essere sempre convertiti in DFAs per l'implementazione pratica.

\section{Limitazioni degli Automi a Stati Finiti}
Nonostante la loro utilità, gli automi a stati finiti (sia deterministici che non-deterministici) non sono modelli di calcolo universali. Hanno delle limitazioni intrinseche.

\begin{example}[Linguaggio $L = \{a^m b^m \mid m \geq 0\}$]
Consideriamo il linguaggio $L$ sull'alfabeto $\Sigma = \{a, b\}$ definito come:
\[ L = \{a^m b^m \mid m \geq 0\} \]
Questo linguaggio include stringhe come $\epsilon$ (per $m=0$), $ab$, $aabb$, $aaabbb$, e così via. Ogni stringa in $L$ è composta da un certo numero di 'a' seguito esattamente dallo stesso numero di 'b'.
Un programma Python o Java saprebbe facilmente verificare se una stringa è di questo tipo, ad esempio contando il numero di 'a' e 'b'.
\end{example}

\subsection{Argomento Intuitivo (Pumping Lemma)}
Gli automi a stati finiti non sono in grado di riconoscere il linguaggio $L = \{a^m b^m \mid m \geq 0\}$. L'intuizione dietro questa limitazione è che gli automi a stati finiti \textbf{non sanno contare} in modo arbitrario e \textbf{non hanno memoria esterna} per ricordare conteggi indefiniti.

\begin{itemize}
    \item \textbf{Assunzione per contraddizione}: Supponiamo che esista un DFA $D$ in grado di decidere il linguaggio $L = \{a^m b^m \mid m \geq 0\}$.
    \item Sia $P$ il numero di stati del DFA $D$. $P$ è un numero finito e costante.
    \item \textbf{Consideriamo una stringa lunga}: Scegliamo una stringa $s = a^P b^P$. Questa stringa appartiene a $L$ ed ha lunghezza $2P$. Poiché $P \geq 1$ (assumendo almeno uno stato), $2P > P$.
    \item \textbf{Il Principio dei Cassetti (Pigeonhole Principle)}: Quando il DFA $D$ processa i primi $P$ simboli 'a' della stringa $s$, esso visiterà $P+1$ configurazioni (partendo da $q_0$ e leggendo $P$ 'a'). Poiché il DFA ha solo $P$ stati, per il principio dei cassetti, l'automa deve \textbf{per forza rivisitare almeno uno stato} durante il processo delle prime $P$ 'a'.
    \item Questo significa che esiste un \textbf{ciclo} nel percorso di computazione dell'automa mentre sta leggendo la porzione $a^P$ della stringa. Possiamo scomporre la stringa $s$ in tre parti: $s = uvw$, dove:
        \begin{itemize}
            \item $u$ è la parte iniziale della stringa prima che si entri nel ciclo (potrebbe essere vuota).
            \item $v$ è la porzione di stringa che corrisponde al ciclo (contiene solo 'a' e non è vuota).
            \item $w$ è la parte restante della stringa dopo aver completato il ciclo una volta, che include le 'a' rimanenti (se ci sono) e tutte le 'b'.
        \end{itemize}
        Graficamente: $q_0 \xrightarrow{u} q_i \xrightarrow{v} q_i \xrightarrow{w} q_f \in F$.
    \item \textbf{Implicazione del ciclo}: Se il DFA accetta $uvw$, dato che $v$ corrisponde a un ciclo, l'automa accetterà anche $uvvw$, $uvvvw$, e in generale $uv^i w$ per qualsiasi $i \geq 0$. Questo perché percorrere il ciclo ($v$) più volte riporta sempre l'automa nello stesso stato $q_i$, e da lì la computazione prosegue identica.
    \item \textbf{Contraddizione}: Poiché $v$ contiene solo 'a' (essendo parte dei primi $P$ simboli 'a'), se consideriamo la stringa $uv^2 w$, essa avrà più 'a' di $uvw$ ma lo stesso numero di 'b'. Ad esempio, se $s = a^P b^P$, allora $uv^2 w$ sarà della forma $a^{P+|v|} b^P$, dove $|v| > 0$. Questa nuova stringa \textbf{non appartiene} al linguaggio $L$ perché il numero di 'a' non è uguale al numero di 'b'.
    \item Tuttavia, per la proprietà dei cicli, il DFA $D$ dovrebbe accettare $uv^2 w$. Questo è una contraddizione, in quanto $D$ dovrebbe accettare solo stringhe in $L$ e rifiutare quelle non in $L$.
\end{itemize}
\textbf{Conclusione}: L'assunzione iniziale che un DFA possa decidere il linguaggio $L = \{a^m b^m \mid m \geq 0\}$ è falsa. Gli automi a stati finiti non hanno la capacità di "contare" in modo arbitrario e confrontare quantità.

Questa limitazione fondamentale degli automi a stati finiti ci spinge a cercare modelli di calcolo più potenti, in grado di svolgere compiti come il conteggio e la memorizzazione di informazioni arbitrarie. Il prossimo passo è l'introduzione delle \emph{Macchine di Turing}, che si dimostreranno essere un modello di calcolo universale, in grado di simulare qualsiasi algoritmo.


% =====================================================
% --- START LECTURE 05 ---
% =====================================================

\chapter{Macchine di Turing}



\section{Introduzione alle Macchine di Turing}

Ripassiamo brevemente i concetti fondamentali delle Macchine di Turing (MT) introdotti nella lezione precedente.

\begin{definition}[Macchina di Turing]
Una Macchina di Turing è un automa a stati finiti con le seguenti peculiarità:
\begin{itemize}
    \item Possiede un \textbf{nastro di input infinito} che può essere letto e scritto. A differenza degli automi a stati finiti che possono solo leggere l'input, le MT possono modificare il loro nastro.
    \item La \textbf{testina} della macchina può spostarsi sia a destra che a sinistra sul nastro. Gli automi a stati finiti, invece, si muovono solo a destra.
    \item Il \textbf{programma} (funzione di transizione) di una MT è fissato e non può cambiare durante l'esecuzione. Una volta che la macchina è "accesa" con un determinato programma, esso rimane invariato. Questo, pur sembrando una limitazione rispetto ai computer moderni, vedremo che non lo è, poiché le MT sono modelli di calcolo universali.
\end{itemize}
Queste caratteristiche rendono le Macchine di Turing un modello di calcolo universale, capace di simulare qualsiasi algoritmo computabile.
\end{definition}

\subsection{Linguaggi: Decidibilità vs. Accettazione}

È fondamentale distinguere due concetti chiave relativi ai linguaggi che una Macchina di Turing può riconoscere.

\begin{definition}[Macchina che Decide un Linguaggio]
Una Macchina di Turing \textbf{decide} un linguaggio $L$ se per ogni stringa $w$ in input:
\begin{itemize}
    \item Se $w \in L$, la macchina si ferma in uno stato accettante.
    \item Se $w \notin L$, la macchina si ferma in uno stato non accettante.
\end{itemize}
In sintesi, una macchina che decide un linguaggio è garantita fermarsi su ogni input, fornendo una risposta "sì" (accettazione) o "no" (rifiuto).
\end{definition}

\begin{definition}[Macchina che Accetta un Linguaggio]
Una Macchina di Turing \textbf{accetta} un linguaggio $L$ se:
\begin{itemize}
    \item Per ogni stringa $w \in L$, la macchina si ferma in uno stato accettante.
    \item Per ogni stringa $w \notin L$, la macchina non accetta $w$. Questo significa che la macchina può fermarsi in uno stato non accettante oppure non fermarsi affatto (loop infinito).
\end{itemize}
Le macchine che accettano linguaggi non danno una garanzia di terminazione per input che non fanno parte del linguaggio.

Per gli esercizi odierni, progetteremo macchine che \textbf{decidono} i linguaggi specificati.
\end{definition}

\section{Tecniche di Programmazione delle Macchine di Turing}

La programmazione delle Macchine di Turing richiede una visualizzazione chiara del loro funzionamento. Il suggerimento principale è "vedere il filmato" del movimento della testina e delle modifiche sul nastro nella propria mente, e poi tradurre questo filmato in una sequenza di transizioni di stato.

Un'altra tecnica importante è \textbf{evitare di cancellare i simboli di input} (sostituendoli con blank) se non strettamente necessario, specialmente per stringhe complesse. Questo può creare "buchi" sul nastro, rendendo difficile tenere traccia della posizione relativa della testina e delle parti rimanenti della stringa. È preferibile \textbf{marcare i simboli di input} con simboli speciali dell'alfabeto del nastro ($\Gamma$), mantenendo l'input compatto.

\subsection{Esercizio 1: Linguaggio dei Palindromi $L = \{ww^R \mid w \in \{0,1\}^*\}$}

Vogliamo progettare una MT che decide il linguaggio delle stringhe palindromiche con due parti $w$ e $w^R$ concatenate. L'alfabeto è $\{0,1\}$. Esempi: $\varepsilon$, $00$, $11$, $0110$, $1001$.

\begin{example}[Strategia]
La strategia consiste nel confrontare il primo carattere di $w$ con l'ultimo carattere di $w^R$, il secondo con il penultimo, e così via, fino a quando tutti i caratteri sono stati confrontati.
\begin{enumerate}
    \item Partire dal primo simbolo non marcato del nastro.
    \item Leggere il simbolo (es. $0$ o $1$), marcarlo (sostituirlo con un Blank $B$).
    \item Spostare la testina fino alla fine della stringa (trovando il primo $B$ a destra), quindi tornare indietro di un passo per posizionarsi sull'ultimo simbolo.
    \item Verificare che l'ultimo simbolo sia uguale a quello letto inizialmente. Se sì, marcarlo (sostituirlo con $B$).
    \item Spostare la testina all'inizio della parte non marcata della stringa.
    \item Ripetere il processo fino a quando tutti i simboli sono stati marcati.
    \item Se tutti i simboli sono stati marcati correttamente, la stringa è accettata.
\end{enumerate}
\end{example}

\begin{tikzpicture}[shorten >=1pt,node distance=2.5cm,on grid,auto]
    \node[state,initial] (q0) {$q_0$};
    \node[state] (q1) [right=of q0] {$q_1$};
    \node[state] (q2) [right=of q1] {$q_2$};
    \node[state] (q3) [below=of q2] {$q_3$};
    \node[state] (q4) [below=of q0] {$q_4$};
    \node[state] (q5) [below=of q4] {$q_5$};
    \node[state,accepting] (q6) [below=of q3] {$q_6$};

    \path[->]
    (q0) edge node {$0/B, R$} (q1)
         edge node {$1/B, R$} (q4)
         edge [bend left] node {$B/B, R$} (q6)
    (q1) edge [loop above] node {$0/0, R$} ()
         edge [loop below] node {$1/1, R$} ()
         edge node {$B/B, L$} (q2)
    (q2) edge node {$0/B, L$} (q3)
    (q3) edge [loop left] node {$0/0, L$} ()
         edge [loop above] node {$1/1, L$} ()
         edge node {$B/B, R$} (q0)
    (q4) edge [loop above] node {$0/0, R$} ()
         edge [loop below] node {$1/1, R$} ()
         edge node {$B/B, L$} (q5)
    (q5) edge node {$1/B, L$} (q3);
\end{tikzpicture}

\vspace{0.5cm}
\noindent \textbf{Spiegazione delle transizioni:}
\begin{itemize}
    \item \textbf{Stato $q_0$ (Iniziale):}
    \begin{itemize}
        \item Se legge $0$: scrive $B$ (blank), sposta a destra ($R$), va in $q_1$.
        \item Se legge $1$: scrive $B$, sposta a destra ($R$), va in $q_4$.
        \item Se legge $B$: scrive $B$, sposta a destra ($R$), va in $q_6$ (stato accettante). Questo accetta la stringa vuota.
    \end{itemize}
    \item \textbf{Stato $q_1$ (Cerca fine stringa dopo aver letto $0$):}
    \begin{itemize}
        \item Se legge $0$ o $1$: lascia il simbolo, sposta a destra ($R$), rimane in $q_1$. Salta tutti i caratteri.
        \item Se legge $B$: lascia $B$, sposta a sinistra ($L$), va in $q_2$. Questo indica che la testina ha raggiunto la fine della stringa e si posiziona sull'ultimo carattere.
    \end{itemize}
    \item \textbf{Stato $q_2$ (Confronta ultimo carattere per $0$):}
    \begin{itemize}
        \item Se legge $0$: scrive $B$, sposta a sinistra ($L$), va in $q_3$. Il $0$ iniziale è stato confrontato con successo.
        \item Se legge $1$ o $B$: la stringa non è del formato corretto (implica rifiuto non fermandosi in stato accettante).
    \end{itemize}
    \item \textbf{Stato $q_3$ (Torna all'inizio):}
    \begin{itemize}
        \item Se legge $0$ o $1$: lascia il simbolo, sposta a sinistra ($L$), rimane in $q_3$. Salta tutti i caratteri.
        \item Se legge $B$: lascia $B$, sposta a destra ($R$), va in $q_0$. Questo indica che la testina ha raggiunto l'inizio del nastro e si posiziona sul prossimo carattere non marcato.
    \end{itemize}
    \item \textbf{Stato $q_4$ (Cerca fine stringa dopo aver letto $1$):}
    \begin{itemize}
        \item Se legge $0$ o $1$: lascia il simbolo, sposta a destra ($R$), rimane in $q_4$. Salta tutti i caratteri.
        \item Se legge $B$: lascia $B$, sposta a sinistra ($L$), va in $q_5$. Raggiunto fine stringa, posiziona sull'ultimo.
    \end{itemize}
    \item \textbf{Stato $q_5$ (Confronta ultimo carattere per $1$):}
    \begin{itemize}
        \item Se legge $1$: scrive $B$, sposta a sinistra ($L$), va in $q_3$. L'$1$ iniziale è stato confrontato con successo.
        \item Se legge $0$ o $B$: la stringa non è del formato corretto (implica rifiuto).
    \end{itemize}
    \item \textbf{Stato $q_6$ (Accettante):} Se la macchina raggiunge questo stato, la stringa è accettata.
\end{itemize}
Questa MT decide il linguaggio, poiché su ogni input valido o meno, si fermerà in uno stato accettante o non accettante.

\subsection{Esercizio 2: Linguaggio $L = \{a^n b^m \mid m > n > 0\}$}

Vogliamo progettare una MT che decide il linguaggio di stringhe con sequenze di 'a' seguite da sequenze di 'b', dove il numero di 'b' è strettamente maggiore del numero di 'a', ed entrambi sono maggiori di $0$. Esempio: $aab^{3}$, $ab^{2}$.

\begin{example}[Strategia]
La strategia prevede di marcare un' 'a' e poi la prima 'b' corrispondente, ripetendo il processo. Alla fine, si verifica che tutte le 'a' siano state marcate e che ci siano 'b' non marcate rimaste.
\begin{enumerate}
    \item Marcare la prima 'a' non marcata (es. con 'X').
    \item Spostarsi a destra, saltando altre 'a' e 'Y' (b già marcate) fino a trovare la prima 'b' non marcata.
    \item Marcare questa 'b' (es. con 'Y').
    \item Spostarsi a sinistra, saltando 'a', 'b', 'Y' fino a trovare la 'X' più a destra.
    \item Spostarsi a destra di una posizione per trovare la prossima 'a' non marcata e ripetere.
    \item Una volta che tutte le 'a' sono state marcate, si controlla che ci siano 'b' non marcate rimaste. Se sì, la stringa è accettata.
\end{enumerate}
\end{example}

\noindent \textbf{Alfabeto del nastro $\Gamma = \{a, b, X, Y, B\}$.}
\noindent \textbf{Stati:} $q_0$ (iniziale), $q_1$ (marcata 'a', cerca 'b'), $q_2$ (salta 'Y' per trovare 'b'), $q_3$ (marcata 'b', torna indietro), $q_4$ (torna indietro per 'X'), $q_5$ (accettazione 'b' residue), $q_6$ (accettante).

\noindent \textbf{Transizioni:}
\begin{itemize}
    \item \textbf{Stato $q_0$ (Iniziale / Cerca 'a'):}
        \begin{itemize}
            \item $q_0 \xrightarrow{a / X, R} q_1$ (Marca la prima 'a' con 'X', si muove a destra)
            \item $q_0 \xrightarrow{Y / Y, R} q_5$ (Se trova una 'Y', significa che tutte le 'a' sono state marcate e si controllano le 'b' residue)
            \item Se $q_0$ legge $B$ (stringa vuota) o $b$ (stringa non valida, n > 0) $\to$ implicitamente rifiuta.
        \end{itemize}
    \item \textbf{Stato $q_1$ (Marca 'a', cerca 'b'):}
        \begin{itemize}
            \item $q_1 \xrightarrow{a / a, R} q_1$ (Salta altre 'a')
            \item $q_1 \xrightarrow{Y / Y, R} q_2$ (Salta 'b' già marcate con 'Y')
            \item $q_1 \xrightarrow{b / Y, L} q_3$ (Marca la prima 'b' con 'Y', va a sinistra. Questo è il caso in cui non ci sono 'Y' tra 'a' e 'b' non marcate)
        \end{itemize}
    \item \textbf{Stato $q_2$ (Salto 'Y' intermedie):}
        \begin{itemize}
            \item $q_2 \xrightarrow{Y / Y, R} q_2$ (Continua a saltare 'Y' già marcate)
            \item $q_2 \xrightarrow{b / Y, L} q_3$ (Marca la 'b' con 'Y', va a sinistra)
        \end{itemize}
    \item \textbf{Stato $q_3$ (Marca 'b', torna indietro):}
        \begin{itemize}
            \item $q_3 \xrightarrow{a / a, L} q_3$ (Salta 'a' mentre torna indietro)
            \item $q_3 \xrightarrow{Y / Y, L} q_3$ (Salta 'Y' mentre torna indietro)
            \item $q_3 \xrightarrow{X / X, R} q_0$ (Ha trovato l'X della 'a' iniziale. Sposta a destra per la prossima 'a' da marcare)
        \end{itemize}
    \item \textbf{Stato $q_5$ (Verifica 'b' residue):}
        \begin{itemize}
            \item $q_5 \xrightarrow{Y / Y, R} q_5$ (Salta 'Y' residue)
            \item $q_5 \xrightarrow{b / b, R} q_5$ (Salta 'b' residue non marcate - devono esistere per $m>n$)
            \item $q_5 \xrightarrow{B / B, R} q_6$ (Se trova $B$, significa che ci sono più 'b' delle 'a' ed è un blank. Stringa accettata)
        \end{itemize}
    \item \textbf{Stato $q_6$ (Accettante):} Stringa accettata.
\end{itemize}
Questa MT decide il linguaggio, rifiutando stringhe che non soddisfano le condizioni (es. $n \ge m$, ordine sbagliato dei caratteri, ecc.).

\subsection{Esercizio 3: Linguaggio $L = \{a^n b^m \mid n > 0, m = 2n\}$}

Vogliamo progettare una MT che decide il linguaggio di stringhe con sequenze di 'a' seguite da sequenze di 'b', dove il numero di 'b' è il doppio del numero di 'a', ed $n$ è maggiore di $0$. Esempio: $aab^{4}$, $ab^{2}$.

\begin{example}[Strategia]
Molto simile al precedente, ma per ogni 'a' marcata, si devono marcare due 'b'.
\begin{enumerate}
    \item Marcare la prima 'a' non marcata (con 'X').
    \item Spostarsi a destra, saltando 'a' e 'Y' fino a trovare la prima 'b' non marcata. Marcarla (con 'Y').
    \item Spostarsi ancora a destra, trovare la seconda 'b' non marcata. Marcarla (con 'Y').
    \item Spostarsi a sinistra, saltando tutti i simboli marcati e non, fino a trovare la 'X' più a destra.
    \item Spostarsi a destra di una posizione per trovare la prossima 'a' non marcata e ripetere.
    \item Una volta che tutte le 'a' sono state marcate, si controlla che tutte le 'b' siano state marcate. Se sì, la stringa è accettata.
\end{enumerate}
\end{example}

\noindent \textbf{Alfabeto del nastro $\Gamma = \{a, b, X, Y, B\}$.}
\noindent \textbf{Stati:} $q_0$ (iniziale), $q_1$ (marcata 'a', cerca prima 'b'), $q_2$ (marcata prima 'b', cerca seconda 'b'), $q_3$ (marcata seconda 'b', torna indietro), $q_4$ (torna indietro), $q_5$ (accettazione 'Y' residue), $q_6$ (accettante).

\noindent \textbf{Transizioni:}
\begin{itemize}
    \item \textbf{Stato $q_0$ (Iniziale / Cerca 'a'):}
        \begin{itemize}
            \item $q_0 \xrightarrow{a / X, R} q_1$ (Marca la prima 'a' con 'X')
            \item $q_0 \xrightarrow{Y / Y, R} q_5$ (Se trova 'Y', tutte le 'a' sono state marcate, si controllano solo 'b' marcate)
        \end{itemize}
    \item \textbf{Stato $q_1$ (Marca 'a', cerca prima 'b'):}
        \begin{itemize}
            \item $q_1 \xrightarrow{a / a, R} q_1$ (Salta 'a')
            \item $q_1 \xrightarrow{Y / Y, R} q_1$ (Salta 'Y' già marcate)
            \item $q_1 \xrightarrow{b / Y, R} q_2$ (Marca la prima 'b' con 'Y', si muove a destra per cercare la seconda 'b')
        \end{itemize}
    \item \textbf{Stato $q_2$ (Cerca seconda 'b'):}
        \begin{itemize}
            \item $q_2 \xrightarrow{b / Y, L} q_3$ (Marca la seconda 'b' con 'Y', si muove a sinistra per tornare)
        \end{itemize}
    \item \textbf{Stato $q_3$ (Marca seconda 'b', torna indietro):}
        \begin{itemize}
            \item $q_3 \xrightarrow{a / a, L} q_3$ (Salta 'a')
            \item $q_3 \xrightarrow{Y / Y, L} q_3$ (Salta 'Y')
            \item $q_3 \xrightarrow{X / X, R} q_0$ (Trovato 'X', sposta a destra per la prossima 'a')
        \end{itemize}
    \item \textbf{Stato $q_5$ (Verifica 'b' residue):}
        \begin{itemize}
            \item $q_5 \xrightarrow{Y / Y, R} q_5$ (Salta 'Y' residue)
            \item $q_5 \xrightarrow{B / B, R} q_6$ (Se trova $B$, tutte le 'b' sono state marcate correttamente. Accetta)
        \end{itemize}
    \item \textbf{Stato $q_6$ (Accettante):} Stringa accettata.
\end{itemize}
Condizioni di rifiuto implicite: se non si trovano due 'b' per ogni 'a', o se l'ordine dei simboli non è 'a' poi 'b'.

\subsection{Esercizio 4: Linguaggio $L = \{w\#w \mid w \in \{a,b\}^+\}$}

Vogliamo progettare una MT che decide il linguaggio di stringhe composte da una sequenza $w$, seguita da un separatore '\#', e poi dalla stessa sequenza $w$. $w$ deve avere almeno un simbolo. Esempi: $a\#a$, $ab\#ab$, $baba\#baba$.

\begin{example}[Strategia]
La strategia si basa sul confronto carattere per carattere delle due metà della stringa, usando il '\#' come punto di riferimento.
\begin{enumerate}
    \item Marcare il primo simbolo non marcato della prima $w$ (es. 'a' o 'b' con 'X').
    \item Spostarsi a destra, saltando tutti i simboli intermedi, fino a trovare il '\#'.
    \item Superare il '\#', e poi saltare i simboli già marcati ('X') nella seconda $w$.
    \item Trovare il simbolo corrispondente nella seconda $w$ (deve essere uguale a quello marcato nel passo 1), marcarlo (con 'X').
    \item Spostarsi a sinistra, saltando tutti i simboli intermedi, fino a trovare il '\#'.
    \item Superare il '\#', e poi saltare i simboli già marcati ('X') nella prima $w$.
    \item Posizionarsi sul prossimo simbolo non marcato nella prima $w$ e ripetere il ciclo.
    \item Se tutti i simboli sono stati marcati correttamente, la stringa è accettata.
\end{enumerate}
\end{example}

\noindent \textbf{Alfabeto del nastro $\Gamma = \{a, b, \#, X, B\}$.}
\noindent \textbf{Stati:} $q_0$ (iniziale), $q_1$ (marcata 'a', cerca match), $q_2$ (dopo '\#', salta 'X', cerca 'a'), $q_3$ (match trovato, torna indietro), $q_4$ (dopo '\#', torna indietro a 'X'), $q_5$ (check finale 'X' prima di '\#'), $q_6$ (marcata 'b', cerca match), $q_7$ (dopo '\#', salta 'X', cerca 'b'), $q_8$ (dopo '\#', check finale 'X' dopo '\#'), $q_9$ (accettante).

\noindent \textbf{Transizioni:}
\begin{itemize}
    \item \textbf{Stato $q_0$ (Iniziale / Cerca simbolo in $w_1$):}
        \begin{itemize}
            \item $q_0 \xrightarrow{a / X, R} q_1$ (Marca 'a' con 'X', va a destra)
            \item $q_0 \xrightarrow{b / X, R} q_6$ (Marca 'b' con 'X', va a destra)
            \item $q_0 \xrightarrow{X / X, R} q_5$ (Tutte le prime $w$ sono marcate, passa alla fase di controllo finale)
        \end{itemize}
    \item \textbf{Stato $q_1$ (Marcata 'a', cerca 'a' in $w_2$):}
        \begin{itemize}
            \item $q_1 \xrightarrow{a / a, R} q_1$ (Salta 'a')
            \item $q_1 \xrightarrow{b / b, R} q_1$ (Salta 'b')
            \item $q_1 \xrightarrow{\# / \#, R} q_2$ (Passa il separatore '\#')
        \end{itemize}
    \item \textbf{Stato $q_2$ (Dopo '\#', cerca 'a' in $w_2$):}
        \begin{itemize}
            \item $q_2 \xrightarrow{X / X, R} q_2$ (Salta 'X' già marcate)
            \item $q_2 \xrightarrow{a / X, L} q_3$ (Trova 'a', marca con 'X', va a sinistra per tornare)
        \end{itemize}
    \item \textbf{Stato $q_6$ (Marcata 'b', cerca 'b' in $w_2$):}
        \begin{itemize}
            \item $q_6 \xrightarrow{a / a, R} q_6$ (Salta 'a')
            \item $q_6 \xrightarrow{b / b, R} q_6$ (Salta 'b')
            \item $q_6 \xrightarrow{\# / \#, R} q_7$ (Passa il separatore '\#')
        \end{itemize}
    \item \textbf{Stato $q_7$ (Dopo '\#', cerca 'b' in $w_2$):}
        \begin{itemize}
            \item $q_7 \xrightarrow{X / X, R} q_7$ (Salta 'X' già marcate)
            \item $q_7 \xrightarrow{b / X, L} q_3$ (Trova 'b', marca con 'X', va a sinistra per tornare)
        \end{itemize}
    \item \textbf{Stato $q_3$ (Match trovato, torna indietro):}
        \begin{itemize}
            \item $q_3 \xrightarrow{X / X, L} q_3$ (Salta 'X' già marcate)
            \item $q_3 \xrightarrow{a / a, L} q_3$ (Salta 'a')
            \item $q_3 \xrightarrow{b / b, L} q_3$ (Salta 'b')
            \item $q_3 \xrightarrow{\# / \#, L} q_4$ (Passa il separatore '\#')
        \end{itemize}
    \item \textbf{Stato $q_4$ (Dopo '\#', torna all'inizio di $w_1$):}
        \begin{itemize}
            \item $q_4 \xrightarrow{a / a, L} q_4$ (Salta 'a')
            \item $q_4 \xrightarrow{b / b, L} q_4$ (Salta 'b')
            \item $q_4 \xrightarrow{X / X, R} q_0$ (Trovata 'X' della 'a' o 'b' iniziale, si posiziona a destra per la prossima iterazione)
        \end{itemize}
    \item \textbf{Stato $q_5$ (Controllo finale, parte $w_1$):}
        \begin{itemize}
            \item $q_5 \xrightarrow{X / X, R} q_5$ (Salta 'X' nella prima metà)
            \item $q_5 \xrightarrow{\# / \#, R} q_8$ (Passa il separatore '\#', ora controlla seconda metà)
        \end{itemize}
    \item \textbf{Stato $q_8$ (Controllo finale, parte $w_2$):}
        \begin{itemize}
            \item $q_8 \xrightarrow{X / X, R} q_8$ (Salta 'X' nella seconda metà)
            \item $q_8 \xrightarrow{B / B, R} q_9$ (Se trova $B$, tutte le parti sono state marcate, stringa accettata)
        \end{itemize}
    \item \textbf{Stato $q_9$ (Accettante):} Stringa accettata.
\end{itemize}
Questa MT decide il linguaggio, rifiutando stringhe come $a\#b$ o $aa\#a$ o $a\#bb$.

\subsection{Esercizio 5: Linguaggio $L = \{a^n b^n c^n \mid n > 0\}$}

Vogliamo progettare una MT che decide il linguaggio di stringhe con un numero uguale di 'a', 'b', e 'c', in sequenza $a^*b^*c^*$. $n$ deve essere maggiore di $0$. Esempio: $abc$, $aabbcc$.

\begin{example}[Strategia]
La strategia è marcare un' 'a', poi una 'b', poi una 'c', e ripetere fino a quando tutti i caratteri sono stati marcati.
\begin{enumerate}
    \item Marcare la prima 'a' non marcata (con 'X').
    \item Spostarsi a destra, saltando altre 'a' e 'Y' (b già marcate), fino a trovare la prima 'b' non marcata. Marcarla (con 'Y').
    \item Spostarsi a destra, saltando altre 'b' e 'Z' (c già marcate), fino a trovare la prima 'c' non marcata. Marcarla (con 'Z').
    \item Spostarsi a sinistra, saltando tutti i simboli marcati e non, fino a trovare la 'X' più a destra.
    \item Spostarsi a destra di una posizione per trovare la prossima 'a' non marcata e ripetere.
    \item Una volta che tutte le 'a' sono state marcate, si controlla che tutte le 'b' e 'c' siano state marcate anch'esse.
\end{enumerate}
\end{example}

\noindent \textbf{Alfabeto del nastro $\Gamma = \{a, b, c, X, Y, Z, B\}$.}
\noindent \textbf{Stati:} $q_0$ (iniziale), $q_1$ (marcata 'a', cerca 'b'), $q_2$ (salta 'Y', cerca 'b'), $q_3$ (marcata 'b', cerca 'c'), $q_4$ (salta 'Z', cerca 'c'), $q_5$ (marcata 'c', torna indietro), $q_6$ (accettazione 'Y' residue), $q_7$ (accettazione 'Z' residue), $q_8$ (accettante).

\noindent \textbf{Transizioni:}
\begin{itemize}
    \item \textbf{Stato $q_0$ (Iniziale / Cerca 'a'):}
        \begin{itemize}
            \item $q_0 \xrightarrow{a / X, R} q_1$ (Marca 'a' con 'X')
            \item $q_0 \xrightarrow{Y / Y, R} q_6$ (Se trova 'Y', tutte le 'a' sono state marcate, si controllano 'b' e 'c' marcate)
        \end{itemize}
    \item \textbf{Stato $q_1$ (Marca 'a', cerca 'b'):}
        \begin{itemize}
            \item $q_1 \xrightarrow{a / a, R} q_1$ (Salta 'a')
            \item $q_1 \xrightarrow{Y / Y, R} q_2$ (Salta 'Y' già marcate)
            \item $q_1 \xrightarrow{b / Y, R} q_3$ (Marca la prima 'b' con 'Y'. Caso iniziale senza 'Y' intermedie)
        \end{itemize}
    \item \textbf{Stato $q_2$ (Salto 'Y' intermedie):}
        \begin{itemize}
            \item $q_2 \xrightarrow{Y / Y, R} q_2$ (Continua a saltare 'Y' già marcate)
            \item $q_2 \xrightarrow{b / Y, R} q_3$ (Marca 'b' con 'Y')
        \end{itemize}
    \item \textbf{Stato $q_3$ (Marca 'b', cerca 'c'):}
        \begin{itemize}
            \item $q_3 \xrightarrow{b / b, R} q_3$ (Salta 'b')
            \item $q_3 \xrightarrow{Z / Z, R} q_4$ (Salta 'Z' già marcate)
            \item $q_3 \xrightarrow{c / Z, L} q_5$ (Marca la prima 'c' con 'Z'. Caso iniziale senza 'Z' intermedie)
        \end{itemize}
    \item \textbf{Stato $q_4$ (Salto 'Z' intermedie):}
        \begin{itemize}
            \item $q_4 \xrightarrow{Z / Z, R} q_4$ (Continua a saltare 'Z' già marcate)
            \item $q_4 \xrightarrow{c / Z, L} q_5$ (Marca 'c' con 'Z')
        \end{itemize}
    \item \textbf{Stato $q_5$ (Marca 'c', torna indietro):}
        \begin{itemize}
            \item $q_5 \xrightarrow{Z / Z, L} q_5$ (Salta 'Z')
            \item $q_5 \xrightarrow{c / c, L} q_5$ (Salta 'c')
            \item $q_5 \xrightarrow{Y / Y, L} q_5$ (Salta 'Y')
            \item $q_5 \xrightarrow{b / b, L} q_5$ (Salta 'b')
            \item $q_5 \xrightarrow{a / a, L} q_5$ (Salta 'a')
            \item $q_5 \xrightarrow{X / X, R} q_0$ (Trovato 'X', sposta a destra per la prossima 'a')
        \end{itemize}
    \item \textbf{Stato $q_6$ (Verifica 'Y' residue):}
        \begin{itemize}
            \item $q_6 \xrightarrow{Y / Y, R} q_6$ (Salta 'Y' residue)
            \item $q_6 \xrightarrow{Z / Z, R} q_7$ (Se trova 'Z', passa a controllare le 'c')
        \end{itemize}
    \item \textbf{Stato $q_7$ (Verifica 'Z' residue):}
        \begin{itemize}
            \item $q_7 \xrightarrow{Z / Z, R} q_7$ (Salta 'Z' residue)
            \item $q_7 \xrightarrow{B / B, R} q_8$ (Se trova $B$, tutte le 'a', 'b', 'c' sono state marcate correttamente. Accetta)
        \end{itemize}
    \item \textbf{Stato $q_8$ (Accettante):} Stringa accettata.
\end{itemize}
Questa MT decide il linguaggio. Implicitamente rifiuta stringhe con ordine errato dei caratteri (es. $acb$) o conteggi non corrispondenti (es. $aabc$).


% =====================================================
% --- START LECTURE 06 ---
% =====================================================

\chapter{Varianti delle Macchine di Turing}



\section{Introduzione alle Varianti delle Macchine di Turing}
Le Macchine di Turing (MT) standard, pur essendo fondamentali per la teoria della computabilità, sono spesso complesse da programmare a causa della loro semplicità definitoria. Oggi esploreremo alcune varianti delle MT che, sebbene sembrino più potenti, si dimostreranno equivalenti alle MT standard in termini di potere computazionale, ma molto più facili da programmare.

\subsection{Consigli per lo Studio}
È utile estrapolare domande chiave dai concetti visti. Ad esempio:
\begin{itemize}
    \item Cos'è un problema? Come si caratterizza?
    \item Problemi di ricerca vs. problemi di decisione: qual è il loro rapporto?
    \item Che ruolo hanno i linguaggi nei problemi di decisione?
    \item Cos'è una Macchina di Turing? Come è definita?
    \item Qual è la condizione di accettazione per una MT?
    \item Cosa significa che una MT decide un linguaggio vs. accetta un linguaggio? Qual è la differenza?
\end{itemize}

\section{Macchine di Turing con Memoria nello Stato}
Riprendiamo l'esempio del linguaggio $WW^R = \{w w^R \mid w \in \{0,1\}^*\}$. Nella MT standard per questo linguaggio, eravamo costretti a creare rami di computazione separati a seconda del primo carattere letto (0 o 1), perché la macchina doveva "ricordarsi" quel carattere. L'idea è: non sarebbe più semplice se la macchina potesse memorizzare direttamente il simbolo letto?

\begin{definition}[Macchina di Turing con Memoria nello Stato]
Una Macchina di Turing con memoria nello stato è una MT che può memorizzare un simbolo dell'alfabeto del nastro all'interno del suo stato finito. Lo stato è quindi una coppia $(q_i, \text{simbolo\_memorizzato})$, dove $q_i$ è lo stato del controllo finito e $\text{simbolo\_memorizzato}$ è il contenuto della "memoria interna".
\begin{itemize}
    \item La macchina può memorizzare uno o più simboli (un numero fissato al momento della progettazione).
    \item La memoria non cresce durante l'esecuzione; è pre-determinata dal design.
\end{itemize}
\end{definition}

\begin{example}[Linguaggio $L = \{w \mid w \in \{0,1\}^*, \text{inizia con } \alpha \text{ e prosegue solo con } \bar{\alpha} \}$, cioè $01^* \cup 10^*$]
Vogliamo riconoscere stringhe binarie che iniziano con un simbolo e tutti gli altri sono diversi (es. $0111$, $1000$).
\begin{itemize}
    \item \textbf{Stato Iniziale}: $(q_0, \emptyset)$, dove $\emptyset$ indica memoria vuota.
    \item \textbf{Lettura Primo Simbolo}:
        \begin{itemize}
            \item Leggiamo un carattere $\alpha \in \{0,1\}$.
            \item Lo lasciamo sul nastro.
            \item Spostiamo la testina a destra (R).
            \item Transitiamo allo stato $(q_1, \alpha)$, memorizzando $\alpha$.
        \end{itemize}
    \item \textbf{Ciclo di Verifica}:
        \begin{itemize}
            \item Nello stato $(q_1, \alpha)$, leggiamo il simbolo $\bar{\alpha}$ (il complemento di $\alpha$).
            \item Lo lasciamo sul nastro.
            \item Spostiamo la testina a destra (R).
            \item Rimaniamo nello stato $(q_1, \alpha)$.
        \end{itemize}
    \item \textbf{Accettazione}:
        \begin{itemize}
            \item Nello stato $(q_1, \alpha)$, se leggiamo un simbolo \texttt{blank} ($\sqcup$).
            \item Lasciamo $\sqcup$ sul nastro.
            \item Spostiamo la testina a destra (R).
            \item Transitiamo allo stato $(q_2, \emptyset)$ (o semplicemente $q_2$), e accettiamo.
        \end{itemize}
\end{itemize}
Questo approccio evita la necessità di duplicare gli stati per 0 e 1, rendendo la programmazione più compatta.
\end{example}

\subsubsection{Potere Computazionale delle MT con Memoria nello Stato}
Le MT con memoria nello stato \emph{non} sono più potenti delle MT standard.
\begin{itemize}
    \item Questa descrizione è un modo compatto per rappresentare una MT standard più grande.
    \item Gli stati di una MT con memoria nello stato sono di fatto ottenibili tramite il prodotto cartesiano degli stati del controllo finito e dei possibili simboli memorizzabili.
    \item Se lo stato $(q_i, \alpha)$ è uno stato, questo può essere semplicemente rinominato come $q_{i,\alpha}$ in una MT standard. Il numero di stati aumenta, ma la natura computazionale rimane la stessa.
\end{itemize}

\section{Macchine di Turing Multitraccia}
Le macchine multitraccia sono un altro trucchetto per semplificare la progettazione delle MT.

\begin{definition}[Macchina di Turing Multitraccia]
Una Macchina di Turing multitraccia ha un singolo nastro, ma questo nastro è diviso orizzontalmente in più \textbf{tracce} (es. 2, 3, 7 tracce).
\begin{itemize}
    \item Ogni cella del nastro può contenere un simbolo per ogni traccia.
    \item La macchina ha una \textbf{singola testina} di grandi dimensioni, capace di leggere e scrivere contemporaneamente su tutte le tracce di una data cella.
    \item Quando la testina si sposta (destra o sinistra), si sposta come un \textbf{monoblocco}, mantenendo la sua posizione relativa su tutte le tracce.
    \item La \textbf{funzione di transizione} è determinata dalla tupla di simboli letti su tutte le tracce nella posizione corrente. Può scrivere una tupla di nuovi simboli su tutte le tracce.
    \item Il numero di tracce è fissato al momento della progettazione e non può variare durante l'esecuzione.
\end{itemize}
\end{definition}

\begin{example}[Linguaggio $L = \{w\#w \mid w \in \{a,b\}^+\}$]
Simuliamo il riconoscimento di questo linguaggio con una MT multitraccia, usando due tracce: la prima per l'input e la seconda per i marcatori.
\begin{itemize}
    \item \textbf{Stato Iniziale} $(q_0, \emptyset)$.
    \item \textbf{Passo 1: Lettura e Marcatura del Primo $w$}
        \begin{itemize}
            \item $(q_0, \emptyset)$: Legge $[\alpha, \sqcup]$ (traccia 1: $\alpha$, traccia 2: blank).
            \item Scrive $[\alpha, *]$ (traccia 1: $\alpha$, traccia 2: asterisco).
            \item Sposta la testina a destra (R).
            \item Transisce a $(q_1, \alpha)$ (memorizza $\alpha$).
            \item Questo processo si ripete per tutti i simboli di $w$.
            \item $(q_1, \alpha)$: Legge $[\beta, \sqcup]$ (qualsiasi simbolo $\beta$ diverso da $\alpha$).
            \item Scrive $[\beta, \sqcup]$.
            \item Sposta la testina a destra (R).
            \item Rimane in $(q_1, \alpha)$.
            \item Al cancelletto:
                \begin{itemize}
                    \item $(q_1, \alpha)$: Legge $[\#, \sqcup]$.
                    \item Scrive $[\#, \sqcup]$.
                    \item Sposta la testina a destra (R).
                    \item Transisce a $(q_2, \alpha)$.
                \end{itemize}
        \end{itemize}
    \item \textbf{Passo 2: Verifica del Secondo $w$}
        \begin{itemize}
            \item $(q_2, \alpha)$: Salta i simboli già marcati (cercando il primo simbolo non marcato di $w^R$).
            \item Legge $[\beta, *]$ (su traccia 1: $\beta$, su traccia 2: asterisco).
            \item Scrive $[\beta, *]$.
            \item Sposta la testina a destra (R).
            \item Rimane in $(q_2, \alpha)$.
            \item Quando trova il simbolo corrispondente a $\alpha$:
                \begin{itemize}
                    \item $(q_2, \alpha)$: Legge $[\alpha, \sqcup]$.
                    \item Scrive $[\alpha, *]$.
                    \item Sposta la testina a sinistra (L).
                    \item Transisce a $(q_3, \emptyset)$ (non serve più memorizzare $\alpha$).
                \end{itemize}
        \end{itemize}
    \item \textbf{Passo 3: Ritorno all'Inizio del Primo $w$}
        \begin{itemize}
            \item $(q_3, \emptyset)$: Salta i simboli marcati (su traccia 2).
            \item Legge $[\beta, *]$.
            \item Scrive $[\beta, *]$.
            \item Sposta la testina a sinistra (L).
            \item Rimane in $(q_3, \emptyset)$.
            \item Al cancelletto:
                \begin{itemize}
                    \item $(q_3, \emptyset)$: Legge $[\#, \sqcup]$.
                    \item Scrive $[\#, \sqcup]$.
                    \item Sposta la testina a sinistra (L).
                    \item Transisce a $(q_4, \emptyset)$.
                \end{itemize}
        \end{itemize}
    \item \textbf{Passo 4: Preparazione per il Prossimo Simbolo}
        \begin{itemize}
            \item $(q_4, \emptyset)$: Salta i simboli non marcati (sulla traccia 2) per trovare il prossimo simbolo da marcare in $w$.
            \item Legge $[\beta, \sqcup]$.
            \item Scrive $[\beta, \sqcup]$.
            \item Sposta la testina a sinistra (L).
            \item Rimane in $(q_5, \emptyset)$. (Separato in $q_5$ e $q_6$ per distinguere tra simboli non marcati e marcati)
            \item Quando trova il prossimo simbolo marcato:
                \begin{itemize}
                    \item $(q_5, \emptyset)$: Legge $[\beta, *]$.
                    \item Scrive $[\beta, *]$.
                    \item Sposta la testina a destra (R).
                    \item Torna a $(q_0, \emptyset)$ per ripetere il ciclo per il prossimo simbolo.
                \end{itemize}
            \item \textbf{Condizione di Accettazione (Fine Verifica)}:
                \begin{itemize}
                    \item Se, durante il ritorno da $q_4$, invece di leggere un simbolo non marcato, si legge un simbolo già marcato su traccia 2:
                        \begin{itemize}
                            \item $(q_4, \emptyset)$: Legge $[\alpha, *]$.
                            \item Scrive $[\alpha, *]$.
                            \item Sposta la testina a destra (R).
                            \item Transisce a $(q_6, \emptyset)$. (Indica che tutti i simboli di $w$ sono stati verificati)
                        \end{itemize}
                    \item Ora bisogna verificare che non ci sia più nulla dopo il cancelletto (cioè anche il secondo $w$ è stato verificato).
                        \begin{itemize}
                            \item $(q_6, \emptyset)$: Legge $[\#, \sqcup]$.
                            \item Scrive $[\#, \sqcup]$.
                            \item Sposta la testina a destra (R).
                            \item Transisce a $(q_7, \emptyset)$.
                        \end{itemize}
                    \item $(q_7, \emptyset)$: Salta i simboli marcati su traccia 2 (assicurandosi che tutto il secondo $w$ sia marcato).
                        \begin{itemize}
                            \item Legge $[\alpha, *]$.
                            \item Scrive $[\alpha, *]$.
                            \item Sposta la testina a destra (R).
                            \item Rimane in $(q_7, \emptyset)$.
                        \end{itemize}
                    \item \textbf{Accettazione Finale}:
                        \begin{itemize}
                            \item $(q_7, \emptyset)$: Legge $[\sqcup, \sqcup]$ (blank su entrambe le tracce).
                            \item Scrive $[\sqcup, \sqcup]$.
                            \item Sposta la testina a destra (R).
                            \item Transisce a $(q_8, \emptyset)$ (Accetta).
                        \end{itemize}
                \end{itemize}
        \end{itemize}
\end{itemize}
Questa macchina è più semplice da progettare rispetto alla versione a nastro singolo perché non si perde il contenuto originale dell'input.
\end{example}

\subsubsection{Potere Computazionale delle MT Multitraccia}
Le MT multitraccia \emph{non} sono più potenti delle MT standard.
\begin{enumerate}
    \item \textbf{Simulazione tramite Alfabeto Esteso}:
    Una MT multitraccia con $K$ tracce può essere simulata da una MT standard (a singolo nastro e singola traccia) che usa un alfabeto esteso. Ogni simbolo del nuovo alfabeto è una tupla di $K$ simboli, rappresentando la combinazione di simboli che apparirebbero su ogni traccia in una data cella. Ad esempio, se le tracce sono 2 e gli alfabeti $\{a,b\}$, $\{*, \sqcup\}$, il nuovo alfabeto può includere simboli come $(a, *)$, $(b, \sqcup)$, ecc. Questo aumenta la dimensione dell'alfabeto, ma non il potere computazionale.
    \item \textbf{Simulazione tramite Spazio Aggiuntivo e Memoria nello Stato (meno comune per la dimostrazione di equivalenza):}
    Una MT standard può simulare una MT multitraccia prendendo l'input della MT multitraccia, inserendo delimitatori tra i simboli per creare "spazio" per le tracce aggiuntive. La MT standard userà poi la memoria nello stato per tenere traccia dei simboli che sarebbero sulle altre tracce e simulerà i movimenti della testina leggendo e scrivendo nei blocchi delimitati. Questo dimostra l'equivalenza ma è più complesso.
\end{enumerate}
In sintesi, le MT multitraccia, sebbene sembrino più sofisticate, hanno lo stesso potere computazionale delle MT standard. Sono solo più facili da programmare.

\section{Macchine di Turing Multinastro}
Introduciamo il modello più conveniente per la programmazione: le Macchine di Turing Multinastro.

\begin{definition}[Macchina di Turing Multinastro]
Una Macchina di Turing multinastro ha \textbf{più nastri completamente indipendenti} (es. 2, 3, 4 nastri).
\begin{itemize}
    \item Ogni nastro ha la propria \textbf{testina indipendente}.
    \item Ogni testina può leggere, scrivere e muoversi \textbf{autonomamente} (destra (R), sinistra (L), o stare ferma (S)).
    \item L'input della macchina si trova sempre sul \textbf{primo nastro} all'avvio. Gli altri nastri (chiamati \textbf{nastri di lavoro} o \emph{work tapes}) sono inizialmente vuoti.
    \item Il numero di nastri è fissato al momento della progettazione e non può cambiare durante l'esecuzione.
\end{itemize}
\end{definition}

\begin{example}[Linguaggio $L = \{w \mid w \in \{0,1,2\}^*, \text{count}(0)=\text{count}(1)=\text{count}(2)\}$]
Vogliamo decidere se il numero di '0', '1' e '2' in una stringa di input è uguale.
Approccio con MT multinastro (4 nastri: 1 input, 3 di lavoro):
\begin{enumerate}
    \item \textbf{Nastro 1 (Input)}: Contiene la stringa $w$.
    \item \textbf{Nastro 2 (Zeros)}: Memorizzerà tutti gli '0'.
    \item \textbf{Nastro 3 (Ones)}: Memorizzerà tutti gli '1'.
    \item \textbf{Nastro 4 (Twos)}: Memorizzerà tutti i '2'.
\end{enumerate}
\textbf{Strategia:}
\begin{itemize}
    \item \textbf{Passo 1: Copia e Separazione (Stato $q_0$)}:
        \begin{itemize}
            \item Si scansiona il Nastro 1 da sinistra a destra.
            \item Se si legge '0' sul Nastro 1:
                \begin{itemize}
                    \item Mantiene '0' sul Nastro 1, muove testina 1 a destra (R).
                    \item Scrive '0' sul Nastro 2 (che era $\sqcup$), muove testina 2 a destra (R).
                \end{itemize}
            \item Se si legge '1' sul Nastro 1:
                \begin{itemize}
                    \item Mantiene '1' sul Nastro 1, muove testina 1 a destra (R).
                    \item Scrive '1' sul Nastro 3 (che era $\sqcup$), muove testina 3 a destra (R).
                \end{itemize}
            \item Se si legge '2' sul Nastro 1:
                \begin{itemize}
                    \item Mantiene '2' sul Nastro 1, muove testina 1 a destra (R).
                    \item Scrive '2' sul Nastro 4 (che era $\sqcup$), muove testina 4 a destra (R).
                \end{itemize}
            \item Questo continua finché il Nastro 1 non incontra $\sqcup$.
        \end{itemize}
    \item \textbf{Passo 2: Riavvolgimento Testine di Lavoro (Transizione da $q_0$ a $q_1$)}:
        \begin{itemize}
            \item Quando Nastro 1 legge $\sqcup$:
                \begin{itemize}
                    \item Nastro 1: $[\sqcup, \sqcup]$, testina 1 ferma (S).
                    \item Nastri 2, 3, 4 (che avranno la testina all'inizio della loro area blank): $[\sqcup, \sqcup]$, testine 2, 3, 4 muovono a sinistra (L).
                    \item Transisce allo stato $q_1$.
                \end{itemize}
        \end{itemize}
    \item \textbf{Passo 3: Confronto Conteggi (Stato $q_1$)}:
        \begin{itemize}
            \item Nello stato $q_1$, le testine dei nastri 2, 3, 4 sono posizionate all'inizio dei simboli scritti.
            \item Si muovono simultaneamente a sinistra (L), leggendo i simboli.
            \item Se Nastro 2 legge '0', Nastro 3 legge '1', Nastro 4 legge '2':
                \begin{itemize}
                    \item Mantengono i simboli, muovono testine 2, 3, 4 a sinistra (L).
                    \item Rimangono in $q_1$.
                \end{itemize}
            \item Questo processo continua finché tutti i nastri incontrano il simbolo $\sqcup$ \textbf{contemporaneamente}.
        \end{itemize}
    \item \textbf{Passo 4: Accettazione (Transizione da $q_1$ a $q_2$)}:
        \begin{itemize}
            \item Quando Nastro 2, Nastro 3 e Nastro 4 leggono $\sqcup$ contemporaneamente:
                \begin{itemize}
                    \item Nastri 2, 3, 4: $[\sqcup, \sqcup]$, testine 2, 3, 4 ferme (S).
                    \item Transisce allo stato $q_2$ e accetta.
                \end{itemize}
        \end{itemize}
\end{itemize}
Le MT multinastro sono molto convenienti perché permettono un controllo indipendente delle testine, rendendo algoritmi come questo (che altrimenti richiederebbero complessi vai-e-vieni sul nastro singolo) molto più intuitivi.
\end{example}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[shorten >=1pt,node distance=3.5cm,on grid,auto]
        \node[state,initial] (q0) {$q_0$};
        \node[state, right=of q0] (q1) {$q_1$};
        \node[state, right=of q1, accepting] (q2) {$q_2$};

        \path[->] (q0) edge [loop above] node[above, align=center, font=\footnotesize] {
            $1:0/0, R; 2:\sqcup/0, R$\\
            $1:1/1, R; 3:\sqcup/1, R$\\
            $1:2/2, R; 4:\sqcup/2, R$
        } (q0);

        \path[->] (q0) edge node[above, align=center, font=\footnotesize] {
            $1:\sqcup/\sqcup, S$\\
            $2:\sqcup/\sqcup, L$\\
            $3:\sqcup/\sqcup, L$\\
            $4:\sqcup/\sqcup, L$
        } (q1);
        
        \path[->] (q1) edge [loop above] node[above, align=center, font=\footnotesize] {
            $2:0/0, L$\\
            $3:1/1, L$\\
            $4:2/2, L$
        } (q1);
        
        \path[->] (q1) edge node[above, align=center, font=\footnotesize] {
            $2:\sqcup/\sqcup, S$\\
            $3:\sqcup/\sqcup, S$\\
            $4:\sqcup/\sqcup, S$
        } (q2);
    \end{tikzpicture}
    \caption{Diagramma di una Macchina di Turing Multinastro per il linguaggio $L=\{w \mid \text{count}(0)=\text{count}(1)=\text{count}(2)\}$}
\end{figure}

\section{Equivalenza e Complessità Temporale}
Come per le varianti precedenti, ci si chiede: le MT multinastro sono più potenti delle MT standard? La risposta è no, ma sono potenzialmente più veloci.

\begin{definition}[Tempo di Esecuzione di una MT]
Il tempo di esecuzione di una Macchina di Turing su una certa stringa di input $W$ è definito come il \textbf{numero di passi} (o il numero di configurazioni nella sua computazione) che la macchina compie prima di arrestarsi. Se la macchina non si arresta, il tempo di esecuzione è infinito. Questa definizione è molto precisa e permette un'analisi fine della complessità.
\end{definition}

\begin{theorem}[Equivalenza MT Multinastro e MT Standard]
Sia $M$ una macchina di Turing multinastro. Allora esiste una macchina $S$ a nastro singolo (e multitraccia) tale che il linguaggio riconosciuto da $M$ è uguale al linguaggio riconosciuto da $S$.
\end{theorem}

\subsubsection{Dimostrazione (Sketch)}
L'idea è simulare una MT multinastro $M$ (con $K$ nastri) tramite una MT $S$ a nastro singolo ma multitraccia.
\begin{itemize}
    \item \textbf{Architettura di $S$}: La macchina $S$ avrà $2K$ tracce.
        \begin{itemize}
            \item Le tracce dispari (es. 1, 3, 5, \dots, $2K-1$) conterranno il contenuto dei $K$ nastri di $M$.
            \item Le tracce pari (es. 2, 4, 6, \dots, $2K$) conterranno un marcatore speciale (es. '\texttt{*}') che indica la posizione della testina del nastro corrispondente in $M$.
            \item Il resto della traccia pari sarà $\sqcup$.
        \end{itemize}
    \item \textbf{Simulazione di un passo di $M$ da parte di $S$}: Per simulare un singolo passo della macchina $M$, $S$ deve eseguire i seguenti sottoprocessi:
        \begin{enumerate}
            \item \textbf{Lettura dei Simboli}: $S$ scansiona il suo nastro da sinistra a destra per trovare tutti i $K$ marcatori delle testine. Quando trova un marcatore su una traccia pari, legge il simbolo corrispondente sulla traccia dispari immediatamente superiore e lo memorizza nello stato di $S$.
            \item \textbf{Decisione della Transizione}: Una volta letti tutti e $K$ i simboli (e memorizzati nel suo stato), $S$ sa quale transizione $M$ avrebbe eseguito (basandosi sullo stato attuale di $M$ e i $K$ simboli letti). Questa transizione specifica quali simboli scrivere e in quale direzione muovere ciascuna testina.
            \item \textbf{Aggiornamento del Nastro e Spostamento delle Testine}: $S$ scansiona il suo nastro una seconda volta (es. da destra a sinistra), e in corrispondenza di ogni marcatore di testina:
                \begin{itemize}
                    \item Sovrascrive il simbolo sulla traccia dispari con il nuovo simbolo deciso nel passo precedente.
                    \item Sposta il marcatore '\texttt{*}' sulla traccia pari nella nuova posizione della testina (spostandolo a destra o a sinistra). Se il marcatore deve muoversi nella direzione opposta a quella della scansione attuale di $S$, ciò richiederà due mosse aggiuntive per il marcatore (sposta avanti, poi indietro e riprende la scansione).
                \end{itemize}
            \item \textbf{Ritorno alla Posizione Canonica}: $S$ riporta la sua testina all'inizio del nastro (o a una posizione prefissata) per il prossimo ciclo di simulazione.
        \end{enumerate}
\end{itemize}

\subsubsection{Analisi della Complessità Temporale della Simulazione}
Sia $M$ una MT multinastro che esegue $N$ passi.
\begin{itemize}
    \item Il tempo di esecuzione di $M$ è $N$.
    \item Durante $N$ passi, una testina di $M$ non può allontanarsi più di $N$ celle dalla sua posizione iniziale.
    \item Nella simulazione di $S$, le testine di $M$ possono essere sparse sul nastro di $S$. Nella peggiore delle ipotesi, la testina di un nastro si trova a $i$ posizioni a sinistra dell'inizio del nastro di $S$, e un'altra testina si trova a $i$ posizioni a destra. La distanza massima tra due testine dopo $i$ passi di $M$ è $2i$ celle.
    \item Per simulare il $i$-esimo passo di $M$, $S$ deve scansionare un segmento del suo nastro di lunghezza proporzionale alla massima distanza raggiunta dalle testine fino a quel momento, che è $O(i)$.
    \item Per ogni passo di $M$, $S$ esegue due scansioni complete ($2 \times O(i)$) e potenzialmente $2K$ mosse aggiuntive (per i marcatori che vanno contro-scansione). Quindi, un passo di $M$ costa $S$ un tempo $O(i) + O(i) + O(K) = O(i+K)$.
    \item Il tempo totale per $S$ per simulare $N$ passi di $M$ sarà la somma dei costi per ogni passo:
    \[
    T_S(N) = \sum_{i=1}^{N} C \cdot (i + K) = C \cdot \sum_{i=1}^{N} i + C \cdot \sum_{i=1}^{N} K
    \]
    \[
    T_S(N) = C \cdot \frac{N(N+1)}{2} + C \cdot NK = O(N^2 + NK) = O(N^2)
    \]
    \item Pertanto, una MT multinastro può essere simulata da una MT multitraccia (e quindi standard) con un \textbf{rallentamento quadratico}. Se $M$ impiega $N$ passi, $S$ impiega $O(N^2)$ passi.
\end{itemize}

\subsection{Conclusioni sul Potere Computazionale}
\begin{itemize}
    \item Le Macchine di Turing con memoria nello stato, le Macchine di Turing multitraccia e le Macchine di Turing multinastro sono tutte \textbf{computazionalmente equivalenti} alle Macchine di Turing standard. Non possono riconoscere linguaggi che una MT standard non possa riconoscere.
    \item Tuttavia, offrono notevoli vantaggi in termini di \textbf{facilità di programmazione} e possono garantire un'esecuzione \textbf{più efficiente} (con un rallentamento polinomiale, non esponenziale, nella simulazione).
    \item Per la loro praticità, le Macchine di Turing multinastro sono spesso il modello preferito nella pratica per studiare la complessità degli algoritmi.
\end{itemize}


% =====================================================
% --- START LECTURE 07 ---
% =====================================================

\chapter{Macchine di Turing Multinastro}



\section{Introduzione alle Macchine di Turing Multinastro}

\subsection{Definizione e Equivalenza}
Le macchine di Turing (MT) multinastro sono estensioni del modello standard che possiedono più nastri, ciascuno con la propria testina indipendente. Questo permette di accedere a diverse parti della memoria contemporaneamente.

\begin{definition}[Macchina di Turing Multinastro]
Una macchina di Turing multinastro è definita come una tupla $(Q, \Sigma, \Gamma, \delta, q_0, q_{acc}, q_{rej})$, dove:
\begin{itemize}
    \item $Q$ è un insieme finito di stati.
    \item $\Sigma$ è l'alfabeto di input (non contiene il simbolo di blank $\Box$).
    \item $\Gamma$ è l'alfabeto del nastro (contiene $\Sigma$ e $\Box$).
    \item $\delta: Q \times \Gamma^k \to Q \times (\Gamma \times \{L, R, S\})^k$ è la funzione di transizione, dove $k$ è il numero di nastri. Per ogni stato e per ogni tupla di simboli letti dalle $k$ testine, la funzione specifica il prossimo stato, i simboli da scrivere su ogni nastro e le direzioni in cui muovere ogni testina (Left, Right, Stay).
    \item $q_0 \in Q$ è lo stato iniziale.
    \item $q_{acc} \in Q$ è lo stato di accettazione.
    \item $q_{rej} \in Q$ è lo stato di rifiuto.
\end{itemize}
\end{definition}

\begin{proposition}[Equivalenza]
Le macchine di Turing multinastro sono equivalenti alle macchine di Turing standard (mononastro). Ciò significa che ogni linguaggio riconoscibile/decidibile da una MT multinastro è riconoscibile/decidibile anche da una MT standard, e viceversa.
\end{proposition}
Sebbene non aumentino il potere computazionale (cioè, non possono risolvere più problemi), le MT multinastro possono essere più efficienti in termini di tempo o più facili da programmare, grazie alla capacità di gestire più flussi di dati in parallelo.

\subsection{Convenzioni di Notazione nelle Transizioni}
Nei diagrammi di stato (e nella loro descrizione testuale), le transizioni sono etichettate con una notazione compatta per rappresentare le operazioni su più nastri. Una transizione da uno stato $q_i$ a $q_j$ etichettata con, ad esempio:
\[ \text{nastro 1: } (\text{leggi}_1/\text{scrivi}_1, \text{muovi}_1), \quad \text{nastro 2: } (\text{leggi}_2/\text{scrivi}_2, \text{muovi}_2) \]
significa che per eseguire la transizione, la MT deve leggere $\text{leggi}_1$ dal nastro 1 e $\text{leggi}_2$ dal nastro 2. Se le condizioni di lettura sono soddisfatte, la MT scrive $\text{scrivi}_1$ su nastro 1 e $\text{scrivi}_2$ su nastro 2, e muove le testine come indicato.

\begin{itemize}
    \item \textbf{Simboli Variabili:} Spesso si usano simboli come $\alpha, \beta, \gamma$ per indicare caratteri generici dall'alfabeto del nastro ($\Gamma$).
    \begin{itemize}
        \item Se lo \textbf{stesso simbolo variabile} (es. $\alpha$) compare nella lettura di \textbf{più nastri} nella stessa etichetta di transizione, significa che su quei nastri deve essere letto lo \textbf{stesso identico carattere}. Es. \texttt{nastro 1: ($\alpha$/$\alpha$, R), nastro 2: ($\alpha$/$\alpha$, R)} implica che se si legge '0' su nastro 1, si deve leggere '0' anche su nastro 2. Se si legge '1' su nastro 1, si deve leggere '1' anche su nastro 2. Se si legge '0' su nastro 1 e '1' su nastro 2, questa transizione non è valida.
        \item Se \textbf{simboli variabili diversi} (es. $\alpha, \beta$) compaiono nella lettura di \textbf{più nastri} nella stessa etichetta di transizione, significa che i caratteri letti possono essere \textbf{diversi o uguali}. Es. \texttt{nastro 1: ($\alpha$/$\alpha$, R), nastro 2: ($\beta$/$\beta$, R)} implica che $\alpha$ può essere '0' e $\beta$ '1' (o viceversa, o uguali). Non viene imposto alcun confronto.
    \end{itemize}
    \item \textbf{Omissione di operazioni:} Se per un nastro non viene specificata alcuna operazione di scrittura o movimento, si intende che la testina rimane ferma (\texttt{S}) e il simbolo non viene modificato.
    \item \textbf{Simbolo di blank:} Il simbolo $\Box$ indica una cella vuota.
\end{itemize}

\section{Esercizi di Riconoscimento di Linguaggi}
Illustriamo il funzionamento delle MT multinastro attraverso esempi pratici di riconoscimento di linguaggi.

\subsection{Linguaggio $L_1 = \{A\#B\#AB \mid A,B \in \{0,1\}^+\}$}
\begin{definition}
Il linguaggio $L_1$ è l'insieme di tutte le stringhe composte da tre parti $A$, $B$ e $AB$ (concatenazione di $A$ e $B$), separate da simboli \texttt{\#}. Le stringhe $A$ e $B$ sono composte da caratteri '0' o '1' e devono avere lunghezza di almeno 1.
\end{definition}
\begin{example}
Esempi di stringhe in $L_1$: $0\texttt{\#}1\texttt{\#}01$, $01\texttt{\#}0\texttt{\#}010$.
Esempi di stringhe non in $L_1$: $0\texttt{\#}1\texttt{\#}10$ (AB sbagliato), $0\texttt{\#}\texttt{\#}0$ (B vuoto), $\texttt{\#}1\texttt{\#}1$ (A vuoto).
\end{example}

\subsubsection{Strategia}
Utilizziamo una MT con 2 nastri.
\begin{itemize}
    \item \textbf{Nastro 1 (Input):} Contiene la stringa di input $A\#B\#AB$.
    \item \textbf{Nastro 2 (Lavoro):} Utilizzato per copiare la stringa $A$ e poi $B$ sequenzialmente, ottenendo $AB$. Successivamente, il contenuto di Nastro 2 (che è $AB$) verrà confrontato con l'ultima parte della stringa sul Nastro 1 (che dovrebbe essere $AB$).
\end{itemize}

\subsubsection{Implementazione (Stati e Transizioni)}
\begin{description}
    \item[$q_0$ (Copia A):] Legge i caratteri di $A$ dal Nastro 1 e li copia sul Nastro 2.
    \begin{itemize}
        \item \textbf{Condizione:} Leggi $\alpha \in \{0,1\}$ su Nastro 1, $\Box$ su Nastro 2.
        \item \textbf{Azione:} Scrivi $\alpha$ su Nastro 2. Sposta entrambe le testine a destra. Rimani in $q_0$.
        \item \textbf{Uscita:} Quando Nastro 1 legge \texttt{\#}.
        \item \textbf{Transizione:} Da $q_0$ a $q_1$: Nastro 1: $(\texttt{\#}/\texttt{\#}, R)$, Nastro 2: $(\Box/\Box, S)$.
    \end{itemize}
    \item[$q_1$ (Copia B):] Legge i caratteri di $B$ dal Nastro 1 e li copia sul Nastro 2 (dopo la copia di $A$).
    \begin{itemize}
        \item \textbf{Condizione:} Leggi $\alpha \in \{0,1\}$ su Nastro 1, $\Box$ su Nastro 2.
        \item \textbf{Azione:} Scrivi $\alpha$ su Nastro 2. Sposta entrambe le testine a destra. Rimani in $q_1$.
        \item \textbf{Uscita:} Quando Nastro 1 legge \texttt{\#}.
        \item \textbf{Transizione:} Da $q_1$ a $q_2$: Nastro 1: $(\texttt{\#}/\texttt{\#}, R)$, Nastro 2: $(\Box/\Box, S)$.
    \end{itemize}
    \item[$q_2$ (Riavvolgi Nastro 2):] Riporta la testina del Nastro 2 all'inizio della stringa $AB$ (cioè all'inizio del Nastro 2).
    \begin{itemize}
        \item \textbf{Condizione:} Leggi $\alpha \in \{0,1\}$ su Nastro 2.
        \item \textbf{Azione:} Scrivi $\alpha$ su Nastro 2. Sposta testina Nastro 2 a sinistra. Rimani in $q_2$.
        \item \textbf{Uscita:} Quando Nastro 2 legge $\Box$.
        \item \textbf{Transizione:} Da $q_2$ a $q_3$: Nastro 2: $(\Box/\Box, R)$. (Nastro 1 rimane fermo).
    \end{itemize}
    \item[$q_3$ (Confronto AB):] Confronta la stringa $AB$ sul Nastro 2 con l'ultima parte della stringa sul Nastro 1.
    \begin{itemize}
        \item \textbf{Condizione:} Leggi $\alpha \in \{0,1\}$ su Nastro 1, $\alpha$ su Nastro 2. (Stesso $\alpha$ indica che devono essere uguali).
        \item \textbf{Azione:} Scrivi $\alpha$ su Nastro 1 e $\alpha$ su Nastro 2. Sposta entrambe le testine a destra. Rimani in $q_3$.
        \item \textbf{Uscita:} Quando entrambe le testine leggono $\Box$.
        \item \textbf{Transizione:} Da $q_3$ a $q_{acc}$: Nastro 1: $(\Box/\Box, S)$, Nastro 2: $(\Box/\Box, S)$.
    \end{itemize}
    \item[Rifiuto:] Se in qualsiasi stato la MT tenta una transizione non definita (es. simboli non corrispondenti durante il confronto, \texttt{\#} al posto di un carattere, stringa non conforme al formato $A\texttt{\#}B\texttt{\#}AB$), la macchina si blocca e rifiuta.
\end{description}

\subsection{Linguaggio $L_2 = \{A\#B\#C \mid A,B,C \in \{0,1\}^+, |A|>|B|>|C|, |C|=|A|-|B|\}$}
\begin{definition}
Il linguaggio $L_2$ consiste in stringhe $A\#B\#C$ dove $A,B,C$ sono stringhe non vuote di '0' o '1'. Devono valere le condizioni sulle lunghezze:
\begin{itemize}
    \item $|A| > |B| > |C|$ (lunghezze strettamente decrescenti)
    \item $|C| = |A| - |B|$
\end{itemize}
\end{definition}

\subsubsection{Strategia}
Utilizziamo 3 nastri.
\begin{itemize}
    \item \textbf{Nastro 1 (Input):} Contiene la stringa di input.
    \item \textbf{Nastro 2 (Lavoro):} Usato per marcare la lunghezza di $A$ (con 'X') e poi sottrarre la lunghezza di $B$. Al termine, conterrà $|A|-|B|$.
    \item \textbf{Nastro 3 (Lavoro):} Usato per marcare la lunghezza di $B$ (con 'X').
\end{itemize}

\subsubsection{Implementazione (Stati e Transizioni)}
\begin{description}
    \item[$q_0$ (Copia $|A|$ su Nastro 2):] Legge i caratteri di $A$ dal Nastro 1 e marca 'X' sul Nastro 2 per ogni carattere letto.
    \begin{itemize}
        \item \textbf{Condizione:} Nastro 1: $\alpha \in \{0,1\}$, Nastro 2: $\Box$.
        \item \textbf{Azione:} Nastro 1: $(\alpha/\alpha, R)$, Nastro 2: $(\Box/X, R)$. Rimani in $q_0$.
        \item \textbf{Uscita:} Quando Nastro 1 legge \texttt{\#}.
        \item \textbf{Transizione:} Da $q_0$ a $q_1$: Nastro 1: $(\texttt{\#}/\texttt{\#}, R)$, Nastro 2: $(X/X, L)$ (sposta indietro per iniziare la \textbf{sottrazione}).
    \end{itemize}
    \item[$q_1$ (Copia $|B|$ su Nastro 3 e calcola $|A|-|B|$):] Legge i caratteri di $B$ dal Nastro 1, marca 'X' sul Nastro 3 per ogni carattere letto, e cancella una 'X' dal Nastro 2 per ogni carattere letto (simulando $|A|-|B|$).
    \begin{itemize}
        \item \textbf{Condizione:} Nastro 1: $\alpha \in \{0,1\}$, Nastro 2: $X$, Nastro 3: $\Box$.
        \item \textbf{Azione:} Nastro 1: $(\alpha/\alpha, R)$, Nastro 2: $(X/\Box, L)$, Nastro 3: $(\Box/X, R)$. Rimani in $q_1$.
        \item \textbf{Controllo $|A|>|B|$:} Se Nastro 2 diventa $\Box$ prima che Nastro 1 trovi \texttt{\#}, significa che $|A| \le |B|$, quindi la condizione $|A|>|B|$ non è rispettata. La MT si blocca e rifiuta. Altrimenti, se Nastro 1 trova \texttt{\#} e Nastro 2 ha ancora X, la condizione è rispettata.
        \item \textbf{Uscita:} Quando Nastro 1 legge \texttt{\#}.
        \item \textbf{Transizione:} Da $q_1$ a $q_2$: Nastro 1: $(\texttt{\#}/\texttt{\#}, R)$, Nastro 2: $(X/X, S)$, Nastro 3: $(\Box/\Box, L)$. (Nastro 2 rimane fermo sulla X residua, Nastro 3 si riavvolge per il confronto con C).
    \end{itemize}
    \item[$q_2$ (Confronto $|C|$ con $|A|-|B|$ e $|B|>|C|$):] Legge i caratteri di $C$ dal Nastro 1, cancella una 'X' dal Nastro 2 (verificando $|C|=|A|-|B|$), e cancella una 'X' dal Nastro 3 (verificando $|B|>|C|$).
    \begin{itemize}
        \item \textbf{Condizione:} Nastro 1: $\alpha \in \{0,1\}$, Nastro 2: $X$, Nastro 3: $X$.
        \item \textbf{Azione:} Nastro 1: $(\alpha/\alpha, R)$, Nastro 2: $(X/\Box, L)$, Nastro 3: $(X/\Box, L)$. Rimani in $q_2$.
        \item \textbf{Controllo $|C|=|A|-|B|$:} Se Nastro 2 diventa $\Box$ e Nastro 1 non è finito, significa $|C|>|A|-|B|$. Se Nastro 1 finisce e Nastro 2 ha ancora X, significa $|C|<|A|-|B|$. In entrambi i casi, rifiuta. Se Nastro 1 e Nastro 2 finiscono assieme (Nastro 2 legge $\Box$ quando Nastro 1 legge $\Box$), allora $|C|=|A|-|B|$.
        \item \textbf{Controllo $|B|>|C|$:} Durante il processo, Nastro 3 deve ancora avere 'X' quando Nastro 1 finisce (cioè Nastro 3 deve diventare $\Box$ *dopo* Nastro 1). Se Nastro 3 diventa $\Box$ prima o insieme a Nastro 1, significa $|B| \le |C|$, quindi rifiuta.
        \item \textbf{Uscita:} Quando Nastro 1 legge $\Box$.
        \item \textbf{Transizione:} Da $q_2$ a $q_{acc}$: Nastro 1: $(\Box/\Box, S)$, Nastro 2: $(\Box/\Box, S)$, Nastro 3: $(X/X, S)$ (Nastro 3 deve ancora avere X per $|B|>|C|$).
    \end{itemize}
    \item[Rifiuto:] Tutte le condizioni non soddisfatte portano al rifiuto (es. testina Nastro 2 su $\Box$ troppo presto, testina Nastro 3 su $\Box$ troppo presto, etc.).
\end{description}

\subsection{Linguaggio $L_3 = \{W\#W\#W \mid W \in \{0,1\}^*\}$}
\begin{definition}
Il linguaggio $L_3$ consiste in stringhe $W\#W\#W$, dove $W$ è una stringa (anche vuota) di '0' o '1'. Le tre occorrenze di $W$ devono essere identiche.
\end{definition}
\begin{example}
Esempi: $01\#01\#01$, $\#\#$ (per $W=\epsilon$).
\end{example}

\subsubsection{Strategia}
Utilizziamo 3 nastri.
\begin{itemize}
    \item \textbf{Nastro 1 (Input):} Contiene la stringa di input.
    \item \textbf{Nastro 2 (Lavoro):} Copia la prima $W$.
    \item \textbf{Nastro 3 (Lavoro):} Copia la prima $W$.
\end{itemize}
Dopo aver copiato la prima $W$, le testine di Nastro 2 e 3 vengono riavvolte. Successivamente, la seconda $W$ sul Nastro 1 viene confrontata con la copia su Nastro 2. Infine, la terza $W$ sul Nastro 1 viene confrontata con la copia su Nastro 3.

\subsubsection{Implementazione (Stati e Transizioni)}
\begin{description}
    \item[$q_0$ (Copia $W_1$ su Nastro 2 e 3):] Legge i caratteri della prima $W$ dal Nastro 1 e li copia su Nastro 2 e Nastro 3.
    \begin{itemize}
        \item \textbf{Condizione:} Nastro 1: $\alpha \in \{0,1\}$, Nastro 2: $\Box$, Nastro 3: $\Box$.
        \item \textbf{Azione:} Nastro 1: $(\alpha/\alpha, R)$, Nastro 2: $(\Box/\alpha, R)$, Nastro 3: $(\Box/\alpha, R)$. Rimani in $q_0$.
        \item \textbf{Uscita:} Quando Nastro 1 legge \texttt{\#}.
        \item \textbf{Transizione:} Da $q_0$ a $q_1$: Nastro 1: $(\#/\#, R)$, Nastro 2: $(\Box/\Box, L)$, Nastro 3: $(\Box/\Box, L)$. (Per $W=\epsilon$, la MT va direttamente a $q_1$ leggendo \texttt{\#} sul Nastro 1 e $\Box$ su Nastro 2 e 3. Le testine 2 e 3 si muovono a sinistra sul $\Box$ e poi, nel successivo riavvolgimento, si riposizionano correttamente).
    \end{itemize}
    \item[$q_1$ (Riavvolgi Nastro 2 e 3):] Riporta le testine di Nastro 2 e 3 all'inizio delle loro rispettive stringhe $W$.
    \begin{itemize}
        \item \textbf{Condizione:} Nastro 2: $\alpha \in \{0,1\}$, Nastro 3: $\alpha \in \{0,1\}$.
        \item \textbf{Azione:} Nastro 2: $(\alpha/\alpha, L)$, Nastro 3: $(\alpha/\alpha, L)$. Rimani in $q_1$.
        \item \textbf{Uscita:} Quando Nastro 2 e Nastro 3 leggono $\Box$.
        \item \textbf{Transizione:} Da $q_1$ a $q_2$: Nastro 2: $(\Box/\Box, R)$, Nastro 3: $(\Box/\Box, R)$. (Nastro 1 rimane fermo).
    \end{itemize}
    \item[$q_2$ (Confronta $W_2$ con Nastro 2):] Confronta la seconda $W$ sul Nastro 1 con la copia su Nastro 2.
    \begin{itemize}
        \item \textbf{Condizione:} Nastro 1: $\alpha \in \{0,1\}$, Nastro 2: $\alpha \in \{0,1\}$.
        \item \textbf{Azione:} Nastro 1: $(\alpha/\alpha, R)$, Nastro 2: $(\alpha/\alpha, R)$. Rimani in $q_2$.
        \item \textbf{Uscita:} Quando Nastro 1 legge \texttt{\#} e Nastro 2 legge $\Box$.
        \item \textbf{Transizione:} Da $q_2$ a $q_3$: Nastro 1: $(\#/\#, R)$, Nastro 2: $(\Box/\Box, S)$.
    \end{itemize}
    \item[$q_3$ (Confronta $W_3$ con Nastro 3):] Confronta la terza $W$ sul Nastro 1 con la copia su Nastro 3.
    \begin{itemize}
        \item \textbf{Condizione:} Nastro 1: $\alpha \in \{0,1\}$, Nastro 3: $\alpha \in \{0,1\}$.
        \item \textbf{Azione:} Nastro 1: $(\alpha/\alpha, R)$, Nastro 3: $(\alpha/\alpha, R)$. Rimani in $q_3$.
        \item \textbf{Uscita:} Quando Nastro 1 e Nastro 3 leggono $\Box$.
        \item \textbf{Transizione:} Da $q_3$ a $q_{acc}$: Nastro 1: $(\Box/\Box, S)$, Nastro 3: $(\Box/\Box, S)$.
    \end{itemize}
    \item[Rifiuto:] Qualsiasi discrepanza tra i simboli durante i confronti o formato non corretto della stringa porta al rifiuto.
\end{description}

\subsection{Linguaggio $L_4 = \{A\#B \mid A,B \in \{a,b\}^*, A \subseteq B \}$}
\begin{definition}
Il linguaggio $L_4$ è l'insieme di stringhe $A\#B$ dove $A$ e $B$ sono stringhe sull'alfabeto $\{a,b\}$, e $A$ è una sottostringa di $B$. La stringa vuota ($\epsilon$) è una sottostringa di ogni stringa.
\end{definition}
\begin{example}
Esempi: $a\#bab$, $ab\#ababa$, $\#ab$ (A è vuota).
Esempi non in $L_4$: $ab\#ba$, $a\#b$.
\end{example}

\subsubsection{Strategia}
Questa è più complessa a causa della natura della sottostringa, che può apparire in qualsiasi posizione all'interno di $B$.
\begin{itemize}
    \item \textbf{Nastro 1 (Input):} Contiene la stringa di input.
    \item \textbf{Nastro 2 (Lavoro):} Copia la stringa $A$.
\end{itemize}
La strategia consiste nel provare a far corrispondere la stringa $A$ (copiata sul Nastro 2) con ogni possibile sottostringa di $B$ sul Nastro 1. Se un tentativo fallisce, si sposta la posizione di partenza su $B$ di un carattere e si riprova.

\subsubsection{Implementazione (Stati e Transizioni)}
\begin{description}
    \item[$q_0$ (Copia A):] Legge i caratteri di $A$ dal Nastro 1 e li copia sul Nastro 2.
    \begin{itemize}
        \item \textbf{Condizione:} Nastro 1: $\alpha \in \{a,b\}$, Nastro 2: $\Box$.
        \item \textbf{Azione:} Nastro 1: $(\alpha/\alpha, R)$, Nastro 2: $(\Box/\alpha, R)$. Rimani in $q_0$.
        \item \textbf{Uscita:} Quando Nastro 1 legge \texttt{\#}.
        \item \textbf{Transizione:} Da $q_0$ a $q_1$: Nastro 1: $(\#/\#, R)$, Nastro 2: $(\Box/\Box, L)$.
    \end{itemize}
    \item[$q_1$ (Riavvolgi Nastro 2):] Riporta la testina del Nastro 2 all'inizio della stringa $A$.
    \begin{itemize}
        \item \textbf{Condizione:} Nastro 2: $\alpha \in \{a,b\}$.
        \item \textbf{Azione:} Nastro 2: $(\alpha/\alpha, L)$. Rimani in $q_1$.
        \item \textbf{Uscita:} Quando Nastro 2 legge $\Box$.
        \item \textbf{Transizione:} Da $q_1$ a $q_2$: Nastro 2: $(\Box/\Box, R)$. (Nastro 1 rimane fermo).
    \end{itemize}
    \item[$q_2$ (Tentativo di Match Iniziale):] Questo stato è il punto di partenza per ogni tentativo di confronto di $A$ con una sottostringa di $B$. Gestisce anche il caso di $A = \epsilon$.
    \begin{itemize}
        \item \textbf{Condizione (A è $\epsilon$):} Nastro 2: $\Box$.
        \item \textbf{Azione:} Nastro 2: $(\Box/\Box, S)$. Spostati a $q_5$ e accetta. (Se $A$ è vuota, è sottostringa di qualsiasi $B$).
        \item \textbf{Condizione (Inizio Match):} Nastro 1: $\alpha \in \{a,b\}$, Nastro 2: $\alpha \in \{a,b\}$.
        \item \textbf{Azione:} Nastro 1: $(\alpha/\alpha, R)$, Nastro 2: $(\alpha/\alpha, R)$. Spostati a $q_3$. (Qui inizia il vero confronto).
        \item \textbf{Condizione (Nastro 1 finisce, A non vuota):} Nastro 1: $\Box$, Nastro 2: $\alpha \in \{a,b\}$. (Se B finisce ma A non è stata trovata)
        \item \textbf{Azione:} Rifiuta. (Non c'è una transizione definita per questo caso, quindi la MT si blocca e rifiuta).
    \end{itemize}
    \item[$q_3$ (Match di $A$):] Continua il confronto tra $A$ (Nastro 2) e la sottostringa di $B$ (Nastro 1).
    \begin{itemize}
        \item \textbf{Condizione (Match):} Nastro 1: $\alpha \in \{a,b\}$, Nastro 2: $\alpha \in \{a,b\}$.
        \item \textbf{Azione:} Nastro 1: $(\alpha/\alpha, R)$, Nastro 2: $(\alpha/\alpha, R)$. Rimani in $q_3$.
        \item \textbf{Uscita (Match di $A$ completato):} Nastro 2 legge $\Box$.
        \item \textbf{Transizione:} Da $q_3$ a $q_4$: Nastro 1: $(\alpha/\alpha, S)$, Nastro 2: $(\Box/\Box, S)$. (Nastro 1 è posizionato dopo la sottostringa, ora controlliamo il resto di B).
        \item \textbf{Uscita (Mismatch):} Nastro 1: $\alpha \in \{a,b\}$, Nastro 2: $\beta \in \{a,b\}$, con $\alpha \ne \beta$.
        \item \textbf{Transizione:} Da $q_3$ a $q_6$: Nastro 1: $(\alpha/\alpha, L)$, Nastro 2: $(\beta/\beta, L)$. (La testina del nastro 1 torna indietro di 1, poi entrambe si riavvolgeranno al punto di inizio per il prossimo tentativo).
    \end{itemize}
    \item[$q_4$ (Verifica fine di B):] Dopo aver trovato $A$ come sottostringa, si assicura che non ci siano simboli \texttt{\#} o altro sul resto di $B$.
    \begin{itemize}
        \item \textbf{Condizione:} Nastro 1: $\alpha \in \{a,b\}$.
        \item \textbf{Azione:} Nastro 1: $(\alpha/\alpha, R)$. Rimani in $q_4$.
        \item \textbf{Uscita:} Nastro 1 legge $\Box$.
        \item \textbf{Transizione:} Da $q_4$ a $q_5$: Nastro 1: $(\Box/\Box, S)$.
    \end{itemize}
    \item[$q_5$ (Accetta):] Stato di accettazione.
    \item[$q_6$ (Riavvolgi Nastro 1 e 2 dopo Mismatch):] Riporta Nastro 1 e Nastro 2 all'inizio della stringa $B$ e $A$ rispettivamente.
    \begin{itemize}
        \item \textbf{Condizione:} Nastro 1: $\gamma \in \{a,b,\#\}$, Nastro 2: $\gamma \in \{a,b,\Box\}$.
        \item \textbf{Azione:} Nastro 1: $(\gamma/\gamma, L)$, Nastro 2: $(\gamma/\gamma, L)$. Rimani in $q_6$. (Questo porta entrambe le testine indietro fino all'inizio di A e all'inizio di B).
        \item \textbf{Uscita:} Nastro 1 legge \texttt{\#}.
        \item \textbf{Transizione:} Da $q_6$ a $q_2$: Nastro 1: $(\#/\#, R)$, Nastro 2: $(\Box/\Box, R)$. (La testina del Nastro 1 si sposta a destra di un posto, la testina del Nastro 2 si posiziona all'inizio di A. Ora Nastro 1 è pronto per un nuovo tentativo di match in $B$ un carattere più avanti).
    \end{itemize}
\end{description}

\subsection{Linguaggio $L_5 = \{WW \mid W \in \{0,1\}^+\}$}
\begin{definition}
Il linguaggio $L_5$ è l'insieme di stringhe che sono la concatenazione di una stringa $W$ con sé stessa, dove $W$ è non vuota e composta da '0' o '1'.
\end{definition}
\begin{example}
Esempi: $00$, $0101$, $1111$.
Esempi non in $L_5$: $0110$ (non $WW$), $0$ (lunghezza dispari), $\epsilon$ (W non vuota).
\end{example}

\subsubsection{Strategia}
Il problema principale è identificare il punto centrale della stringa senza un delimitatore.
\begin{itemize}
    \item \textbf{Nastro 1 (Input):} Contiene la stringa di input.
    \item \textbf{Nastro 2 (Lavoro):} Utilizzato per determinare la lunghezza di $W$. Si marca un carattere sì e uno no per trovare la metà. Se la lunghezza totale è dispari, rifiuta.
    \item \textbf{Nastro 3 (Lavoro):} Copia la prima metà (il primo $W$).
\end{itemize}

\subsubsection{Implementazione (Stati e Transizioni)}
\begin{description}
    \item[$q_0$ (Determina metà lunghezza):] Scorre la stringa di input, scrivendo una 'X' sul Nastro 2 ogni due caratteri del Nastro 1. Questo identifica la metà della stringa.
    \begin{itemize}
        \item \textbf{Condizione (Pari):} Nastro 1: $\alpha \in \{0,1\}$, Nastro 2: $\Box$.
        \item \textbf{Azione:} Nastro 1: $(\alpha/\alpha, R)$, Nastro 2: $(\Box/X, R)$. Spostati a $q_1$. (Marca il primo carattere di una coppia).
        \item \textbf{Uscita (Nastro 1 finito, dispari):} Se Nastro 1 legge $\Box$ mentre Nastro 2 legge $\Box$, significa che la stringa era di lunghezza dispari. Rifiuta. (Non c'è transizione definita per Nastro 1: $\Box$, Nastro 2: $\Box$).
    \end{itemize}
    \item[$q_1$ (Salta un carattere):] Salta un carattere sul Nastro 1 senza scrivere sul Nastro 2.
    \begin{itemize}
        \item \textbf{Condizione:} Nastro 1: $\alpha \in \{0,1\}$, Nastro 2: $\Box$.
        \item \textbf{Azione:} Nastro 1: $(\alpha/\alpha, R)$, Nastro 2: $(\Box/\Box, R)$. Rimani in $q_0$. (Va a marcare il prossimo carattere della coppia, o si ferma se la stringa finisce).
        \item \textbf{Uscita (Nastro 1 finito, pari):} Se Nastro 1 legge $\Box$ e Nastro 2 legge $X$, significa che la stringa era di lunghezza pari.
        \item \textbf{Transizione:} Da $q_1$ a $q_2$: Nastro 1: $(\Box/\Box, S)$, Nastro 2: $(X/X, S)$. (Nastro 1 si è fermato alla fine, Nastro 2 sulla sua ultima X).
    \end{itemize}
    \item[$q_2$ (Riavvolgi Nastro 1 e 2):] Riporta le testine del Nastro 1 e 2 all'inizio delle stringhe.
    \begin{itemize}
        \item \textbf{Condizione (Nastro 2):} Nastro 2: $X$.
        \item \textbf{Azione:} Nastro 2: $(X/X, L)$. Rimani in $q_2$.
        \item \textbf{Uscita (Nastro 2 finito):} Nastro 2 legge $\Box$.
        \item \textbf{Transizione:} Da $q_2$ a $q_3$: Nastro 2: $(\Box/\Box, R)$.
        \item \textbf{Condizione (Nastro 1):} Nastro 1: $\alpha \in \{0,1\}$.
        \item \textbf{Azione:} Nastro 1: $(\alpha/\alpha, L)$. Rimani in $q_3$.
        \item \textbf{Uscita (Nastro 1 finito):} Nastro 1 legge $\Box$.
        \item \textbf{Transizione:} Da $q_3$ a $q_4$: Nastro 1: $(\Box/\Box, R)$.
    \end{itemize}
    \item[$q_4$ (Copia $W_1$ su Nastro 3):] Copia la prima metà della stringa (i primi $|W|$ caratteri) dal Nastro 1 al Nastro 3, cancellando le 'X' dal Nastro 2.
    \begin{itemize}
        \item \textbf{Condizione:} Nastro 1: $\alpha \in \{0,1\}$, Nastro 2: $X$, Nastro 3: $\Box$.
        \item \textbf{Azione:} Nastro 1: $(\alpha/\alpha, R)$, Nastro 2: $(X/\Box, R)$, Nastro 3: $(\Box/\alpha, R)$. Rimani in $q_4$.
        \item \textbf{Uscita:} Nastro 2 legge $\Box$. (Abbiamo copiato esattamente metà della stringa).
        \item \textbf{Transizione:} Da $q_4$ a $q_5$: Nastro 2: $(\Box/\Box, S)$.
    \end{itemize}
    \item[$q_5$ (Riavvolgi Nastro 3):] Riporta la testina del Nastro 3 all'inizio della stringa $W_1$.
    \begin{itemize}
        \item \textbf{Condizione:} Nastro 3: $\alpha \in \{0,1\}$.
        \item \textbf{Azione:} Nastro 3: $(\alpha/\alpha, L)$. Rimani in $q_5$.
        \item \textbf{Uscita:} Nastro 3 legge $\Box$.
        \item \textbf{Transizione:} Da $q_5$ a $q_6$: Nastro 3: $(\Box/\Box, R)$.
    \end{itemize}
    \item[$q_6$ (Confronta $W_2$ con Nastro 3):] Confronta la seconda metà della stringa sul Nastro 1 con la copia della prima metà sul Nastro 3.
    \begin{itemize}
        \item \textbf{Condizione:} Nastro 1: $\alpha \in \{0,1\}$, Nastro 3: $\alpha \in \{0,1\}$.
        \item \textbf{Azione:} Nastro 1: $(\alpha/\alpha, R)$, Nastro 3: $(\alpha/\alpha, R)$. Rimani in $q_6$.
        \item \textbf{Uscita:} Nastro 1 e Nastro 3 leggono $\Box$.
        \item \textbf{Transizione:} Da $q_6$ a $q_{acc}$: Nastro 1: $(\Box/\Box, S)$, Nastro 3: $(\Box/\Box, S)$.
    \end{itemize}
    \item[Rifiuto:] Qualsiasi incongruenza (lunghezza dispari, simboli non corrispondenti durante il confronto) porta al rifiuto.
\end{description}

\subsection{Esercizio Aggiuntivo per Casa}
\begin{definition}
Sia $L_6 = \{W\#A \mid W,A \in \{0,1\}^+, A=W \text{ OR } A=W^R\}$
\end{definition}
Il linguaggio $L_6$ include stringhe $W\#A$ dove $W$ e $A$ sono stringhe non vuote di '0' o '1'. La stringa $A$ deve essere identica a $W$ oppure essere la sua inversa ($W^R$).


% =====================================================
% --- START LECTURE 08 ---
% =====================================================

\chapter{Macchine di Turing Non Deterministiche e Classi di Computabilità}



\section{Introduzione alle Macchine di Turing Non Deterministiche}

Nella lezione precedente abbiamo esplorato le macchine di Turing multinastro, constatando che, sebbene più pratiche da programmare, sono equivalenti in termini di potenza computazionale alle macchine di Turing mononastro. La simulazione di una macchina multinastro su una mononastro ha un costo polinomiale (quadratico), il che significa che non cambia l'ordine di complessità degli algoritmi. Questo ci permette di utilizzare le macchine multinastro per semplicità, sapendo che il risultato in termini di calcolabilità (e classi di complessità polinomiale) rimane invariato.

Oggi introduciamo un nuovo modello di macchina di Turing: la \textbf{Macchina di Turing Non Deterministica (NDTM)}. Questo modello sarà fondamentale per il resto del corso per la sua praticità.

\begin{example}[Una Macchina di Turing Non Deterministica]
Consideriamo la seguente macchina di Turing (parzialmente descritta):
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,
                    semithick,state/.style={circle,draw,initial text=}]

  \node[state] (q0) {$q_0$};
  \node[state, right of=q0] (q1) {$q_1$};
  \node[state, right of=q1, accepting] (q2) {$q_2$};

  \path (q0) edge [loop left] node {$0/1, R$} (q0);
  \path (q0) edge node {$1/0, R$} (q1);

  % Non-determinismo
  \path (q1) edge [loop left] node {$0/0, L$} (q1);
  \path (q1) edge node {$0/0, L$} (q0); % Questo è il punto di non-determinismo

  \path (q1) edge [loop right] node {$1/1, R$} (q1);
  \path (q1) edge node {$\sqcup/\sqcup, R$} (q2);

  \path (q2) edge [loop right] node {$0/0, R$} (q2); % Aggiunta per completezza
  \path (q2) edge [loop right] node {$1/1, R$} (q2); % Aggiunta per completezza
\end{tikzpicture}
\end{center}
Questa macchina presenta una peculiarità nello stato $q_1$: se legge il simbolo $0$, ha due possibili transizioni:
\begin{enumerate}
    \item Rimanere in $q_1$, scrivendo $0$ e spostandosi a sinistra ($q_1 \xrightarrow{0/0, L} q_1$).
    \item Spostarsi in $q_0$, scrivendo $0$ e spostandosi a sinistra ($q_1 \xrightarrow{0/0, L} q_0$).
\end{enumerate}
In un dato stato e con un dato simbolo letto, la macchina ha più di una scelta per il prossimo passo. Questo comportamento è ciò che definisce il \textbf{non-determinismo}.
\end{example}

Le macchine viste finora (mononastro, multinastro, multitraccia) erano tutte \textbf{deterministich}, ovvero per ogni coppia (stato, simbolo letto) esiste un solo passo successivo possibile.

\section{Definizione Formale di Macchina di Turing Non Deterministica}

Una \textbf{Macchina di Turing Non Deterministica (NDTM)} $N$ è una tupla $N = \langle \Sigma, \Gamma, \sqcup, Q, q_0, F, \delta \rangle$, dove:
\begin{itemize}
    \item $\Sigma$: l'alfabeto di input (simboli che possono essere letti sull'input iniziale del nastro).
    \item $\Gamma$: l'alfabeto di nastro (tutti i simboli che la macchina può maneggiare sul nastro, con $\Sigma \subset \Gamma$).
    \item $\sqcup$: il simbolo di blank, $\sqcup \in \Gamma \setminus \Sigma$.
    \item $Q$: l'insieme finito degli stati della macchina.
    \item $q_0$: lo stato iniziale, $q_0 \in Q$.
    \item $F$: l'insieme degli stati finali (o accettanti), $F \subseteq Q$.
    \item $\delta$: la \textbf{funzione di transizione} (o relazione di transizione). Per una NDTM, $\delta$ mappa una coppia (stato, simbolo letto) a un \textbf{insieme} di possibili prossimi passi:
    \[ \delta: Q \times \Gamma \to \mathcal{P}(Q \times \Gamma \times \{L, R\}) \]
    dove $\mathcal{P}(S)$ denota l'insieme delle parti di $S$ (l'insieme di tutti i possibili sottoinsiemi di $S$). Questo significa che per una data coppia $(q, a)$, $\delta(q, a)$ restituisce un insieme di triple $(q', b, D)$, dove $q'$ è il nuovo stato, $b$ il simbolo da scrivere, e $D$ la direzione di movimento della testina. La cardinalità di questo insieme può essere maggiore di uno.

    \begin{example}
    Per l'esempio precedente, la transizione non deterministica è formalmente descritta come:
    \[ \delta(q_1, 0) = \{ (q_0, 0, L), (q_1, 0, R) \} \]
    \end{example}
    Non è necessario che la funzione di transizione sia non deterministica per tutte le coppie $(q,a)$; basta che lo sia per almeno una coppia.
\end{itemize}

\section{Computazione di una Macchina di Turing Non Deterministica}

\subsection{Configurazioni e Albero di Computazione}
Le \textbf{configurazioni} (o descrizioni istantanee) di una NDTM sono definite in modo analogo a quelle delle DTM: una stringa che cattura il contenuto corrente del nastro, lo stato della macchina e la posizione della testina.
Una configurazione iniziale è $q_0W$, dove $W$ è la stringa di input.
Una configurazione è finale se non ammette configurazioni successive. Una configurazione finale è accettante se lo stato in cui si trova la macchina è uno stato accettante ($q \in F$).

La principale differenza è che una data configurazione può avere più "successori legali". Questo porta a organizzare la sequenza delle configurazioni non più come una lista, ma come un \textbf{albero di computazione (Computation Tree)}.

\begin{definition}[Computation Tree]
Il \textbf{computation tree} di una macchina di Turing non deterministica $M$ su una stringa di input $W$ è un albero i cui nodi sono tutte le possibili configurazioni in cui la macchina $M$ può trovarsi processando $W$.
\begin{itemize}
    \item La radice dell'albero è la configurazione iniziale $q_0W$.
    \item C'è un arco da una configurazione $\alpha$ a una configurazione $\beta$ se $\beta$ è uno dei successori legali di $\alpha$.
\end{itemize}
\end{definition}

\begin{example}[Computazione di $N$ su input $01$]
Usiamo l'esempio di NDTM di cui sopra e la stringa di input $W = 01$.
La configurazione iniziale è $q_001$.
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.8\textwidth]{pag1_computation_tree.jpeg}
%     \caption{Albero di Computazione della NDTM su input $01$}
% \end{figure}
Analizziamo il processo passo-passo:
\begin{enumerate}
    \item Iniziamo da $q_001$.
    \item Da $q_0$ leggendo $0$: si va a $q_0$, si scrive $1$, si sposta a destra. Nuova configurazione: $1q_01$.
    \item Da $q_0$ leggendo $1$: si va a $q_1$, si scrive $0$, si sposta a destra. Nuova configurazione: $10q_1\sqcup$.
    \item Da $q_1$ leggendo $\sqcup$: c'è il non-determinismo (in realtà l'esempio della lezione aveva $q_1$ che leggeva $0$, ma nel grafico è $\sqcup$, seguiamo il grafico per la computazione e l'esempio della $\delta(q_1,0)$). Il professore ha disegnato la freccia per $(q_1, \sqcup)$ in $q_2$. Rivediamo l'esempio del disegno, $q_1$ legge $0$, sposta a destra. C'è un arco da $q_1$ con $1/1,R$ che cicla in $q_1$. Poi da $q_1$ con $\sqcup/\sqcup,R$ va a $q_2$.
    *   **Rivediamo l'esempio del professore basandoci sul disegno e l'input 01:**
        *   Iniziale: $\mathbf{q_0}01$
        *   $q_0,0 \to (q_0,1,R)$: $1\mathbf{q_0}1$
        *   $q_0,1 \to (q_1,0,R)$: $10\mathbf{q_1}\sqcup$
        *   Ora siamo in $10q_1\sqcup$. La testina è sul blank. Dal diagramma, $q_1$ leggendo blank porta a $q_2$ scrivendo blank e spostandosi a destra.
        *   $q_1,\sqcup \to (q_2,\sqcup,R)$: $10\sqcup\mathbf{q_2}\sqcup$ (questa è una configurazione accettante, in quanto $q_2 \in F$).
    *   **Rivediamo invece l'esempio di non-determinismo che il professore ha esplicitamente discusso $\delta(q_1,0)$:**
        *   Configurazione $q_001$
        *   $q_0,0 \to (q_0,1,R)$: $1q_01$
        *   $q_0,1 \to (q_1,0,R)$: $10q_1\sqcup$ (Questo è il punto di divergenza tra il diagramma mostrato e l'input $01$ effettivo, in quanto dopo $10q_1\sqcup$ la testina è sul blank, non sul $0$ per attivare il non-determinismo).
        *   \textbf{Per allineare l'esempio alla discussione del professore, dobbiamo immaginare un input diverso che porti a $\mathbf{q_1}0...$}: Il professore usa \texttt{10q10} nell'esempio sull'albero, non \texttt{10q1\_blank\_}. Questo implica che l'input doveva essere più lungo.
        *   **Seguiamo l'esempio disegnato dal professore, che mostra la configurazione $10q_10$ come punto di non-determinismo**:
            *   Siamo in $10\mathbf{q_1}0$. Testina su $0$, stato $q_1$.
            *   $\delta(q_1, 0) = \{ (q_0, 0, L), (q_1, 0, R) \}$
            *   \textbf{Primo branch:} $(q_0, 0, L)$. Scriviamo $0$, spostiamo a sinistra, andiamo in $q_0$.
                Configurazione successiva: $1\mathbf{q_0}00$.
                Da $1\mathbf{q_0}00$: $q_0,0 \to (q_0,1,R)$. Scriviamo $1$, spostiamo a destra. $11\mathbf{q_0}0$.
                Da $11\mathbf{q_0}0$: $q_0,0 \to (q_0,1,R)$. Scriviamo $1$, spostiamo a destra. $111\mathbf{q_0}\sqcup$.
                Da $111\mathbf{q_0}\sqcup$: $q_0,\sqcup \to (q_2,\sqcup,R)$. (Non specificato nel disegno iniziale, assumiamo). Se non c'è transizione, si blocca e rifiuta. Nell'esempio disegnato, questo ramo è bloccato e non accettante (X).
            *   \textbf{Secondo branch:} $(q_1, 0, R)$. Scriviamo $0$, spostiamo a destra, andiamo in $q_1$.
                Configurazione successiva: $100\mathbf{q_1}\sqcup$.
                Da $100\mathbf{q_1}\sqcup$: $q_1,\sqcup \to (q_2,\sqcup,R)$. Scriviamo $\sqcup$, spostiamo a destra.
                Configurazione successiva: $100\sqcup\mathbf{q_2}\sqcup$. (Questa è accettante, $q_2 \in F$).
        *   L'esempio nel disegno ha un ramo ulteriore: $10\mathbf{q_1}0 \to 10\mathbf{q_1}0$ (ciclo su $0/0,L$) $\to \dots$. Questo dimostra che possono esserci più rami e che un ramo può anche andare in loop.

\end{enumerate}
\end{example}

\subsection{Condizione di Accettazione per NDTM}

Una macchina di Turing non deterministica $M$ \textbf{accetta} un input $W$ se e solo se all'interno del computation tree di $M$ su $W$ \textbf{esiste almeno una configurazione accettante}.

Ciò significa che la macchina non deve trovare \emph{tutti} i cammini accettanti, ne basta uno. Se la macchina ha un modo per accettare, allora accetta.
Perché una NDTM \textbf{rifiuti} un input, deve succedere che \emph{tutte} le computazioni all'interno del computation tree o terminano in una configurazione non accettante, o non terminano mai (loop).

\section{Simulazione di NDTM tramite DTM}

Una domanda fondamentale è: le Macchine di Turing Non Deterministiche sono più potenti delle Macchine di Turing Deterministiche (DTM) in termini di capacità di calcolo? Ovvero, possono calcolare funzioni o accettare linguaggi che le DTM non possono? La risposta è \textbf{No}.

\begin{theorem}
Per ogni linguaggio $L$ accettato da una macchina di Turing non deterministica $N$, esiste una macchina di Turing deterministica $M$ che accetta $L$.
\end{theorem}
Questo teorema è cruciale, perché significa che, anche se le NDTM non sono fisicamente realizzabili (non "sanno" quale ramo scegliere o non si "sdoppiano"), possiamo usarle come modello di calcolo astratto perché tutto ciò che possono fare può essere fatto anche da una DTM.

\subsection{Strategia di Simulazione (BFS)}
Per dimostrare il teorema, è necessario mostrare come una DTM possa simulare una NDTM. La strategia comune è una \textbf{ricerca in ampiezza (Breadth-First Search - BFS)} sull'albero di computazione della NDTM.

Supponiamo di avere una NDTM $N$ (che possiamo assumere mononastro, poiché sappiamo convertire multinastro in mononastro). Vogliamo costruire una DTM $M$ (multinastro per facilità, poi riconvertibile a mononastro) che simuli $N$.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.8\textwidth]{pag2_DTM_simulation.jpeg}
%     \caption{Simulazione BFS di NDTM su DTM}
% \end{figure}

La macchina $M$ utilizzerà più nastri. Una configurazione tipica per la simulazione è l'uso di tre nastri:
\begin{enumerate}
    \item \textbf{Nastro 1 (Input/Originale)}: Contiene l'input originale $W$ e non viene modificato.
    \item \textbf{Nastro 2 (Simulazione)}: Contiene la configurazione corrente di $N$ che $M$ sta simulando.
    \item \textbf{Nastro 3 (Coda delle configurazioni)}: Contiene una coda di configurazioni di $N$ che devono ancora essere esplorate, separate da un simbolo speciale (es. \texttt{*}).
\end{enumerate}

\textbf{Algoritmo di Simulazione (BFS):}
\begin{enumerate}
    \item $M$ inizializza il Nastro 3 scrivendo la configurazione iniziale di $N$ su $W$ ($q_0W$).
    \item $M$ entra in un ciclo di esplorazione:
        \begin{enumerate}
            \item Prende la prima configurazione $ID_k$ dal Nastro 3. Se il Nastro 3 è vuoto, $M$ rifiuta (non ha trovato alcun percorso accettante) e si ferma.
            \item Copia $ID_k$ sul Nastro 2.
            \item $M$ simula un singolo passo della NDTM $N$ a partire da $ID_k$ (Nastro 2).
                \begin{itemize}
                    \item Se $ID_k$ è una configurazione finale accettante (lo stato è in $F$), $M$ accetta $W$ e si ferma.
                    \item Se $ID_k$ è una configurazione finale non accettante (si blocca o lo stato non è in $F$), $M$ scarta questo ramo e continua con il passo (a).
                    \item Se $ID_k$ ammette $k'$ successori legali (data la $\delta$ di $N$, $M$ sa quanti e quali sono), $M$ genera questi $k'$ nuovi $ID$ e li aggiunge in coda al Nastro 3, separati da asterischi o altri delimitatori.
                \end{itemize}
        \end{enumerate}
\end{enumerate}

Questa strategia garantisce che $M$ accetti $W$ se e solo se $N$ accetta $W$. La BFS è cruciale perché impedisce a $M$ di bloccarsi in un ramo infinito non accettante, garantendo che se un cammino accettante esiste, $M$ lo troverà prima o poi (a meno che l'albero intero non sia infinito e senza cammini accettanti).

\subsection{Costo della Simulazione}

Il costo di questa simulazione è \textbf{esponenziale}. Se la NDTM $N$ compie $K$ passi per accettare (ovvero il cammino accettante più breve ha lunghezza $K$), e il fattore di branching massimo della NDTM è $C$ (cioè, ogni configurazione può avere al massimo $C$ successori), allora:
\begin{itemize}
    \item Al livello 0: 1 configurazione.
    \item Al livello 1: $C$ configurazioni.
    \item Al livello 2: $C^2$ configurazioni.
    \item Al livello $K$: $C^K$ configurazioni.
\end{itemize}
La DTM $M$ deve esplorare tutti i nodi fino al livello $K$ per trovare il cammino accettante (nella BFS), o tutti i nodi fino a un certo punto per esaurire le opzioni. Il numero di configurazioni da esplorare cresce esponenzialmente con la lunghezza del cammino più breve ($K$). Quindi, il tempo di esecuzione di $M$ è $O(C^K)$.
Questo significa che c'è un \textbf{gap esponenziale} tra la velocità di una NDTM e quella di una DTM che la simula. Se un problema può essere risolto da una NDTM in tempo polinomiale, la DTM che lo simula potrebbe richiedere tempo esponenziale. Questo è il cuore del famoso problema \textbf{P vs NP}.

Non si sa se esista una simulazione più efficiente (es. polinomiale) delle NDTM su DTM. Nessuno è riuscito a trovarla, né a dimostrare che non esista.

\section{La Tesi di Church-Turing}

Abbiamo esaminato vari modelli di calcolo: DTM mononastro, DTM multitraccia, DTM multinastro, NDTM. Tutti questi modelli, pur variando in efficienza, hanno la stessa potenza computazionale: possono calcolare lo stesso insieme di funzioni e accettare lo stesso insieme di linguaggi.

Negli anni '30, quando Alan Turing e altri svilupparono i loro modelli di calcolo (es. $\lambda$-calcolo di Alonzo Church, sistemi di Post), si scoprì che tutti i modelli conosciuti erano computazionalmente equivalenti alle Macchine di Turing. Questo portò alla formulazione della \textbf{Tesi di Church-Turing}:

\begin{theorem}[Tesi di Church-Turing]
\emph{Tutto ciò che è calcolabile è calcolabile da una macchina di Turing.}
\end{theorem}
È chiamata "tesi" e non "teorema" perché non è una dimostrazione formale, ma un'affermazione riguardante la natura della computazione. Non si definisce la calcolabilità a priori e poi si dimostra che la MT la raggiunge, ma si propone che la MT catturi il concetto intuitivo di calcolabilità. Ad oggi, nessun modello di calcolo più potente è stato scoperto o inventato, il che rafforza la fiducia in questa tesi.

\section{Classi di Computabilità}

Sulla base della Tesi di Church-Turing, possiamo definire le principali classi di problemi (o linguaggi) in base alla loro calcolabilità da parte di una Macchina di Turing.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=0.6\textwidth]{pag2_computability_classes.jpeg}
%     \caption{Classi di Computabilità: R e RE}
% \end{figure}

\subsection{Classe R (Linguaggi Ricorsivi / Problemi Decidibili)}
\begin{definition}[Classe R]
La classe $R$ (linguaggi \textbf{ricorsivi}) contiene tutti i linguaggi $L$ per i quali esiste una Macchina di Turing $M$ che \textbf{decide} $L$.
\end{definition}
Una Macchina di Turing $M$ \textbf{decide} un linguaggio $L$ se per ogni input $W \in \Sigma^*$:
\begin{itemize}
    \item Se $W \in L$, $M$ si arresta in uno stato accettante.
    \item Se $W \notin L$, $M$ si arresta in uno stato non accettante.
\end{itemize}
In altre parole, una macchina che decide un linguaggio \textbf{termina sempre} per ogni input, dando una risposta definitiva (sì o no). I problemi che corrispondono ai linguaggi in $R$ sono chiamati \textbf{problemi decidibili}.

\subsection{Classe RE (Linguaggi Ricorsivamente Enumerabili / Problemi Accettabili)}
\begin{definition}[Classe RE]
La classe $RE$ (linguaggi \textbf{ricorsivamente enumerabili}) contiene tutti i linguaggi $L$ per i quali esiste una Macchina di Turing $M$ che \textbf{accetta} $L$.
\end{definition}
Una Macchina di Turing $M$ \textbf{accetta} un linguaggio $L$ se per ogni input $W \in \Sigma^*$:
\begin{itemize}
    \item Se $W \in L$, $M$ si arresta in uno stato accettante.
    \item Se $W \notin L$, $M$ \emph{potrebbe non arrestarsi mai} o arrestarsi in uno stato non accettante.
\end{itemize}
I problemi che corrispondono ai linguaggi in $RE$ sono chiamati \textbf{problemi accettabili}.

\subsection{Relazione tra R e RE}
È chiaro dalla definizione che ogni linguaggio che può essere deciso può anche essere accettato (una macchina che termina sempre e dà una risposta è anche una macchina che accetta). Quindi, $R \subseteq RE$.
In realtà, $R \subset RE$, ovvero esistono linguaggi che sono accettabili ma non decidibili. Questi problemi sono chiamati \textbf{problemi indecidibili}.
A volte, i linguaggi in $RE \setminus R$ (cioè, i linguaggi accettabili ma non decidibili) sono chiamati \textbf{semidecidibili}. Per questi problemi, se la risposta è "sì", l'algoritmo si ferma e lo comunica. Ma se la risposta è "no", l'algoritmo potrebbe non fermarsi mai, lasciandoci nell'incertezza sulla risposta. Questo li rende problematici per l'uso pratico.

Con queste definizioni, abbiamo stabilito un quadro per classificare i problemi in base alla loro calcolabilità da parte delle Macchine di Turing.


% =====================================================
% --- START LECTURE 09 ---
% =====================================================

\chapter{Macchine di Turing Non Deterministiche: Esercitazioni}



\section{Introduzione alle Macchine di Turing Non Deterministiche (NDTM)}

Riprendiamo dalla discussione sulle Macchine di Turing Non Deterministiche (NDTM).
Una caratteristica fondamentale delle NDTM è la loro capacità di \texttt{"indovinare"} o \texttt{"guessare"} porzioni di stringa o decisioni computazionali. Questa non è una capacità intrinseca della macchina di indovinare nel senso umano, né implica una computazione parallela di tutte le possibilità. È una metafora per descrivere il fatto che se esiste almeno una sequenza di scelte che porta all'accettazione, allora la macchina accetterà.

Tuttavia, per garantire che una NDTM accetti solo le stringhe che appartengono al linguaggio desiderato, la fase di \texttt{guess} deve essere sempre seguita da una fase di \texttt{check} (controllo). La fase di \texttt{check} è cruciale per filtrare le \texttt{"scelte sbagliate"} fatte dalla macchina non deterministica, assicurando che solo le computazioni corrette portino a uno stato di accettazione.

Un esempio pratico di questa capacità delle NDTM è la possibilità di scrivere in anticipo su un nastro ausiliario delle stringhe che saranno necessarie in un momento successivo della computazione. Questo comportamento è particolarmente utile quando si analizzano le classi di complessità computazionale (es. NP).

\section{Esercizi}

Analizziamo alcuni esercizi per applicare i concetti delle NDTM.

\subsection{Esercizio 1: Sottostringa Palindroma di Lunghezza Variabile}

\begin{definition}[Linguaggio $L_1$]
Sia $L_1$ il linguaggio definito come:
$L_1 = \{ X^N \# W_1 \# W_2 \# \dots \# W_N \mid N > 0, W_i \in \{a,b,c,d\}^+, \forall i \in [1, N], \exists S_i \subseteq W_i \text{ t.c. } |S_i| = i \text{ e } S_i = S_i^R \}$
dove $X^N$ indica $N$ occorrenze del simbolo $X$.
\end{definition}

\subsubsection{Strategia della Macchina di Turing}
La macchina utilizzerà cinque nastri:
\begin{itemize}
    \item \textbf{Nastro 1 (Input):} Contiene la stringa di input.
    \item \textbf{Nastro 2 (Conteggio $N$):} Per memorizzare il numero di blocchi $W_i$.
    \item \textbf{Nastro 3 (Conteggio $i$):} Per memorizzare il valore corrente dell'indice $i$.
    \item \textbf{Nastro 4 (Guess $S_i$):} Per scrivere la stringa $S_i$ \texttt{"indovinata"}.
    \item \textbf{Nastro 5 (Guess $S_i$ per $S_i^R$):} Una copia di $S_i$ per il controllo di palindromia.
\end{itemize}

L'idea principale è che per ogni $W_i$, la macchina non deterministicamente \texttt{"indovina"} la stringa $S_i$ sul Nastro 4 e 5. Successivamente, verifica che $S_i$ sia palindroma (confrontando Nastro 4 e Nastro 5) e che sia effettivamente una sottostringa di $W_i$ (confrontando Nastro 4 con il Nastro 1). La lunghezza di $S_i$ viene controllata usando il Nastro 3.

\subsubsection{Descrizione degli Stati e delle Transizioni}
Sia $\Sigma_I = \{X, \#, a,b,c,d\}$ l'alfabeto di input e $\Gamma_T = \Sigma_I \cup \{\B, \text{ausiliari}\}$ l'alfabeto del nastro. Usiamo $\alpha$ per un simbolo generico da $\{a,b,c,d\}$.

\begin{itemize}
    \item \textbf{Stato $Q_0$ (Inizio):}
        \begin{itemize}
            \item \textbf{Transizione per $X^N$:} Legge $X$ dal Nastro 1, lo riscrive e si muove a destra. Scrive $X$ sul Nastro 2 (inizialmente vuoto) e si muove a destra. Passa allo stato $Q_1$. Questo ciclo si ripete per tutti gli $X$ iniziali.
            \item $(X, \B, \B, \B, \B) \to (X, X, \B, \B, \B), (R,R,S,S,S)$
        \end{itemize}
    \item \textbf{Stato $Q_1$ (Copia $N$ e Inizio $W_1$):}
        \begin{itemize}
            \item \textbf{Transizione per \texttt{\#}:} Quando legge $\#$ sul Nastro 1, lo riscrive e si muove a destra (posizionandosi all'inizio di $W_1$). Sul Nastro 2 (che contiene gli $X$ per $N$), legge un $X$, lo cancella ($\B$) e si muove a sinistra (si posizione sull'ultimo $X$ restante o $\B$). Sul Nastro 3 (inizialmente vuoto), scrive $X$ e si muove a destra (questo inizia il conteggio di $i=1$). Passa a $Q_2$.
            \item $(\#, X, \B, \B, \B) \to (\#, \B, X, \B, \B), (R,L,R,S,S)$
        \end{itemize}
    \item \textbf{Stato $Q_2$ (Guess $S_i$):}
        Questo stato gestisce la generazione non deterministica di $S_i$ sui nastri 4 e 5. Il loop avviene basandosi sul Nastro 3 (che contiene $i$).
        \begin{itemize}
            \item \textbf{Transizione (Loop per guess $S_i$):}
                Mentre Nastro 3 contiene $X$ (cioè, $S_i$ non ha ancora raggiunto la lunghezza $i$):
                \begin{itemize}
                    \item Nastro 1: Legge $\alpha$ (qualsiasi simbolo di input), lo riscrive e si muove a destra (continua a leggere $W_i$).
                    \item Nastro 2: Non modificato.
                    \item Nastro 3: Legge $X$, lo riscrive e si muove a destra (avanza nel conteggio di $i$).
                    \item Nastro 4: Legge $\B$, scrive $\alphaSym$ (un simbolo non deterministico dall'alfabeto di input) e si muove a destra.
                    \item Nastro 5: Legge $\B$, scrive $\alphaSym$ (lo stesso simbolo di Nastro 4) e si muove a destra.
                \end{itemize}
                Questo loop rimane in $Q_3$ (si passa a $Q_3$ nella descrizione, il diagramma usa $Q_2 \to Q_3 \to Q_3$).
                \item $(\alpha, \text{any}, X, \B, \B) \to (\alpha, \text{any}, X, \alphaSym, \alphaSym), (R,S,R,R,R)$ (Questo $\alpha$ su Nastro 1 è solo lettura, $\alphaSym$ è il simbolo guessed).
            \item \textbf{Transizione (Fine guess $S_i$):}
                Quando Nastro 3 legge $\B$ (ha scritto $i$ simboli su Nastro 4 e 5):
                \begin{itemize}
                    \item Nastro 1: Legge $\alpha$, lo riscrive e si muove a destra.
                    \item Nastro 2: Non modificato.
                    \item Nastro 3: Legge $\B$, lo riscrive e si muove a sinistra (riavvolge il conteggio di $i$).
                    \item Nastro 4: Legge $\B$, lo riscrive e si muove a sinistra (riavvolge Nastro 4).
                    \item Nastro 5: Legge $\B$, lo riscrive e si muove a sinistra (riavvolge Nastro 5).
                \end{itemize}
                Passa a $Q_4$.
                \item $(\alpha, \text{any}, \B, \B, \B) \to (\alpha, \text{any}, \B, \B, \B), (R,S,L,L,L)$
        \end{itemize}
    \item \textbf{Stato $Q_4$ (Riavvolgimento):}
        Questo stato riavvolge i nastri 3, 4 e 5 per prepararsi al controllo. Si continua a muoversi a sinistra su Nastro 3, 4 e 5 finché non si raggiunge il $\B$ iniziale.
        \begin{itemize}
            \item \textbf{Transizione (Loop di riavvolgimento):}
                Mentre Nastro 4 non è $\B$:
                \begin{itemize}
                    \item Nastro 1: Ignorato (ma continua a leggere $W_i$).
                    \item Nastro 2: Ignorato.
                    \item Nastro 3: Legge $X$, lo riscrive, si muove a sinistra.
                    \item Nastro 4: Legge $\alphaSym$, lo riscrive, si muove a sinistra.
                    \item Nastro 5: Legge $\alphaSym$, lo riscrive, si muove a sinistra.
                \end{itemize}
                Questo loop rimane in $Q_4$.
                \item $(\text{any}, \text{any}, X, \alphaSym, \alphaSym) \to (\text{any}, \text{any}, X, \alphaSym, \alphaSym), (S,S,L,L,L)$
            \item \textbf{Transizione (Fine riavvolgimento):}
                Quando Nastro 4 e Nastro 5 leggono $\B$ (sono all'inizio di $S_i$):
                \begin{itemize}
                    \item Nastro 1: Ignorato.
                    \item Nastro 2: Ignorato.
                    \item Nastro 3: Legge $\B$, lo riscrive, si muove a destra.
                    \item Nastro 4: Legge $\B$, lo riscrive, si muove a destra.
                    \item Nastro 5: Legge $\B$, lo riscrive, si muove a destra (posiziona il capo del Nastro 5 sulla fine di $S_i$ per il reverse check).
                \end{itemize}
                Passa a $Q_5$.
                \item $(\text{any}, \text{any}, \B, \B, \B) \to (\text{any}, \text{any}, \B, \B, \B), (S,S,R,R,R)$
        \end{itemize}
    \item \textbf{Stato $Q_5$ (Check $S_i$ e $S_i^R$ in $W_i$):}
        Questo stato non deterministicamente cerca l'inizio di $S_i$ in $W_i$ sul Nastro 1 e contemporaneamente verifica la palindromia di $S_i$ tra Nastro 4 (leggendo in avanti) e Nastro 5 (leggendo all'indietro).
        \begin{itemize}
            \item \textbf{Transizione (Salto in $W_i$):}
                Nondeterministicamente salta caratteri in $W_i$ sul Nastro 1 fino a trovare un possibile inizio di $S_i$.
                \begin{itemize}
                    \item Nastro 1: Legge $\alpha$, lo riscrive, si muove a destra.
                \end{itemize}
                Loop su $Q_5$.
                \item $(\alpha, \text{any}, \text{any}, \text{any}, \text{any}) \to (\alpha, \text{any}, \text{any}, \text{any}, \text{any}), (R,S,S,S,S)$
            \item \textbf{Transizione (Confronto e Palindromia):}
                Quando si decide di iniziare il confronto:
                \begin{itemize}
                    \item Nastro 1: Legge $\alphaSym$, lo riscrive, si muove a destra (confronta con $S_i$).
                    \item Nastro 4: Legge $\alphaSym$, lo cancella ($\B$), si muove a destra (consuma $S_i$).
                    \item Nastro 5: Legge $\alphaSym$, lo cancella ($\B$), si muove a sinistra (consuma $S_i^R$).
                \end{itemize}
                Passa a $Q_6$. Questo loop continua in $Q_6$.
                \item $(\alphaSym, \text{any}, \text{any}, \alphaSym, \alphaSym) \to (\alphaSym, \text{any}, \text{any}, \B, \B), (R,S,S,R,L)$
        \end{itemize}
    \item \textbf{Stato $Q_6$ (Continuazione del Check):}
        \begin{itemize}
            \item \textbf{Transizione (Fine del confronto):}
                Quando Nastro 4 e Nastro 5 leggono $\B$ (hanno verificato tutta $S_i$):
                \begin{itemize}
                    \item Nastro 1: Ignorato.
                    \item Nastro 4: Legge $\B$, lo riscrive, si ferma.
                    \item Nastro 5: Legge $\B$, lo riscrive, si ferma.
                \end{itemize}
                Passa a $Q_7$.
                \item $(\text{any}, \text{any}, \text{any}, \B, \B) \to (\text{any}, \text{any}, \text{any}, \B, \B), (S,S,S,S,S)$
        \end{itemize}
    \item \textbf{Stato $Q_7$ (Controllo Prossima $W_i$ o Accettazione):}
        \begin{itemize}
            \item \textbf{Transizione (Prossima $W_i$):}
                Se Nastro 2 contiene ancora $X$ (ci sono altre $W_i$ da processare):
                \begin{itemize}
                    \item Nastro 1: Continua a leggere $\alpha$ e si sposta a destra (fino a fine $W_i$ o $\#$).
                    \item Nastro 2: Legge $X$, lo riscrive, si ferma (la cancellazione avverrà tornando a $Q_1$).
                    \item Nastro 3: Riavvolge a sinistra fino a $\B$.
                \end{itemize}
                Questa è una transizione composta:
                $\{ (\alpha, X, \alpha, \B, \B) \to (\alpha, X, \alpha, \B, \B), (R,S,L,S,S) \}$ (Loop per riavvolgere Nastro 3)
                Al raggiungimento del $\B$ su Nastro 3 e $\#$ su Nastro 1:
                $(\#, X, \B, \B, \B) \to (\#, X, \B, \B, \B), (R,S,R,S,S)$ e torna a $Q_1$ (per processare il prossimo $\#$ e la prossima $W_i$, decrementando $N$ su Nastro 2).
            \item \textbf{Transizione (Accettazione):}
                Se Nastro 2 legge $\B$ (non ci sono più $X$ per $N$, quindi tutte le $W_i$ sono state processate):
                \begin{itemize}
                    \item Nastro 1: Controlla che sia finito (legga $\B$).
                    \item Nastro 2: Legge $\B$, lo riscrive, si ferma.
                \end{itemize}
                Passa a $Q_{acc}$.
                \item $(\B, \B, \text{any}, \B, \B) \to (\B, \B, \text{any}, \B, \B), (S,S,S,S,S)$ (e si sposta in $Q_{acc}$).
        \end{itemize}
\end{itemize}

\subsection{Esercizio 2: Coppie di $W_i$}

\begin{definition}[Linguaggio $L_2$]
Sia $L_2$ il linguaggio definito come:
$L_2 = \{ A\#B\#W_1 W_1 W_2 W_2 \dots W_N W_N \mid A, B, W_i \in \{0,1\}^+, |A| > |B|, N = |A| - |B|, |W_i| \ge |B| \}$
\end{definition}

\subsubsection{Strategia della Macchina di Turing}
\begin{itemize}
    \item \textbf{Nastro 1 (Input):} Contiene $A\#B\#W_1 W_1 \dots$.
    \item \textbf{Nastro 2 (Conteggio $N$):} Per memorizzare $N = |A| - |B|$.
    \item \textbf{Nastro 3 (Copia $B$):} Per memorizzare la stringa $B$.
    \item \textbf{Nastro 4 (Copia $W_i$):} Per memorizzare $W_i$ e confrontarla con la sua seconda occorrenza.
\end{itemize}

\subsubsection{Descrizione degli Stati e delle Transizioni}
Sia $\Sigma_I = \{0,1,\#\}$ l'alfabeto di input e $\Gamma_T = \Sigma_I \cup \{\B, X\}$ l'alfabeto del nastro. Usiamo $\alpha$ per un simbolo generico da $\{0,1\}$.

\begin{itemize}
    \item \textbf{Stato $Q_0$ (Copia $A$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Scrive $X$ su Nastro 2, $R$.
            \item Alla lettura di $\#$: riscrive $\#$, $R$. Su Nastro 2, $S$. Passa a $Q_1$.
        \end{itemize}
    \item \textbf{Stato $Q_1$ (Copia $B$ e Calcolo $N$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$.
            \item Legge $X$ da Nastro 2, lo cancella ($\B$), $L$. (Inizia a calcolare $|A|-|B|$).
            \item Scrive $\alpha$ su Nastro 3, $R$. (Copia $B$).
            \item Loop in $Q_2$.
        \end{itemize}
    \item \textbf{Stato $Q_2$ (Fine Calcolo $N$ e Copia $B$):}
        \begin{itemize}
            \item Alla lettura di $\#$ (dopo $B$): riscrive $\#$, $R$.
            \item Su Nastro 2: legge $X$, lo riscrive, $S$. (Controlla $|A|>|B|$: se Nastro 2 fosse $\B$ qui, allora $|A| \le |B|$, quindi rifiuterebbe implicitàmente).
            \item Su Nastro 3: legge $\B$, riscrive $\B$, $L$. (Riavvolge Nastro 3 per preparare $B$ per confronti futuri).
            \item Passa a $Q_3$.
        \end{itemize}
    \item \textbf{Stato $Q_3$ (Processa $W_i$ - prima occorrenza):}
        Questo stato si occupa di copiare la prima occorrenza di $W_i$ sul Nastro 4 e di verificare che $|W_i| \ge |B|$.
        \begin{itemize}
            \item Nondeterministicamente, la macchina può saltare caratteri in $W_i$ finché non decide di iniziare a copiare la sottostringa $W_i$ e allo stesso tempo verificare $|W_i| \ge |B|$.
            \item Loop in $Q_3$: Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Copia $\alpha$ su Nastro 4, $R$.
            \item Questo loop continua finché non si raggiunge la fine di $W_i$ (prossimo $\#$ o $\B$).
            \item Alla lettura di $\#$: riscrive $\#$, $R$.
            \item Cancella un $X$ dal Nastro 2, $L$. (Decrementa $N$).
            \item Riavvolge Nastro 4 a sinistra per preparare il confronto. Passa a $Q_5$.
        \end{itemize}
    \item \textbf{Stato $Q_5$ (Verifica $W_i W_i$):}
        Confronta la $W_i$ copiata sul Nastro 4 con la seconda occorrenza di $W_i$ sul Nastro 1.
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, lo riscrive, $R$.
            \item Legge $\alpha$ da Nastro 4, lo cancella ($\B$), $R$.
            \item Se i simboli non corrispondono, la macchina si blocca e rifiuta (non c'è transizione definita per questa situazione).
            \item Loop in $Q_6$.
            \item Quando Nastro 4 è $\B$ (fine di $W_i$ copiata):
                \begin{itemize}
                    \item Su Nastro 1: deve esserci l'inizio della seconda $W_i$.
                    \item Passa a $Q_7$.
                \end{itemize}
        \end{itemize}
    \item \textbf{Stato $Q_7$ (Prossima Coppia $W_i W_i$ o Accettazione):}
        \begin{itemize}
            \item Se Nastro 2 contiene $X$ (ci sono altre coppie $W_i W_i$):
                \begin{itemize}
                    \item Nastro 1: Posiziona il capo lettura sull'inizio della prossima $W_i$ (dopo il $\#$).
                    \item Riavvolge Nastro 3 e 4.
                    \item Torna a $Q_3$.
                \end{itemize}
            \item Se Nastro 2 è $\B$ (tutte le coppie $W_i W_i$ sono state processate):
                \begin{itemize}
                    \item Nastro 1: Controlla che sia $\B$ (fine input).
                    \item Accetta ($Q_{acc}$).
                \end{itemize}
        \end{itemize}
\end{itemize}

\subsection{Esercizio 3: Parità di Lunghezza di $W_i$}

\begin{definition}[Linguaggio $L_3$]
Sia $L_3$ il linguaggio definito come:
$L_3 = \{A\#B\#W_1 \# W_2 \# \dots \# W_N \mid A, B, W_i \in \{a,b,c,d\}^+, |A| > |B|, N = |A| - |B|, $
$\qquad (\text{se } |W_i| \text{ è pari, allora } B \subseteq W_i) \land (\text{se } |W_i| \text{ è dispari, allora } B^R \subseteq W_i) \}$
\end{definition}

\subsubsection{Strategia della Macchina di Turing}
\begin{itemize}
    \item \textbf{Nastro 1 (Input):} Contiene la stringa di input.
    \item \textbf{Nastro 2 (Conteggio $N$):} Per memorizzare $N = |A| - |B|$.
    \item \textbf{Nastro 3 (Copia $B$):} Per memorizzare la stringa $B$. La stringa $B^R$ sarà controllata rileggendo $B$ dal Nastro 3 all'indietro.
    \item \textbf{Nastro 4:} Non esplicitamente usata per copie permanenti in questa strategia, ma potrebbe servire per un flag temporaneo di parità o per verificare sottostringhe temporanee. Il professore descrive un metodo che evita un nastro specifico per la parità, ma usa stati distinti.
\end{itemize}

\subsubsection{Descrizione degli Stati e delle Transizioni}
Sia $\Sigma_I = \{a,b,c,d,\#\}$ l'alfabeto di input e $\Gamma_T = \Sigma_I \cup \{\B, X\}$ l'alfabeto del nastro. Usiamo $\alpha$ per un simbolo generico da $\{a,b,c,d\}$.

\begin{itemize}
    \item \textbf{Stato $Q_0$ (Copia $A$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Scrive $X$ su Nastro 2, $R$. (Copia $A$ come $X$s su Nastro 2).
            \item Alla lettura di $\#$: riscrive $\#$, $R$. Su Nastro 2, $L$. Passa a $Q_1$.
        \end{itemize}
    \item \textbf{Stato $Q_1$ (Copia $B$ e Calcolo $N$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$.
            \item Legge $X$ da Nastro 2, lo cancella ($\B$), $L$. (Inizia a calcolare $|A|-|B|$).
            \item Scrive $\alpha$ su Nastro 3, $R$. (Copia $B$).
            \item Loop in $Q_2$.
        \end{itemize}
    \item \textbf{Stato $Q_2$ (Fine Calcolo $N$ e Copia $B$):}
        \begin{itemize}
            \item Alla lettura di $\#$ (dopo $B$): riscrive $\#$, $R$.
            \item Su Nastro 2: legge $X$, lo riscrive, $S$. (Controlla $|A|>|B|$).
            \item Su Nastro 3: legge $\B$, riscrive $\B$, $L$. (Riavvolge Nastro 3 all'inizio di $B$).
            \item Passa a $Q_3$.
        \end{itemize}
    \item \textbf{Stato $Q_3$ (Inizio Parity Check per $W_i$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. (Legge il primo carattere di $W_i$).
            \item Su Nastro 2: legge $X$, lo cancella ($\B$), $L$. (Decrementa $N$ per questa $W_i$).
            \item Su Nastro 3: $S$.
            \item Passa a $Q_4$ (stato per lunghezza dispari, avendo letto il 1° carattere).
        \end{itemize}
    \item \textbf{Stato $Q_4$ (Lunghezza $W_i$ dispari / Prossimo carattere):}
        \begin{itemize}
            \item \textbf{Transizione (Prossimo carattere - pari):}
                Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Passa a $Q_5$.
                (Se siamo in $Q_4$ con $k$ caratteri letti, il prossimo carattere rende la lunghezza $k+1$, che è pari).
            \item \textbf{Transizione (Fine $W_i$ - dispari):}
                Legge $\#$ o $\B$ da Nastro 1, riscrive, $L$. (Ritorna all'inizio di $W_i$).
                Passa a $Q_6$. (La lunghezza di $W_i$ è dispari, quindi cerchiamo $B^R$).
        \end{itemize}
    \item \textbf{Stato $Q_5$ (Lunghezza $W_i$ pari / Prossimo carattere):}
        \begin{itemize}
            \item \textbf{Transizione (Prossimo carattere - dispari):}
                Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Passa a $Q_4$.
                (Se siamo in $Q_5$ con $k$ caratteri letti, il prossimo carattere rende la lunghezza $k+1$, che è dispari).
            \item \textbf{Transizione (Fine $W_i$ - pari):}
                Legge $\#$ o $\B$ da Nastro 1, riscrive, $L$. (Ritorna all'inizio di $W_i$).
                Passa a $Q_9$. (La lunghezza di $W_i$ è pari, quindi cerchiamo $B$).
        \end{itemize}
    \item \textbf{Stato $Q_6$ (Cerca $B^R$ in $W_i$ - Lunghezza dispari):}
        \begin{itemize}
            \item Si muove non deterministicamente su Nastro 1 (leggendo $\alpha$, riscrivendo, $L$) per trovare l'inizio della $B^R$ in $W_i$.
            \item Quando decide di iniziare il confronto: Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Legge $\alpha$ da Nastro 3, riscrive $\alpha$, $L$. Passa a $Q_7$.
            (Nastro 3 ha $B$, leggendolo $L$ si legge $B^R$).
        \end{itemize}
    \item \textbf{Stato $Q_7$ (Confronto $B^R$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Legge $\alpha$ da Nastro 3, riscrive $\alpha$, $L$.
            \item Loop in $Q_7$.
            \item Quando Nastro 3 legge $\B$ (fine di $B^R$): Nastro 1 continua a leggere $\alpha$, riscrive $\alpha$, $R$. Passa a $Q_8$.
        \end{itemize}
    \item \textbf{Stato $Q_8$ (Pulizia dopo $B^R$):}
        \begin{itemize}
            \item Riavvolge Nastro 1 a destra fino al prossimo $\#$ o $\B$. Riavvolge Nastro 3 a destra fino al $\B$ iniziale.
            \item Quando $\#$ o $\B$ su Nastro 1 e $\B$ su Nastro 3 sono raggiunti, passa a $Q_{loop\_check}$ (stato intermedio per decidere se continuare o accettare).
        \end{itemize}
    \item \textbf{Stato $Q_9$ (Cerca $B$ in $W_i$ - Lunghezza pari):}
        \begin{itemize}
            \item Si muove non deterministicamente su Nastro 1 (leggendo $\alpha$, riscrivendo, $R$) per trovare l'inizio della $B$ in $W_i$.
            \item Quando decide di iniziare il confronto: Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Legge $\alpha$ da Nastro 3, riscrive $\alpha$, $R$. Passa a $Q_{10}$.
            (Nastro 3 ha $B$, leggendolo $R$ si legge $B$).
        \end{itemize}
    \item \textbf{Stato $Q_{10}$ (Confronto $B$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Legge $\alpha$ da Nastro 3, riscrive $\alpha$, $R$.
            \item Loop in $Q_{10}$.
            \item Quando Nastro 3 legge $\B$ (fine di $B$): Nastro 1 continua a leggere $\alpha$, riscrive $\alpha$, $R$. Passa a $Q_{11}$.
        \end{itemize}
    \item \textbf{Stato $Q_{11}$ (Pulizia dopo $B$):}
        \begin{itemize}
            \item Riavvolge Nastro 1 a destra fino al prossimo $\#$ o $\B$. Riavvolge Nastro 3 a destra fino al $\B$ iniziale.
            \item Quando $\#$ o $\B$ su Nastro 1 e $\B$ su Nastro 3 sono raggiunti, passa a $Q_{loop\_check}$.
        \end{itemize}
    \item \textbf{Stato $Q_{loop\_check}$ (Controllo $N$ e Ciclo/Accetta):}
        \begin{itemize}
            \item Se Nastro 2 ha ancora $X$ (altre $W_i$):
                \begin{itemize}
                    \item Nastro 1: legge $\#$ o $\B$, riscrive, $R$.
                    \item Nastro 2: legge $X$, riscrive $X$, $S$. (Indica che ci sono ancora $W_i$ da processare, la cancellazione è stata fatta in $Q_3$).
                    \item Nastro 3: riavvolge.
                    \item Torna a $Q_3$.
                \end{itemize}
            \item Se Nastro 2 è $\B$ (tutte le $W_i$ sono state processate):
                \begin{itemize}
                    \item Nastro 1: legge $\B$, riscrive $\B$, $S$.
                    \item Accetta ($Q_{acc}$).
                \end{itemize}
        \end{itemize}
\end{itemize}

\subsection{Esercizio 4: $A$ o $B^R$ in $W_i$ in base alla Parità dell'Indice $i$}

\begin{definition}[Linguaggio $L_4$]
Sia $L_4$ il linguaggio definito come:
$L_4 = \{A\#B\#W_1 \# W_2 \# \dots \# W_N \mid A, B, W_i \in \{a,b,c,d\}^+, |A| > |B| > 0, N = |A| + |B|, $
$\qquad (\text{se } i \text{ è dispari, allora } A \subseteq W_i) \land (\text{se } i \text{ è pari, allora } B^R \subseteq W_i) \}$
\end{definition}

\subsubsection{Strategia della Macchina di Turing}
\begin{itemize}
    \item \textbf{Nastro 1 (Input):} Contiene la stringa di input.
    \item \textbf{Nastro 2 (Conteggio $N$ e Parità Indice):} Memorizza $N = |A| + |B|$. Per ogni $W_i$ processata, un $X$ viene cancellato, permettendo al nastro di fungere anche da contatore di indice $i$ (implicitamente).
    \item \textbf{Nastro 3 (Copia $A$):} Memorizza la stringa $A$.
    \item \textbf{Nastro 4 (Copia $B$):} Memorizza la stringa $B$. Per $B^R$, Nastro 4 verrà letto all'indietro.
\end{itemize}

\subsubsection{Descrizione degli Stati e delle Transizioni}
Sia $\Sigma_I = \{a,b,c,d,\#\}$ l'alfabeto di input e $\Gamma_T = \Sigma_I \cup \{\B, X\}$ l'alfabeto del nastro. Usiamo $\alpha$ per un simbolo generico da $\{a,b,c,d\}$.

\begin{itemize}
    \item \textbf{Stato $Q_0$ (Copia $A$ e $|A|$ su Nastro 2):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Scrive $X$ su Nastro 2, $R$. (Copia $A$ come $X$s su Nastro 2). Scrive $\alpha$ su Nastro 3, $R$. (Copia $A$ su Nastro 3).
            \item Alla lettura di $\#$: riscrive $\#$, $R$. Riavvolge Nastro 3 ($L$). Passa a $Q_1$.
        \end{itemize}
    \item \textbf{Stato $Q_1$ (Copia $B$ e Calcolo $|A|+|B|$ su Nastro 2):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Scrive $X$ su Nastro 2, $R$. (Aggiunge $|B|$ a Nastro 2). Scrive $\alpha$ su Nastro 4, $R$. (Copia $B$ su Nastro 4).
            \item Alla lettura di $\#$: riscrive $\#$, $R$. Riavvolge Nastro 4 ($L$).
            \item Controlla $|A|>|B|>0$: questo avviene verificando che Nastro 2 ha almeno due $X$s dopo aver contato $A$ e $B$ e che Nastro 4 ha almeno un $X$. Implicitamente, si assicura che $B$ non sia vuoto e che $A$ sia più lungo di $B$.
            \item Passa a $Q_2$.
        \end{itemize}
    \item \textbf{Stato $Q_2$ (Preparazione e Inizio Loop $W_i$):}
        \begin{itemize}
            \item Riavvolge Nastro 2 a sinistra per posizionarsi sul primo $X$ da consumare. Riavvolge Nastro 3 all'inizio di $A$. Riavvolge Nastro 4 all'inizio di $B$.
            \item Passa a $Q_3$.
        \end{itemize}
    \item \textbf{Stato $Q_3$ (Processa $W_i$ - indice dispari):}
        Questo stato gestisce $W_i$ con $i$ dispari (es. $W_1, W_3, \dots$).
        \begin{itemize}
            \item Su Nastro 2: legge $X$, lo cancella ($\B$), $L$. (Decrementa $N$ per questa $W_i$).
            \item Nondeterministicamente cerca l'inizio di $A$ in $W_i$ su Nastro 1 (legge $\alpha$, riscrive, $R$).
            \item Quando decide di iniziare il confronto: Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Legge $\alpha$ da Nastro 3, riscrive $\alpha$, $R$. Passa a $Q_4$.
        \end{itemize}
    \item \textbf{Stato $Q_4$ (Confronto $A$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Legge $\alpha$ da Nastro 3, riscrive $\alpha$, $R$.
            \item Loop in $Q_4$.
            \item Quando Nastro 3 è $\B$ (fine di $A$): Nastro 1 continua a leggere $\alpha$, riscrive $\alpha$, $R$. Passa a $Q_5$.
        \end{itemize}
    \item \textbf{Stato $Q_5$ (Pulizia dopo $A$ e Preparazione per $W_{i+1}$):}
        \begin{itemize}
            \item Riavvolge Nastro 3 a sinistra al $\B$ iniziale.
            \item Riavvolge Nastro 1 a destra fino al prossimo $\#$ o $\B$.
            \item Quando $\#$ su Nastro 1 è raggiunto e Nastro 3 è riavvolto: Legge $\#$, riscrive $\#$, $R$. Passa a $Q_6$.
        \end{itemize}
    \item \textbf{Stato $Q_6$ (Processa $W_i$ - indice pari):}
        Questo stato gestisce $W_i$ con $i$ pari (es. $W_2, W_4, \dots$).
        \begin{itemize}
            \item Su Nastro 2: legge $X$, lo cancella ($\B$), $L$. (Decrementa $N$ per questa $W_i$).
            \item Nondeterministicamente cerca l'inizio di $B^R$ in $W_i$ su Nastro 1 (legge $\alpha$, riscrive, $L$ - posizionandosi per il reverse check).
            \item Quando decide di iniziare il confronto: Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Legge $\alpha$ da Nastro 4, riscrive $\alpha$, $L$. Passa a $Q_7$.
        \end{itemize}
    \item \textbf{Stato $Q_7$ (Confronto $B^R$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Legge $\alpha$ da Nastro 4, riscrive $\alpha$, $L$.
            \item Loop in $Q_7$.
            \item Quando Nastro 4 è $\B$ (fine di $B^R$): Nastro 1 continua a leggere $\alpha$, riscrive $\alpha$, $R$. Passa a $Q_8$.
        \end{itemize}
    \item \textbf{Stato $Q_8$ (Pulizia dopo $B^R$ e Preparazione per $W_{i+1}$):}
        \begin{itemize}
            \item Riavvolge Nastro 4 a sinistra al $\B$ iniziale.
            \item Riavvolge Nastro 1 a destra fino al prossimo $\#$ o $\B$.
            \item Quando $\#$ su Nastro 1 è raggiunto e Nastro 4 è riavvolto: Legge $\#$, riscrive $\#$, $R$. Passa a $Q_3$ (per la prossima $W_i$ dispari).
        \end{itemize}
    \item \textbf{Accettazione ($Q_{acc}$):}
        \begin{itemize}
            \item Dopo l'elaborazione dell'ultima $W_N$, se Nastro 2 legge $\B$ (tutti gli $X$ sono stati consumati) e Nastro 1 legge $\B$ (fine input):
                \begin{itemize}
                    \item $(\B, \B, \text{any}, \text{any}) \to (\B, \B, \text{any}, \text{any}), (S,S,S,S)$ e accetta.
                \end{itemize}
        \end{itemize}
\end{itemize}

\section{Conclusioni}
Le esercitazioni di oggi hanno permesso di approfondire la progettazione di Macchine di Turing Non Deterministiche, in particolare mostrando come la capacità di \texttt{guess} (indovinare) combinata con un robusto \texttt{check} (controllo) possa semplificare la logica di alcune verifiche complesse. È stato evidenziato come le NDTM possano \texttt{scrivere in anticipo} su nastri ausiliari stringhe che verranno poi validate.
Nella prossima lezione, si inizierà a esplorare i concetti di calcolabilità, inclusi problemi indecidibili e il concetto fondamentale di riduzione, che sarà un pilastro per comprendere la complessità dei problemi.


% =====================================================
% --- START LECTURE 10 ---
% =====================================================

\chapter{Macchine di Turing Non Deterministiche: Esercitazioni}



\section{Introduzione alle Macchine di Turing Non Deterministiche (NDTM)}

Riprendiamo dalla discussione sulle Macchine di Turing Non Deterministiche (NDTM).
Una caratteristica fondamentale delle NDTM è la loro capacità di \textit{indovinare} o \textit{guessare} porzioni di stringa o decisioni computazionali. Questa non è una capacità intrinseca della macchina di indovinare nel senso umano, né implica una computazione parallela di tutte le possibilità. È una metafora per descrivere il fatto che se esiste almeno una sequenza di scelte che porta all'accettazione, allora la macchina accetterà.

Tuttavia, per garantire che una NDTM accetti solo le stringhe che appartengono al linguaggio desiderato, la fase di \textit{guess} deve essere sempre seguita da una fase di \textit{check} (controllo). La fase di \textit{check} è cruciale per filtrare le \textit{scelte sbagliate} fatte dalla macchina non deterministica, assicurando che solo le computazioni corrette portino a uno stato di accettazione.

Un esempio pratico di questa capacità delle NDTM è la possibilità di \textit{scrivere in anticipo} su un nastro ausiliario delle stringhe che saranno necessarie in un momento successivo della computazione. Questo comportamento è particolarmente utile quando si analizzano le classi di complessità computazionale (es. NP).

\section{Esercizi}

Analizziamo alcuni esercizi per applicare i concetti delle NDTM.

\subsection{Esercizio 1: Sottostringa Palindroma di Lunghezza Variabile}

\begin{definition}[Linguaggio $L_1$]
Sia $L_1$ il linguaggio definito come:
$L_1 = \{ X^N \texttt{\#} W_1 \texttt{\#} W_2 \texttt{\#} \dots \texttt{\#} W_N \mid N > 0, W_i \in \{a,b,c,d\}^+, \forall i \in [1, N], \exists S_i \subseteq W_i \text{ t.c. } |S_i| = i \text{ e } S_i = S_i^R \}$
dove $X^N$ indica $N$ occorrenze del simbolo \texttt{X}.
\end{definition}

\subsubsection{Strategia della Macchina di Turing}
La macchina utilizzerà cinque nastri:
\begin{itemize}
    \item \textbf{Nastro 1 (Input):} Contiene la stringa di input.
    \item \textbf{Nastro 2 (Conteggio $N$):} Per memorizzare il numero di blocchi $W_i$.
    \item \textbf{Nastro 3 (Conteggio $i$):} Per memorizzare il valore corrente dell'indice $i$.
    \item \textbf{Nastro 4 (\textit{Guess} $S_i$):} Per scrivere la stringa $S_i$ \textit{indovinata}.
    \item \textbf{Nastro 5 (\textit{Guess} $S_i$ per $S_i^R$):} Una copia di $S_i$ per il controllo di palindromia.
\end{itemize}

L'idea principale è che per ogni $W_i$, la macchina non deterministicamente \textit{indovina} la stringa $S_i$ sul Nastro 4 e 5. Successivamente, verifica che $S_i$ sia palindroma (confrontando Nastro 4 e Nastro 5) e che sia effettivamente una sottostringa di $W_i$ (confrontando Nastro 4 con il Nastro 1). La lunghezza di $S_i$ viene controllata usando il Nastro 3.

\subsubsection{Descrizione degli Stati e delle Transizioni}
Sia $\Sigma_I = \{\texttt{X}, \texttt{\#}, a,b,c,d\}$ l'alfabeto di input e $\Gamma_T = \Sigma_I \cup \{\B, \text{ausiliari}\}$ l'alfabeto del nastro. Usiamo $\alpha$ per un simbolo generico da $\{a,b,c,d\}$.

\begin{itemize}
    \item \textbf{Stato $Q_0$ (Inizio):}
        \begin{itemize}
            \item \textbf{Transizione per $X^N$:} Legge \texttt{X} dal Nastro 1, lo riscrive e si muove a destra. Scrive \texttt{X} sul Nastro 2 (inizialmente vuoto) e si muove a destra. Passa allo stato $Q_1$. Questo ciclo si ripete per tutti gli \texttt{X} iniziali.
            \item $(\texttt{X}, \B, \B, \B, \B) \to (\texttt{X}, \texttt{X}, \B, \B, \B), (R,R,S,S,S)$
        \end{itemize}
    \item \textbf{Stato $Q_1$ (Copia $N$ e Inizio $W_1$):}
        \begin{itemize}
            \item \textbf{Transizione per \texttt{\#}:} Quando legge \texttt{\#} sul Nastro 1, lo riscrive e si muove a destra (posizionandosi all'inizio di $W_1$). Sul Nastro 2 (che contiene gli \texttt{X} per $N$), legge un \texttt{X}, lo cancella ($\B$) e si muove a sinistra (si posizione sull'ultimo \texttt{X} restante o $\B$). Sul Nastro 3 (inizialmente vuoto), scrive \texttt{X} e si muove a destra (questo inizia il conteggio di $i=1$). Passa a $Q_2$.
            \item $(\texttt{\#}, \texttt{X}, \B, \B, \B) \to (\texttt{\#}, \B, \texttt{X}, \B, \B), (R,L,R,S,S)$
        \end{itemize}
    \item \textbf{Stato $Q_2$ (\textit{Guess} $S_i$):}
        Questo stato gestisce la generazione non deterministica di $S_i$ sui nastri 4 e 5. Il loop avviene basandosi sul Nastro 3 (che contiene $i$).
        \begin{itemize}
            \item \textbf{Transizione (Loop per \textit{guess} $S_i$):}
                Mentre Nastro 3 contiene \texttt{X} (cioè, $S_i$ non ha ancora raggiunto la lunghezza $i$):
                \begin{itemize}
                    \item Nastro 1: Legge $\alpha$ (qualsiasi simbolo di input), lo riscrive e si muove a destra (continua a leggere $W_i$).
                    \item Nastro 2: Non modificato.
                    \item Nastro 3: Legge \texttt{X}, lo riscrive e si muove a destra (avanza nel conteggio di $i$).
                    \item Nastro 4: Legge $\B$, scrive $\alphaSym$ (un simbolo non deterministico dall'alfabeto di input) e si muove a destra.
                    \item Nastro 5: Legge $\B$, scrive $\alphaSym$ (lo stesso simbolo di Nastro 4) e si muove a destra.
                \end{itemize}
                Questo loop rimane in $Q_3$ (si passa a $Q_3$ nella descrizione, il diagramma usa $Q_2$ $\to$ $Q_3$ $\to$ $Q_3$).
                \item $(\alpha, \text{any}, \texttt{X}, \B, \B) \to (\alpha, \text{any}, \texttt{X}, \alphaSym, \alphaSym), (R,S,R,R,R)$ (Questo $\alpha$ su Nastro 1 è solo lettura, $\alphaSym$ è il simbolo guessed).
            \item \textbf{Transizione (Fine \textit{guess} $S_i$):}
                Quando Nastro 3 legge $\B$ (ha scritto $i$ simboli su Nastro 4 e 5):
                \begin{itemize}
                    \item Nastro 1: Legge $\alpha$, lo riscrive e si muove a destra.
                    \item Nastro 2: Non modificato.
                    \item Nastro 3: Legge $\B$, lo riscrive e si muove a sinistra (riavvolge il conteggio di $i$).
                    \item Nastro 4: Legge $\B$, lo riscrive e si muove a sinistra (riavvolge Nastro 4).
                    \item Nastro 5: Legge $\B$, lo riscrive e si muove a sinistra (riavvolge Nastro 5).
                \end{itemize}
                Passa a $Q_4$.
                \item $(\alpha, \text{any}, \B, \B, \B) \to (\alpha, \text{any}, \B, \B, \B), (R,S,L,L,L)$
        \end{itemize}
    \item \textbf{Stato $Q_4$ (Riavvolgimento):}
        Questo stato riavvolge i nastri 3, 4 e 5 per prepararsi al controllo. Si continua a muoversi a sinistra su Nastro 3, 4 e 5 finché non si raggiunge il $\B$ iniziale.
        \begin{itemize}
            \item \textbf{Transizione (Loop di riavvolgimento):}
                Mentre Nastro 4 non è $\B$:
                \begin{itemize}
                    \item Nastro 1: Ignorato (ma continua a leggere $W_i$).
                    \item Nastro 2: Ignorato.
                    \item Nastro 3: Legge \texttt{X}, lo riscrive, si muove a sinistra.
                    \item Nastro 4: Legge $\alphaSym$, lo riscrive, si muove a sinistra.
                    \item Nastro 5: Legge $\alphaSym$, lo riscrive, si muove a sinistra.
                \end{itemize}
                Questo loop rimane in $Q_4$.
                \item $(\text{any}, \text{any}, \texttt{X}, \alphaSym, \alphaSym) \to (\text{any}, \text{any}, \texttt{X}, \alphaSym, \alphaSym), (S,S,L,L,L)$
            \item \textbf{Transizione (Fine riavvolgimento):}
                Quando Nastro 4 e Nastro 5 leggono $\B$ (sono all'inizio di $S_i$):
                \begin{itemize}
                    \item Nastro 1: Ignorato.
                    \item Nastro 2: Ignorato.
                    \item Nastro 3: Legge $\B$, lo riscrive, si muove a destra.
                    \item Nastro 4: Legge $\B$, lo riscrive, si muove a destra.
                    \item Nastro 5: Legge $\B$, lo riscrive, si muove a destra (posiziona il capo del Nastro 5 sulla fine di $S_i$ per il reverse check).
                \end{itemize}
                Passa a $Q_5$.
                \item $(\text{any}, \text{any}, \B, \B, \B) \to (\text{any}, \text{any}, \B, \B, \B), (S,S,R,R,R)$
        \end{itemize}
    \item \textbf{Stato $Q_5$ (Check $S_i$ e $S_i^R$ in $W_i$):}
        Questo stato non deterministicamente cerca l'inizio di $S_i$ in $W_i$ sul Nastro 1 e contemporaneamente verifica la palindromia di $S_i$ tra Nastro 4 (leggendo in avanti) e Nastro 5 (leggendo all'indietro).
        \begin{itemize}
            \item \textbf{Transizione (Salto in $W_i$):}
                Nondeterministicamente salta caratteri in $W_i$ sul Nastro 1 fino a trovare un possibile inizio di $S_i$.
                \begin{itemize}
                    \item Nastro 1: Legge $\alpha$, lo riscrive, si muove a destra.
                \end{itemize}
                Loop su $Q_5$.
                \item $(\alpha, \text{any}, \text{any}, \text{any}, \text{any}) \to (\alpha, \text{any}, \text{any}, \text{any}, \text{any}), (R,S,S,S,S)$
            \item \textbf{Transizione (Confronto e Palindromia):}
                Quando si decide di iniziare il confronto:
                \begin{itemize}
                    \item Nastro 1: Legge $\alphaSym$, lo riscrive, si muove a destra (confronta con $S_i$).
                    \item Nastro 4: Legge $\alphaSym$, lo cancella ($\B$), si muove a destra (consuma $S_i$).
                    \item Nastro 5: Legge $\alphaSym$, lo cancella ($\B$), si muove a sinistra (consuma $S_i^R$).
                \end{itemize}
                Passa a $Q_6$. Questo loop continua in $Q_6$.
                \item $(\alphaSym, \text{any}, \text{any}, \alphaSym, \alphaSym) \to (\alphaSym, \text{any}, \text{any}, \B, \B), (R,S,S,R,L)$
        \end{itemize}
    \item \textbf{Stato $Q_6$ (Continuazione del Check):}
        \begin{itemize}
            \item \textbf{Transizione (Fine del confronto):}
                Quando Nastro 4 e Nastro 5 leggono $\B$ (hanno verificato tutta $S_i$):
                \begin{itemize}
                    \item Nastro 1: Ignorato.
                    \item Nastro 4: Legge $\B$, lo riscrive, si ferma.
                    \item Nastro 5: Legge $\B$, lo riscrive, si ferma.
                \end{itemize}
                Passa a $Q_7$.
                \item $(\text{any}, \text{any}, \text{any}, \B, \B) \to (\text{any}, \text{any}, \text{any}, \B, \B), (S,S,S,S,S)$
        \end{itemize}
    \item \textbf{Stato $Q_7$ (Controllo Prossima $W_i$ o Accettazione):}
        \begin{itemize}
            \item \textbf{Transizione (Prossima $W_i$):}
                Se Nastro 2 contiene ancora \texttt{X} (ci sono altre $W_i$ da processare):
                \begin{itemize}
                    \item Nastro 1: Continua a leggere $\alpha$ e si sposta a destra (fino a fine $W_i$ o \texttt{\#}).
                    \item Nastro 2: Legge \texttt{X}, lo riscrive, si ferma (la cancellazione avverrà tornando a $Q_1$).
                    \item Nastro 3: Riavvolge a sinistra fino a $\B$.
                \end{itemize}
                Questa è una transizione composta:
                $\{ (\alpha, \texttt{X}, \alpha, \B, \B) \to (\alpha, \texttt{X}, \alpha, \B, \B), (R,S,L,S,S) \}$ (Loop per riavvolgere Nastro 3)
                Al raggiungimento del $\B$ su Nastro 3 e \texttt{\#} su Nastro 1:
                $(\texttt{\#}, \texttt{X}, \B, \B, \B) \to (\texttt{\#}, \texttt{X}, \B, \B, \B), (R,S,R,S,S)$ e torna a $Q_1$ (per processare il prossimo \texttt{\#} e la prossima $W_i$, decrementando $N$ su Nastro 2).
            \item \textbf{Transizione (Accettazione):}
                Se Nastro 2 legge $\B$ (non ci sono più \texttt{X} per $N$, quindi tutte le $W_i$ sono state processate):
                \begin{itemize}
                    \item Nastro 1: Controlla che sia finito (legga $\B$).
                    \item Nastro 2: Legge $\B$, lo riscrive, si ferma.
                \end{itemize}
                Passa a $Q_{acc}$.
                \item $(\B, \B, \text{any}, \B, \B) \to (\B, \B, \text{any}, \B, \B), (S,S,S,S,S)$ (e si sposta in $Q_{acc}$).
        \end{itemize}
\end{itemize}

\subsection{Esercizio 2: Coppie di $W_i$}

\begin{definition}[Linguaggio $L_2$]
Sia $L_2$ il linguaggio definito come:
$L_2 = \{ A\texttt{\#}B\texttt{\#}W_1 W_1 W_2 W_2 \dots W_N W_N \mid A, B, W_i \in \{0,1\}^+, |A| > |B|, N = |A| - |B|, |W_i| \ge |B| \}$
\end{definition}

\subsubsection{Strategia della Macchina di Turing}
\begin{itemize}
    \item \textbf{Nastro 1 (Input):} Contiene $A\texttt{\#}B\texttt{\#}W_1 W_1 \dots$.
    \item \textbf{Nastro 2 (Conteggio $N$):} Per memorizzare $N = |A| - |B|$.
    \item \textbf{Nastro 3 (Copia $B$):} Per memorizzare la stringa $B$.
    \item \textbf{Nastro 4 (Copia $W_i$):} Per memorizzare $W_i$ e confrontarla con la sua seconda occorrenza.
\end{itemize}

\subsubsection{Descrizione degli Stati e delle Transizioni}
Sia $\Sigma_I = \{0,1,\texttt{\#}\}$ l'alfabeto di input e $\Gamma_T = \Sigma_I \cup \{\B, \texttt{X}\}$ l'alfabeto del nastro. Usiamo $\alpha$ per un simbolo generico da $\{0,1\}$.

\begin{itemize}
    \item \textbf{Stato $Q_0$ (Copia $A$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Scrive \texttt{X} su Nastro 2, $R$.
            \item Alla lettura di \texttt{\#}: riscrive \texttt{\#}, $R$. Su Nastro 2, $S$. Passa a $Q_1$.
        \end{itemize}
    \item \textbf{Stato $Q_1$ (Copia $B$ e Calcolo $N$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$.
            \item Legge \texttt{X} da Nastro 2, lo cancella ($\B$), $L$. (Inizia a calcolare $|A|-|B|$).
            \item Scrive $\alpha$ su Nastro 3, $R$. (Copia $B$).
            \item Loop in $Q_2$.
        \end{itemize}
    \item \textbf{Stato $Q_2$ (Fine Calcolo $N$ e Copia $B$):}
        \begin{itemize}
            \item Alla lettura di \texttt{\#} (dopo $B$): riscrive \texttt{\#}, $R$.
            \item Su Nastro 2: legge \texttt{X}, lo riscrive, $S$. (Controlla $|A|>|B|$: se Nastro 2 fosse $\B$ qui, allora $|A| \le |B|$, quindi rifiuterebbe implicitamente).
            \item Su Nastro 3: legge $\B$, riscrive $\B$, $L$. (Riavvolge Nastro 3 per preparare $B$ per confronti futuri).
            \item Passa a $Q_3$.
        \end{itemize}
    \item \textbf{Stato $Q_3$ (Processa $W_i$ - prima occorrenza):}
        Questo stato si occupa di copiare la prima occorrenza di $W_i$ sul Nastro 4 e di verificare che $|W_i| \ge |B|$.
        \begin{itemize}
            \item Nondeterministicamente, la macchina può saltare caratteri in $W_i$ finché non decide di iniziare a copiare la sottostringa $W_i$ e allo stesso tempo verificare $|W_i| \ge |B|$.
            \item Loop in $Q_3$: Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Copia $\alpha$ su Nastro 4, $R$.
            \item Questo loop continua finché non si raggiunge la fine di $W_i$ (prossimo \texttt{\#} o $\B$).
            \item Alla lettura di \texttt{\#}: riscrive \texttt{\#}, $R$.
            \item Cancella un \texttt{X} dal Nastro 2, $L$. (Decrementa $N$).
            \item Riavvolge Nastro 4 a sinistra per preparare il confronto. Passa a $Q_5$.
        \end{itemize}
    \item \textbf{Stato $Q_5$ (Verifica $W_i W_i$):}
        Confronta la $W_i$ copiata sul Nastro 4 con la seconda occorrenza di $W_i$ sul Nastro 1.
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, lo riscrive, $R$.
            \item Legge $\alpha$ da Nastro 4, lo cancella ($\B$), $R$.
            \item Se i simboli non corrispondono, la macchina si blocca e rifiuta (non c'è transizione definita per questa situazione).
            \item Loop in $Q_6$.
            \item Quando Nastro 4 è $\B$ (fine di $W_i$ copiata):
                \begin{itemize}
                    \item Su Nastro 1: deve esserci l'inizio della seconda $W_i$.
                    \item Passa a $Q_7$.
                \end{itemize}
        \end{itemize}
    \item \textbf{Stato $Q_7$ (Prossima Coppia $W_i W_i$ o Accettazione):}
        \begin{itemize}
            \item Se Nastro 2 contiene \texttt{X} (ci sono altre coppie $W_i W_i$):
                \begin{itemize}
                    \item Nastro 1: Posiziona il capo lettura sull'inizio della prossima $W_i$ (dopo il \texttt{\#}).
                    \item Riavvolge Nastro 3 e 4.
                    \item Torna a $Q_3$.
                \end{itemize}
            \item Se Nastro 2 è $\B$ (tutte le coppie $W_i W_i$ sono state processate):
                \begin{itemize}
                    \item Nastro 1: Controlla che sia $\B$ (fine input).
                    \item Accetta ($Q_{acc}$).
                \end{itemize}
        \end{itemize}
\end{itemize}

\subsection{Esercizio 3: Parità di Lunghezza di $W_i$}

\begin{definition}[Linguaggio $L_3$]
Sia $L_3$ il linguaggio definito come:
$L_3 = \{A\texttt{\#}B\texttt{\#}W_1 \texttt{\#} W_2 \texttt{\#} \dots \texttt{\#} W_N \mid A, B, W_i \in \{a,b,c,d\}^+, |A| > |B|, N = |A| - |B|, $
$\qquad (\text{se } |W_i| \text{ è pari, allora } B \subseteq W_i) \land (\text{se } |W_i| \text{ è dispari, allora } B^R \subseteq W_i) \}$
\end{definition}

\subsubsection{Strategia della Macchina di Turing}
\begin{itemize}
    \item \textbf{Nastro 1 (Input):} Contiene la stringa di input.
    \item \textbf{Nastro 2 (Conteggio $N$):} Per memorizzare $N = |A| - |B|$.
    \item \textbf{Nastro 3 (Copia $B$):} Per memorizzare la stringa $B$. La stringa $B^R$ sarà controllata rileggendo $B$ dal Nastro 3 all'indietro.
    \item \textbf{Nastro 4:} Non esplicitamente usata per copie permanenti in questa strategia, ma potrebbe servire per un flag temporaneo di parità o per verificare sottostringhe temporanee. Il professore descrive un metodo che evita un nastro specifico per la parità, ma usa stati distinti.
\end{itemize}

\subsubsection{Descrizione degli Stati e delle Transizioni}
Sia $\Sigma_I = \{a,b,c,d,\texttt{\#}\}$ l'alfabeto di input e $\Gamma_T = \Sigma_I \cup \{\B, \texttt{X}\}$ l'alfabeto del nastro. Usiamo $\alpha$ per un simbolo generico da $\{a,b,c,d\}$.

\begin{itemize}
    \item \textbf{Stato $Q_0$ (Copia $A$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Scrive \texttt{X} su Nastro 2, $R$. (Copia $A$ come \texttt{X}s su Nastro 2).
            \item Alla lettura di \texttt{\#}: riscrive \texttt{\#}, $R$. Su Nastro 2, $L$. Passa a $Q_1$.
        \end{itemize}
    \item \textbf{Stato $Q_1$ (Copia $B$ e Calcolo $N$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$.
            \item Legge \texttt{X} da Nastro 2, lo cancella ($\B$), $L$. (Inizia a calcolare $|A|-|B|$).
            \item Scrive $\alpha$ su Nastro 3, $R$. (Copia $B$).
            \item Loop in $Q_2$.
        \end{itemize}
    \item \textbf{Stato $Q_2$ (Fine Calcolo $N$ e Copia $B$):}
        \begin{itemize}
            \item Alla lettura di \texttt{\#} (dopo $B$): riscrive \texttt{\#}, $R$.
            \item Su Nastro 2: legge \texttt{X}, lo riscrive, $S$. (Controlla $|A|>|B|$).
            \item Su Nastro 3: legge $\B$, riscrive $\B$, $L$. (Riavvolge Nastro 3 all'inizio di $B$).
            \item Passa a $Q_3$.
        \end{itemize}
    \item \textbf{Stato $Q_3$ (Inizio Parity Check per $W_i$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. (Legge il primo carattere di $W_i$).
            \item Su Nastro 2: legge \texttt{X}, lo cancella ($\B$), $L$. (Decrementa $N$ per questa $W_i$).
            \item Su Nastro 3: $S$.
            \item Passa a $Q_4$ (stato per lunghezza dispari, avendo letto il 1° carattere).
        \end{itemize}
    \item \textbf{Stato $Q_4$ (Lunghezza $W_i$ dispari / Prossimo carattere):}
        \begin{itemize}
            \item \textbf{Transizione (Prossimo carattere - pari):}
                Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Passa a $Q_5$.
                (Se siamo in $Q_4$ con $k$ caratteri letti, il prossimo carattere rende la lunghezza $k+1$, che è pari).
            \item \textbf{Transizione (Fine $W_i$ - dispari):}
                Legge \texttt{\#} o $\B$ da Nastro 1, riscrive, $L$. (Ritorna all'inizio di $W_i$).
                Passa a $Q_6$. (La lunghezza di $W_i$ è dispari, quindi cerchiamo $B^R$).
        \end{itemize}
    \item \textbf{Stato $Q_5$ (Lunghezza $W_i$ pari / Prossimo carattere):}
        \begin{itemize}
            \item \textbf{Transizione (Prossimo carattere - dispari):}
                Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Passa a $Q_4$.
                (Se siamo in $Q_5$ con $k$ caratteri letti, il prossimo carattere rende la lunghezza $k+1$, che è dispari).
            \item \textbf{Transizione (Fine $W_i$ - pari):}
                Legge \texttt{\#} o $\B$ da Nastro 1, riscrive, $L$. (Ritorna all'inizio di $W_i$).
                Passa a $Q_9$. (La lunghezza di $W_i$ è pari, quindi cerchiamo $B$).
        \end{itemize}
    \item \textbf{Stato $Q_6$ (Cerca $B^R$ in $W_i$ - Lunghezza dispari):}
        \begin{itemize}
            \item Si muove non deterministicamente su Nastro 1 (leggendo $\alpha$, riscrivendo, $L$) per trovare l'inizio della $B^R$ in $W_i$.
            \item Quando decide di iniziare il confronto: Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Legge $\alpha$ da Nastro 3, riscrive $\alpha$, $L$. Passa a $Q_7$.
            (Nastro 3 ha $B$, leggendolo $L$ si legge $B^R$).
        \end{itemize}
    \item \textbf{Stato $Q_7$ (Confronto $B^R$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Legge $\alpha$ da Nastro 3, riscrive $\alpha$, $L$.
            \item Loop in $Q_7$.
            \item Quando Nastro 3 legge $\B$ (fine di $B^R$): Nastro 1 continua a leggere $\alpha$, riscrive $\alpha$, $R$. Passa a $Q_8$.
        \end{itemize}
    \item \textbf{Stato $Q_8$ (Pulizia dopo $B^R$):}
        \begin{itemize}
            \item Riavvolge Nastro 1 a destra fino al prossimo \texttt{\#} o $\B$. Riavvolge Nastro 3 a destra fino al $\B$ iniziale.
            \item Quando \texttt{\#} o $\B$ su Nastro 1 e $\B$ su Nastro 3 sono raggiunti, passa a \texttt{Q\_{loop\_check}} (stato intermedio per decidere se continuare o accettare).
        \end{itemize}
    \item \textbf{Stato $Q_9$ (Cerca $B$ in $W_i$ - Lunghezza pari):}
        \begin{itemize}
            \item Si muove non deterministicamente su Nastro 1 (leggendo $\alpha$, riscrivendo, $R$) per trovare l'inizio della $B$ in $W_i$.
            \item Quando decide di iniziare il confronto: Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Legge $\alpha$ da Nastro 3, riscrive $\alpha$, $R$. Passa a \texttt{Q\_{10}}.
            (Nastro 3 ha $B$, leggendolo $R$ si legge $B$).
        \end{itemize}
    \item \textbf{Stato \texttt{Q\_{10}} (Confronto $B$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Legge $\alpha$ da Nastro 3, riscrive $\alpha$, $R$.
            \item Loop in \texttt{Q\_{10}}.
            \item Quando Nastro 3 legge $\B$ (fine di $B$): Nastro 1 continua a leggere $\alpha$, riscrive $\alpha$, $R$. Passa a \texttt{Q\_{11}}.
        \end{itemize}
    \item \textbf{Stato \texttt{Q\_{11}} (Pulizia dopo $B$):}
        \begin{itemize}
            \item Riavvolge Nastro 1 a destra fino al prossimo \texttt{\#} o $\B$. Riavvolge Nastro 3 a destra fino al $\B$ iniziale.
            \item Quando \texttt{\#} o $\B$ su Nastro 1 e $\B$ su Nastro 3 sono raggiunti, passa a \texttt{Q\_{loop\_check}}.
        \end{itemize}
    \item \textbf{Stato \texttt{Q\_{loop\_check}} (Controllo $N$ e Ciclo/Accetta):}
        \begin{itemize}
            \item Se Nastro 2 ha ancora \texttt{X} (altre $W_i$):
                \begin{itemize}
                    \item Nastro 1: legge \texttt{\#} o $\B$, riscrive, $R$.
                    \item Nastro 2: legge \texttt{X}, riscrive \texttt{X}, $S$. (Indica che ci sono ancora $W_i$ da processare, la cancellazione è stata fatta in $Q_3$).
                    \item Nastro 3: riavvolge.
                    \item Torna a $Q_3$.
                \end{itemize}
            \item Se Nastro 2 è $\B$ (tutte le $W_i$ sono state processate):
                \begin{itemize}
                    \item Nastro 1: legge $\B$, riscrive $\B$, $S$.
                    \item Accetta ($Q_{acc}$).
                \end{itemize}
        \end{itemize}
\end{itemize}

\subsection{Esercizio 4: $A$ o $B^R$ in $W_i$ in base alla Parità dell'Indice $i$}

\begin{definition}[Linguaggio $L_4$]
Sia $L_4$ il linguaggio definito come:
$L_4 = \{A\texttt{\#}B\texttt{\#}W_1 \texttt{\#} W_2 \texttt{\#} \dots \texttt{\#} W_N \mid A, B, W_i \in \{a,b,c,d\}^+, |A| > |B| > 0, N = |A| + |B|, $
$\qquad (\text{se } i \text{ è dispari, allora } A \subseteq W_i) \land (\text{se } i \text{ è pari, allora } B^R \subseteq W_i) \}$
\end{definition}

\subsubsection{Strategia della Macchina di Turing}
\begin{itemize}
    \item \textbf{Nastro 1 (Input):} Contiene la stringa di input.
    \item \textbf{Nastro 2 (Conteggio $N$ e Parità Indice):} Memorizza $N = |A| + |B|$. Per ogni $W_i$ processata, un \texttt{X} viene cancellato, permettendo al nastro di fungere anche da contatore di indice $i$ (implicitamente).
    \item \textbf{Nastro 3 (Copia $A$):} Memorizza la stringa $A$.
    \item \textbf{Nastro 4 (Copia $B$):} Memorizza la stringa $B$. Per $B^R$, Nastro 4 verrà letto all'indietro.
\end{itemize}

\subsubsection{Descrizione degli Stati e delle Transizioni}
Sia $\Sigma_I = \{a,b,c,d,\texttt{\#}\}$ l'alfabeto di input e $\Gamma_T = \Sigma_I \cup \{\B, \texttt{X}\}$ l'alfabeto del nastro. Usiamo $\alpha$ per un simbolo generico da $\{a,b,c,d\}$.

\begin{itemize}
    \item \textbf{Stato $Q_0$ (Copia $A$ e $|A|$ su Nastro 2):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Scrive \texttt{X} su Nastro 2, $R$. (Copia $A$ come \texttt{X}s su Nastro 2). Scrive $\alpha$ su Nastro 3, $R$. (Copia $A$ su Nastro 3).
            \item Alla lettura di \texttt{\#}: riscrive \texttt{\#}, $R$. Riavvolge Nastro 3 ($L$). Passa a $Q_1$.
        \end{itemize}
    \item \textbf{Stato $Q_1$ (Copia $B$ e Calcolo $|A|+|B|$ su Nastro 2):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Scrive \texttt{X} su Nastro 2, $R$. (Aggiunge $|B|$ a Nastro 2). Scrive $\alpha$ su Nastro 4, $R$. (Copia $B$ su Nastro 4).
            \item Alla lettura di \texttt{\#}: riscrive \texttt{\#}, $R$. Riavvolge Nastro 4 ($L$).
            \item Controlla $|A|>|B|>0$: questo avviene verificando che Nastro 2 ha almeno due \texttt{X}s dopo aver contato $A$ e $B$ e che Nastro 4 ha almeno un \texttt{X}. Implicitamente, si assicura che $B$ non sia vuoto e che $A$ sia più lungo di $B$.
            \item Passa a $Q_2$.
        \end{itemize}
    \item \textbf{Stato $Q_2$ (Preparazione e Inizio Loop $W_i$):}
        \begin{itemize}
            \item Riavvolge Nastro 2 a sinistra per posizionarsi sul primo \texttt{X} da consumare. Riavvolge Nastro 3 all'inizio di $A$. Riavvolge Nastro 4 all'inizio di $B$.
            \item Passa a $Q_3$.
        \end{itemize}
    \item \textbf{Stato $Q_3$ (Processa $W_i$ - indice dispari):}
        Questo stato gestisce $W_i$ con $i$ dispari (es. $W_1, W_3, \dots$).
        \begin{itemize}
            \item Su Nastro 2: legge \texttt{X}, lo cancella ($\B$), $L$. (Decrementa $N$ per questa $W_i$).
            \item Nondeterministicamente cerca l'inizio di $A$ in $W_i$ su Nastro 1 (legge $\alpha$, riscrive, $R$).
            \item Quando decide di iniziare il confronto: Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Legge $\alpha$ da Nastro 3, riscrive $\alpha$, $R$. Passa a $Q_4$.
        \end{itemize}
    \item \textbf{Stato $Q_4$ (Confronto $A$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Legge $\alpha$ da Nastro 3, riscrive $\alpha$, $R$.
            \item Loop in $Q_4$.
            \item Quando Nastro 3 è $\B$ (fine di $A$): Nastro 1 continua a leggere $\alpha$, riscrive $\alpha$, $R$. Passa a $Q_5$.
        \end{itemize}
    \item \textbf{Stato $Q_5$ (Pulizia dopo $A$ e Preparazione per $W_{i+1}$):}
        \begin{itemize}
            \item Riavvolge Nastro 3 a sinistra al $\B$ iniziale.
            \item Riavvolge Nastro 1 a destra fino al prossimo \texttt{\#} o $\B$.
            \item Quando \texttt{\#} su Nastro 1 è raggiunto e Nastro 3 è riavvolto: Legge \texttt{\#}, riscrive \texttt{\#}, $R$. Passa a $Q_6$.
        \end{itemize}
    \item \textbf{Stato $Q_6$ (Processa $W_i$ - indice pari):}
        Questo stato gestisce $W_i$ con $i$ pari (es. $W_2, W_4, \dots$).
        \begin{itemize}
            \item Su Nastro 2: legge \texttt{X}, lo cancella ($\B$), $L$. (Decrementa $N$ per questa $W_i$).
            \item Nondeterministicamente cerca l'inizio di $B^R$ in $W_i$ su Nastro 1 (legge $\alpha$, riscrive, $L$ - posizionandosi per il reverse check).
            \item Quando decide di iniziare il confronto: Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Legge $\alpha$ da Nastro 4, riscrive $\alpha$, $L$. Passa a $Q_7$.
        \end{itemize}
    \item \textbf{Stato $Q_7$ (Confronto $B^R$):}
        \begin{itemize}
            \item Legge $\alpha$ da Nastro 1, riscrive $\alpha$, $R$. Legge $\alpha$ da Nastro 4, riscrive $\alpha$, $L$.
            \item Loop in $Q_7$.
            \item Quando Nastro 4 è $\B$ (fine di $B^R$): Nastro 1 continua a leggere $\alpha$, riscrive $\alpha$, $R$. Passa a $Q_8$.
        \end{itemize}
    \item \textbf{Stato $Q_8$ (Pulizia dopo $B^R$ e Preparazione per $W_{i+1}$):}
        \begin{itemize}
            \item Riavvolge Nastro 4 a sinistra al $\B$ iniziale.
            \item Riavvolge Nastro 1 a destra fino al prossimo \texttt{\#} o $\B$.
            \item Quando \texttt{\#} su Nastro 1 è raggiunto e Nastro 4 è riavvolto: Legge \texttt{\#}, riscrive \texttt{\#}, $R$. Passa a $Q_3$ (per la prossima $W_i$ dispari).
        \end{itemize}
    \item \textbf{Accettazione ($Q_{acc}$):}
        \begin{itemize}
            \item Dopo l'elaborazione dell'ultima $W_N$, se Nastro 2 legge $\B$ (tutti gli \texttt{X} sono stati consumati) e Nastro 1 legge $\B$ (fine input):
                \begin{itemize}
                    \item $(\B, \B, \text{any}, \text{any}) \to (\B, \B, \text{any}, \text{any}), (S,S,S,S)$ e accetta.
                \end{itemize}
        \end{itemize}
\end{itemize}

\section{Conclusioni}
Le esercitazioni di oggi hanno permesso di approfondire la progettazione di Macchine di Turing Non Deterministiche, in particolare mostrando come la capacità di \textit{guess} (indovinare) combinata con un robusto \textit{check} (controllo) possa semplificare la logica di alcune verifiche complesse. È stato evidenziato come le NDTM possano \textit{scrivere in anticipo} su nastri ausiliari stringhe che verranno poi validate.
Nella prossima lezione, si inizierà a esplorare i concetti di calcolabilità, inclusi problemi indecidibili e il concetto fondamentale di riduzione, che sarà un pilastro per comprendere la complessità dei problemi.


% =====================================================
% --- START LECTURE 11 ---
% =====================================================

\chapter{Macchina di Turing Universale e Linguaggi Indecidibili}



\section{Introduzione e Ripasso}

Nella lezione precedente abbiamo introdotto le Macchine di Turing Non-Deterministiche (NTM) e le classi di problemi decidibili (R) e semi-decidibili (RE).

\begin{itemize}
    \item La simulazione di una \textbf{NTM} da parte di una \textbf{DTM} è possibile, sebbene con un costo significativo in termini di tempo di esecuzione (esponenziale o doppio esponenziale). Ai fini della sola \textit{decidibilità} di un problema, l'efficienza temporale non è un fattore determinante.
    \item La classe \textbf{R} include tutti i problemi \textbf{decidibili}, cioè per i quali esiste un algoritmo (una macchina di Turing) che termina sempre, fornendo una risposta definita (sì/no).
    \item La classe \textbf{RE} include i problemi \textbf{semi-decidibili}. Per questi, esiste una macchina di Turing che garantisce una risposta "sì" in tempo finito se la stringa appartiene al linguaggio; tuttavia, se la stringa non appartiene al linguaggio, la macchina potrebbe non terminare mai. Questo li rende, di fatto, non algoritmicamente utili per il "no".
    \item Esistono problemi completamente \textbf{indecidibili}, che non rientrano né in R né in RE. Per questi non è garantita alcuna risposta.
\end{itemize}

\section{La Macchina di Turing Universale (UTM)}

Le Macchine di Turing (MT) studiate finora sono intrinsecamente fisse nel loro comportamento: la loro funzione di transizione è immutabile e determinano un unico compito. I computer moderni, al contrario, sono programmabili, ovvero possono eseguire programmi diversi. Per colmare questa apparente discrepanza, introduciamo il concetto di \textbf{Macchina di Turing Universale (UTM)}.

\begin{definition}[Macchina di Turing Universale (UTM)]
Una Macchina di Turing Universale ($M_U$) è una macchina di Turing che è in grado di \textbf{simulare il comportamento di qualsiasi altra Macchina di Turing} ($M$) quando le viene fornita la descrizione (codifica) di $M$ e l'input $w$ su cui $M$ dovrebbe operare.
\end{definition}

\subsection{Codifica di una Macchina di Turing (MT)}

Per consentire a una UTM di simulare un'altra MT, il "programma" della MT (la sua funzione di transizione) deve essere codificato in una stringa binaria. Assumiamo, per semplicità, che le MT considerate abbiano un alfabeto di input binario $\Sigma = \{0,1\}$.

Una generica istruzione della funzione di transizione $\delta$ ha la forma:
\[ \delta(Q_i, X_j) = (Q_k, X_L, D_m) \]
dove:
\begin{itemize}
    \item $Q_i$: stato corrente.
    \item $X_j$: simbolo letto sul nastro.
    \item $Q_k$: nuovo stato dopo la transizione.
    \item $X_L$: simbolo da scrivere sul nastro.
    \item $D_m$: direzione del movimento della testina (sinistra o destra).
\end{itemize}

La codifica segue queste convenzioni:
\begin{enumerate}
    \item \textbf{Stati ($Q_i$)}: Ogni stato $Q_i$ è codificato con una sequenza di $i$ zeri.
    \begin{itemize}
        \item $Q_1$ è lo stato iniziale, codificato come $0$.
        \item $Q_2$ è lo stato accettante, codificato come $00$.
        \item Esempio: $Q_5$ è codificato come $00000$.
    \end{itemize}
    \item \textbf{Simboli di Nastro ($X_j$)}: Ogni simbolo $X_j$ è codificato con una sequenza di $j$ zeri.
    \begin{itemize}
        \item $X_1$ è il simbolo $0$, codificato come $0$.
        \item $X_2$ è il simbolo $1$, codificato come $00$.
        \item $X_3$ è il simbolo blank ($\phi$), codificato come $000$.
    \end{itemize}
    \item \textbf{Direzioni ($D_m$)}:
    \begin{itemize}
        \item $D_1$ (sinistra, L) è codificato come $0$.
        \item $D_2$ (destra, R) è codificato come $00$.
    \end{itemize}
    \item \textbf{Separatori}:
    \begin{itemize}
        \item Tra i componenti di una singola transizione (es. $Q_i$ e $X_j$), si usa il simbolo $1$.
        \item Tra diverse transizioni (coppie $\delta(\dots)$), si usa la sequenza $11$.
    \end{itemize}
\end{enumerate}

\begin{example}
Codifica della transizione: $\delta(Q_3, \phi) = (Q_5, 0, \rightarrow)$
\begin{itemize}
    \item $Q_3 \rightarrow \textbf{000}$
    \item $\phi$ ($X_3$) $\rightarrow \textbf{000}$
    \item $Q_5 \rightarrow \textbf{00000}$
    \item $0$ ($X_1$) $\rightarrow \textbf{0}$
    \item $\rightarrow$ ($D_2$) $\rightarrow \textbf{00}$
\end{itemize}
La codifica di questa singola transizione sarà:
$\textbf{000} \text{1} \textbf{000} \text{1} \textbf{00000} \text{1} \textbf{0} \text{1} \textbf{00}$

La codifica completa di una MT è la concatenazione di tutte le sue transizioni, separate da $11$. Ad esempio:
$\text{Codifica}(\delta_1) \textbf{11} \text{Codifica}(\delta_2) \textbf{11} \text{Codifica}(\delta_3) \dots$
\end{example}

\begin{remark}
Non tutte le stringhe binarie corrisponderanno a una MT validamente codificata. Per convenzione, le stringhe che non seguono il formato di codifica (es. quelle con $111$ o $1111$, o quelle non interpretabili come programma) sono considerate codifiche di MT che rifiutano qualsiasi input (il cui linguaggio è vuoto).
\end{remark}

\subsection{Architettura e Funzionamento della UTM}

La UTM ($M_U$) è tipicamente una Macchina di Turing a quattro nastri. Riceve come input una stringa che è la concatenazione della codifica di una MT $M$ e di una stringa $w$, separate da una sequenza di $1$ (es. $111$ per distinguere dalla separazione delle transizioni). L'obiettivo di $M_U$ è simulare il comportamento di $M$ su $w$.

I quattro nastri di $M_U$ hanno i seguenti ruoli:
\begin{enumerate}
    \item \textbf{Nastro 1 (Input/Programma $M$)}: Contiene la stringa di codifica di $M$. Questo nastro è letto dalla $M_U$ per conoscere le regole di transizione di $M$ e non viene modificato durante la simulazione.
    \item \textbf{Nastro 2 (Nastro Simulato di $M$)}: Contiene la rappresentazione del nastro della MT $M$. Inizialmente, l'input $w$ viene copiato su questo nastro e ricodificato nel formato dei simboli $X_j$ (es. $0$ diventa $0$, $1$ diventa $00$, etc.). La testina della $M_U$ su questo nastro simula la posizione della testina di $M$.
    \item \textbf{Nastro 3 (Stato Simulato di $M$)}: Contiene la codifica dello stato corrente in cui si trova la MT $M$ nella simulazione. Inizia con lo stato iniziale di $M$ ($Q_1$).
    \item \textbf{Nastro 4 (Scratchpad/Ausiliario)}: Utilizzato per operazioni temporanee, come la gestione dello spazio sul Nastro 2 quando un simbolo deve essere riscritto e la sua codifica occupa più o meno spazio.
\end{enumerate}

\subsubsection{Passi di Simulazione}
$M_U$ esegue la simulazione passo per passo:
\begin{enumerate}
    \item $M_U$ legge lo stato corrente di $M$ (dal Nastro 3) e il simbolo sotto la testina di $M$ (dal Nastro 2).
    \item Usando questa coppia (stato, simbolo), $M_U$ cerca sul Nastro 1 (il "programma" di $M$) la transizione $\delta(Q_i, X_j) = (Q_k, X_L, D_m)$ corrispondente.
    \item Una volta trovata la transizione, $M_U$ aggiorna la simulazione:
    \begin{itemize}
        \item Aggiorna lo stato sul Nastro 3 a $Q_k$.
        \item Scrive il simbolo $X_L$ sul Nastro 2 (spostando il contenuto se necessario, usando il Nastro 4).
        \item Sposta la testina sul Nastro 2 nella direzione $D_m$.
    \end{itemize}
    \item $M_U$ ripete questi passi. Se $M$ entra in uno stato di accettazione (la $M_U$ lo rileva leggendo il Nastro 3), allora $M_U$ accetta. Se $M$ si blocca in uno stato non accettante, $M_U$ si blocca. Se $M$ entra in un loop infinito, $M_U$ entrerà anch'essa in un loop infinito.
\end{enumerate}
In questo modo, $M_U$ replica fedelmente il comportamento di $M$ su $w$.

\subsection{Connessione con i Computer Reali}

Un computer che utilizziamo quotidianamente è l'equivalente di una Macchina di Turing Universale. Il suo "programma" fisso e immutabile non è l'applicazione che lanciamo, ma il \textbf{microcodice della CPU}. La CPU esegue ciclicamente un programma interno fisso (il microcodice) che è progettato per \textit{interpretare} ed \textit{eseguire} le istruzioni dei programmi che gli vengono dati in input (le nostre applicazioni). Questo risolve il dilemma: i computer non "cambiano" programma, ma hanno un programma fisso (il microcodice) che li rende universali, capaci di simulare qualsiasi altro programma.

\section{Linguaggi Indecidibili: L'Argomento di Diagonalizzazione}

Abbiamo affermato l'esistenza di problemi completamente indecidibili (non in RE). Una delle dimostrazioni più celebri di ciò è l'argomento di diagonalizzazione di Cantor.

\subsection{Enumerazione di Stringhe e Macchine di Turing}

\begin{enumerate}
    \item \textbf{Enumerazione di stringhe binarie ($\Sigma^*$)}: L'insieme di tutte le stringhe binarie finite ($\Sigma^*$, dove $\Sigma = \{0,1\}$) è \textbf{numerabile}. Possiamo ordinarle per lunghezza crescente e, a parità di lunghezza, lessicograficamente. Questo ci permette di parlare della $1^{a}$ stringa, $2^{a}$ stringa, ecc. (es., $\epsilon, 0, 1, 00, 01, 10, 11, \dots$). La cardinalità di $\Sigma^*$ è $\aleph_0$ (aleph-zero), la stessa dei numeri naturali.
    \item \textbf{Enumerazione delle Macchine di Turing (MT)}: Poiché ogni MT può essere codificata come una stringa binaria finita, e le stringhe binarie sono numerabili, anche l'insieme di tutte le Macchine di Turing è \textbf{numerabile}. Possiamo quindi parlare della $1^{a}$ MT ($M_1$), della $2^{a}$ MT ($M_2$), ecc.
\end{enumerate}

\subsection{Vettore Caratteristico di un Linguaggio}

\begin{definition}[Vettore Caratteristico ($Q_L$)]
Per un linguaggio $L$ su un alfabeto $\Sigma$, il suo \textbf{vettore caratteristico} $Q_L$ è una sequenza binaria infinita. Per ogni $i$-esima stringa $w_i$ (secondo l'ordine di enumerazione di $\Sigma^*$), la $i$-esima componente di $Q_L$ è $1$ se $w_i \in L$, e $0$ se $w_i \notin L$. Questo vettore descrive in modo univoco il linguaggio $L$.
\end{definition}

\subsection{Costruzione del Linguaggio di Diagonalizzazione ($L_D$)}

Consideriamo una tabella infinita dove:
\begin{itemize}
    \item Le \textbf{colonne} sono indicizzate dalle stringhe enumerate di $\Sigma^*$: $w_1, w_2, w_3, \dots$
    \item Le \textbf{righe} sono indicizzate dalle MT enumerate: $M_1, M_2, M_3, \dots$
    \item La cella $(i, j)$ contiene $1$ se la MT $M_i$ \textbf{accetta} la stringa $w_j$, e $0$ altrimenti.
\end{itemize}
Ogni riga $i$ di questa tabella rappresenta il vettore caratteristico del linguaggio $L(M_i)$ accettato dalla macchina $M_i$.

Esempio di una porzione di tabella:
\[
\begin{array}{c|cccccc}
        & w_1 & w_2 & w_3 & w_4 & \dots \\
    \hline
    M_1 & 0 & 1 & 0 & 0 & \dots & \quad (L(M_1)) \\
    M_2 & 1 & 1 & 0 & 1 & \dots & \quad (L(M_2)) \\
    M_3 & 0 & 0 & 0 & 1 & \dots & \quad (L(M_3)) \\
    M_4 & 1 & 0 & 0 & 1 & \dots & \quad (L(M_4)) \\
    \vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\
\end{array}
\]

Procediamo alla costruzione del linguaggio di diagonalizzazione ($L_D$):
\begin{enumerate}
    \item \textbf{Prendere la diagonale ($D$)}: Estrarre la sequenza dei bit sulla diagonale principale: $D = d_1 d_2 d_3 d_4 \dots$, dove $d_i$ è il valore della cella $(i,i)$ (cioè se $M_i$ accetta $w_i$).
    \textit{Nell'esempio sopra:} $D = 0101\dots$
    \item \textbf{Complementare la diagonale ($\overline{D}$)}: Invertire ogni bit della sequenza $D$.
    $\overline{D} = \overline{d_1} \overline{d_2} \overline{d_3} \overline{d_4} \dots$
    \textit{Nell'esempio sopra:} $\overline{D} = 1010\dots$
    \item \textbf{Definire $L_D$}: Interpretiamo $\overline{D}$ come il vettore caratteristico di un nuovo linguaggio, $L_D$.
    Formalmente, $L_D = \{ w_i \mid \text{l'i-esima bit di } \overline{D} \text{ è } 1 \}$.
\end{enumerate}

\subsection{Definizione Formale di $L_D$}

Il linguaggio $L_D$ può essere più intuitivamente definito come:
\begin{definition}[Linguaggio di Diagonalizzazione $L_D$]
$L_D = \{ \langle M_i \rangle \mid M_i \text{ è una MT e } M_i \text{ rifiuta la propria codifica } \langle M_i \rangle \}$
dove $\langle M_i \rangle$ (o $w_i$) denota la stringa che codifica la MT $M_i$.
In altre parole, una stringa $\langle M_i \rangle$ appartiene a $L_D$ se e solo se la macchina $M_i$ non accetta $\langle M_i \rangle$.
\end{definition}

\subsection{Teorema: $L_D$ non appartiene a RE}

\begin{theorem}
Il linguaggio $L_D$ non appartiene alla classe RE (Linguaggi Ricorsivamente Enumerabili). In altre parole, non esiste alcuna Macchina di Turing che accetta $L_D$.
\end{theorem}

\begin{proof}
Supponiamo per contraddizione che $L_D$ sia ricorsivamente enumerabile. Se $L_D \in RE$, allora esisterebbe una Macchina di Turing, diciamo $M_k$, che accetta $L_D$. Poiché tutte le MT sono enumerate nella nostra tabella, $M_k$ corrisponde a una specifica riga $k$ in quella tabella.

Consideriamo la stringa $w_k$, che è la codifica $\langle M_k \rangle$ della macchina $M_k$.
Ora poniamoci la domanda: $w_k \in L_D$ o $w_k \notin L_D$?

\begin{enumerate}[(i)]
    \item \textbf{Caso 1: Supponiamo $w_k \in L_D$.}
    \begin{itemize}
        \item Per definizione di $L_D$, se $w_k \in L_D$, allora $M_k$ deve rifiutare $\langle M_k \rangle$ (cioè $w_k$).
        \item Se $M_k$ rifiuta $w_k$, allora la cella $(k,k)$ della tabella deve essere $0$.
        \item D'altra parte, poiché $M_k$ accetta $L_D$, e abbiamo supposto $w_k \in L_D$, $M_k$ dovrebbe accettare $w_k$. Ciò implicherebbe che la cella $(k,k)$ è $1$.
    \end{itemize}
    Abbiamo una contraddizione: la cella $(k,k)$ non può essere contemporaneamente $0$ e $1$.

    \item \textbf{Caso 2: Supponiamo $w_k \notin L_D$.}
    \begin{itemize}
        \item Per definizione di $L_D$, se $w_k \notin L_D$, allora $M_k$ deve accettare $\langle M_k \rangle$ (cioè $w_k$).
        \item Se $M_k$ accetta $w_k$, allora la cella $(k,k)$ della tabella deve essere $1$.
        \item D'altra parte, poiché $M_k$ accetta $L_D$, e abbiamo supposto $w_k \notin L_D$, $M_k$ dovrebbe rifiutare $w_k$. Ciò implicherebbe che la cella $(k,k)$ è $0$.
    \end{itemize}
    Abbiamo di nuovo una contraddizione: la cella $(k,k)$ non può essere contemporaneamente $1$ e $0$.
\end{enumerate}
Poiché in entrambi i casi si arriva a una contraddizione, l'assunzione iniziale che $L_D \in RE$ deve essere falsa. Pertanto, $L_D$ non appartiene a RE.
\end{proof}

\subsection{Intuizione sull'Esistenza di Linguaggi Indecidibili}

La ragione profonda per l'esistenza di linguaggi indecidibili (e non ricorsivamente enumerabili) è una questione di \textbf{cardinalità} degli insiemi:
\begin{itemize}
    \item L'insieme di tutte le \textbf{Macchine di Turing} è numerabile (ha cardinalità $\aleph_0$). Ogni MT riconosce esattamente un linguaggio.
    \item L'insieme di tutti i \textbf{linguaggi} su un alfabeto infinito (come $\Sigma^*$) è l'insieme potenza di $\Sigma^*$. Poiché $\Sigma^*$ è numerabile, il suo insieme potenza ha la cardinalità del continuo ($2^{\aleph_0}$), che è strettamente maggiore di $\aleph_0$.
\end{itemize}
Questo significa che ci sono \textbf{molti più linguaggi che Macchine di Turing}. Dal momento che non ci sono abbastanza MT per riconoscere tutti i linguaggi possibili, devono esistere linguaggi per i quali non esiste alcuna MT in grado di accettarli. $L_D$ è un esempio concreto di uno di questi linguaggi.

\section{Proprietà sui Linguaggi Ricorsivi e Ricorsivamente Enumerabili}

Introduciamo due importanti teoremi che collegano le classi R e RE con le operazioni di complemento.

\begin{theorem}[Chiusura di R sotto Complemento]
Se un linguaggio $L$ appartiene alla classe R (è ricorsivo), allora anche il suo complemento $\overline{L}$ appartiene a R.
\end{theorem}
\begin{proof}
Sia $L \in R$. Per definizione, esiste una MT $M_L$ che decide $L$. Questo significa che per ogni stringa $w$ data in input, $M_L$ termina sempre, accettando $w$ se $w \in L$ e rifiutando $w$ se $w \notin L$.

Possiamo costruire una nuova MT $M_{\overline{L}}$ per decidere $\overline{L}$ come segue:
$M_{\overline{L}}$ simula $M_L$ su qualsiasi input $w$. Poiché $M_L$ termina sempre:
\begin{itemize}
    \item Se $M_L$ accetta $w$ (cioè $w \in L$), allora $M_{\overline{L}}$ modifica il suo comportamento per rifiutare $w$.
    \item Se $M_L$ rifiuta $w$ (cioè $w \notin L$), allora $M_{\overline{L}}$ modifica il suo comportamento per accettare $w$.
\end{itemize}
In pratica, $M_{\overline{L}}$ può essere costruita da $M_L$ semplicemente scambiando gli stati di accettazione e rifiuto (o, più precisamente, rendendo accettanti gli stati che prima conducevano al rifiuto e viceversa, assicurandosi che tutte le computazioni terminino).
Dato che $M_L$ termina sempre, anche $M_{\overline{L}}$ terminerà sempre, fornendo una decisione per ogni input. Quindi, $M_{\overline{L}}$ è una macchina che decide $\overline{L}$, il che implica $\overline{L} \in R$.
\end{proof}

\begin{theorem}[Condizione Necessaria e Sufficiente per R]
Un linguaggio $L$ appartiene alla classe R (è ricorsivo) se e solo se $L$ appartiene alla classe RE \textbf{e} il suo complemento $\overline{L}$ appartiene anch'esso alla classe RE.
\[ L \in R \iff L \in RE \land \overline{L} \in RE \]
\end{theorem}
\begin{proof}
($\Rightarrow$) \textbf{Se $L \in R$, allora $L \in RE \land \overline{L} \in RE$.}
Se $L \in R$, allora per definizione $L$ è decidibile. Ogni linguaggio decidibile è anche ricorsivamente enumerabile, quindi $L \in RE$.
Inoltre, per il teorema precedente, se $L \in R$, allora $\overline{L} \in R$. Di conseguenza, anche $\overline{L} \in RE$.

($\Leftarrow$) \textbf{Se $L \in RE \land \overline{L} \in RE$, allora $L \in R$.}
Supponiamo che $L \in RE$ e $\overline{L} \in RE$.
\begin{itemize}
    \item Poiché $L \in RE$, esiste una Macchina di Turing $M_L$ che accetta $L$ (garantisce terminazione per i "sì", ma potrebbe non terminare per i "no").
    \item Poiché $\overline{L} \in RE$, esiste una Macchina di Turing $M_{\overline{L}}$ che accetta $\overline{L}$ (garantisce terminazione per i "sì" in $\overline{L}$, ovvero per i "no" in $L$).
\end{itemize}
Possiamo costruire una nuova MT $M_{decider}$ che decide $L$. $M_{decider}$ opera come segue su qualsiasi input $w$:
\begin{enumerate}
    \item $M_{decider}$ simula $M_L$ e $M_{\overline{L}}$ in \textbf{parallelo}. Questo può essere fatto, ad esempio, alternando i passi di esecuzione delle due macchine (un passo di $M_L$, poi un passo di $M_{\overline{L}}$, poi un altro passo di $M_L$, ecc., utilizzando un approccio multi-nastro o di interleaving).
    \item Se la simulazione di $M_L$ accetta $w$ (ovvero $M_L$ entra in uno stato di accettazione), allora $M_{decider}$ \textbf{accetta} $w$.
    \item Se la simulazione di $M_{\overline{L}}$ accetta $w$ (ovvero $M_{\overline{L}}$ entra in uno stato di accettazione), allora $M_{decider}$ \textbf{rifiuta} $w$.
\end{enumerate}
Dato che $w$ deve appartenere o a $L$ o a $\overline{L}$ (ma non a entrambi), una delle due macchine ($M_L$ o $M_{\overline{L}}$) è garantita a terminare e accettare $w$. Pertanto, $M_{decider}$ terminerà sempre su ogni input $w$, fornendo una risposta definita ("sì" o "no"). Questo dimostra che $M_{decider}$ decide $L$, e quindi $L \in R$.
\end{proof}

Questo teorema è cruciale: esso stabilisce che un problema è decidibile se e solo se sia il problema stesso che il suo complemento sono ricorsivamente enumerabili. Queste proprietà saranno fondamentali per classificare altri importanti problemi nelle prossime lezioni.


% =====================================================
% --- START LECTURE 12 ---
% =====================================================

\chapter{Decidibilità e Indecidibilità}



\section{Introduzione e Ripasso}
Ripassiamo i concetti fondamentali della scorsa lezione, chiarendo alcuni punti sulla Macchina Universale (MU) e introducendo nuovi linguaggi e problemi legati alla decidibilità.

\subsection{Macchina Universale (MU)}
La Macchina Universale è una Macchina di Turing programmabile, capace di simulare il comportamento di qualsiasi altra Macchina di Turing. Sebbene la sua funzione di transizione sia complessa, la sua progettazione è fattibile e si può realizzare con un numero limitato di stati.

\subsection{Linguaggio Diagonale (LD)}
La scorsa lezione abbiamo introdotto il linguaggio diagonale $LD$:
\begin{definition}[Linguaggio Diagonale (LD)]
Il linguaggio diagonale $LD$ è l'insieme delle codifiche di Macchine di Turing $M_i$ tali che $M_i$ non accetta la propria codifica $\langle M_i \rangle$.
\[ LD = \{ \langle M_i \rangle \mid M_i \text{ non accetta } \langle M_i \rangle \} \]
\end{definition}

Abbiamo dimostrato che $LD$ non appartiene alla classe dei linguaggi ricorsivamente enumerabili ($R_e$).
\begin{theorem}
$LD \notin R_e$.
\end{theorem}
\begin{proof}
La dimostrazione si basa sulla tecnica di diagonalizzazione di Cantor, già vista in precedenza. Assumendo per assurdo l'esistenza di una Macchina di Turing $M_D$ che accetta $LD$, si può costruire una contraddizione nel comportamento di $M_D$ sulla propria codifica.
\end{proof}

Una conseguenza diretta di questo teorema è che $LD$ non può appartenere nemmeno alla classe dei linguaggi ricorsivi ($R$).
\begin{proposition}
Se $L \in R$, allora $L \in R_e$.
Poiché $LD \notin R_e$, allora $LD \notin R$.
\end{proposition}

\section{Linguaggio Universale (LU)}

Introduciamo il Linguaggio Universale.
\begin{definition}[Linguaggio Universale (LU)]
Il linguaggio universale $LU$ è l'insieme delle coppie $\langle M, w \rangle$, dove $M$ è una Macchina di Turing e $w$ è una stringa, tali che $M$ accetta $w$. L'operatore "accetta" (indicato anche con $\models$) implica che $M$ si arresta in uno stato accettante su $w$.
\[ LU = \{ \langle M, w \rangle \mid M \text{ accetta } w \} \]
\end{definition}

\begin{theorem}
$LU \in R_e$.
\end{theorem}
\begin{proof}
Per dimostrare che $LU \in R_e$, dobbiamo mostrare l'esistenza di una Macchina di Turing che lo accetti. Tale macchina è la Macchina Universale (MU).
\begin{enumerate}
    \item La MU prende in input la codifica $\langle M, w \rangle$.
    \item La MU simula passo-passo il comportamento di $M$ su $w$.
    \item Se $M$ si arresta in uno stato accettante su $w$, la MU si arresta e accetta (risponde "sì").
    \item Se $M$ si arresta in uno stato non accettante su $w$, la MU si arresta e rifiuta (risponde "no").
    \item Se $M$ non si arresta su $w$ (entra in un loop infinito), la MU simulerà all'infinito e non si arresterà (non darà risposta "sì").
\end{enumerate}
Poiché la MU accetta tutte le istanze di $LU$ e non accetta quelle che non vi appartengono, $LU$ è ricorsivamente enumerabile.
\end{proof}

Ora, la domanda cruciale: $LU \in R$? Intuitivamente, se $M$ va in loop su $w$, la MU non si arresterà per dare una risposta "no". Questo suggerisce che $LU$ non sia decidibile.

\begin{theorem}
$LU \notin R$.
\end{theorem}
\begin{proof}
La dimostrazione procede per assurdo, utilizzando una riduzione da $LD$ (di cui sappiamo la non appartenenza a $R_e$, e quindi a $R$).

\textbf{Assunzione per assurdo:} Supponiamo che $LU \in R$.
\begin{enumerate}
    \item Se $LU \in R$, allora per le proprietà delle classi di linguaggi, anche il suo complemento $\overline{LU} \in R$.
    \item Se $\overline{LU} \in R$, allora esiste una Macchina di Turing $M_{\overline{LU}}$ che \textbf{decide} $\overline{LU}$. Ciò significa che $M_{\overline{LU}}$ si arresta sempre (su ogni input) e dà una risposta corretta (sì/no).
    \item Costruiamo una nuova Macchina di Turing $M'$ che prende in input una codifica di macchina $\langle M_i \rangle$. Il comportamento di $M'$ è il seguente:
    \begin{enumerate}
        \item Riceve $\langle M_i \rangle$ come input.
        \item Crea una copia di $\langle M_i \rangle$ e la usa come stringa $w$. Forma la coppia $\langle M_i, \langle M_i \rangle \rangle$. (Quindi l'input per la fase successiva è $\langle M_i, w \rangle$ dove $w = \langle M_i \rangle$).
        \item Dà in input la coppia $\langle M_i, \langle M_i \rangle \rangle$ alla macchina $M_{\overline{LU}}$ (la cui esistenza è garantita dalla nostra assunzione).
        \item $M'$ adotta la risposta di $M_{\overline{LU}}$:
        \begin{itemize}
            \item Se $M_{\overline{LU}}$ risponde "sì", allora $M'$ risponde "sì".
            \item Se $M_{\overline{LU}}$ risponde "no", allora $M'$ risponde "no".
        \end{itemize}
    \end{enumerate}
\end{enumerate}
Ora analizziamo il linguaggio deciso da $M'$, $L(M')$:
\begin{itemize}
    \item \textbf{Se $M'$ risponde "sì":}
    Ciò significa che $M_{\overline{LU}}$ ha risposto "sì" sull'input $\langle M_i, \langle M_i \rangle \rangle$.
    Poiché $M_{\overline{LU}}$ decide $\overline{LU}$, la risposta "sì" implica che $\langle M_i, \langle M_i \rangle \rangle \in \overline{LU}$.
    Per definizione di $\overline{LU}$, ciò significa che $M_i$ \textbf{non accetta} $\langle M_i \rangle$.
    \item \textbf{Se $M'$ risponde "no":}
    Ciò significa che $M_{\overline{LU}}$ ha risposto "no" sull'input $\langle M_i, \langle M_i \rangle \rangle$.
    Poiché $M_{\overline{LU}}$ decide $\overline{LU}$, la risposta "no" implica che $\langle M_i, \langle M_i \rangle \rangle \notin \overline{LU}$, ovvero $\langle M_i, \langle M_i \rangle \rangle \in LU$.
    Per definizione di $LU$, ciò significa che $M_i$ \textbf{accetta} $\langle M_i \rangle$.
\end{itemize}
Il comportamento di $M'$ è esattamente quello di una macchina che decide $LD$. Infatti, $M'$ risponde "sì" se $M_i$ non accetta $\langle M_i \rangle$, e "no" se $M_i$ accetta $\langle M_i \rangle$. Dunque, $L(M') = LD$.

Poiché $M'$ è costruita usando $M_{\overline{LU}}$ (che è un decider e si arresta sempre), $M'$ è anch'essa una Macchina di Turing che si arresta sempre, ovvero un decider.
Questo implicherebbe che $LD \in R$.

\textbf{Contraddizione:} Sappiamo che $LD \notin R_e$, e quindi $LD \notin R$.
L'assunzione iniziale ($LU \in R$) deve essere falsa.

\textbf{Conclusione:} $LU \notin R$.
\end{proof}

\subsection{Il Complemento del Linguaggio Diagonale ($\overline{LD}$)}
Consideriamo ora il complemento di $LD$:
\begin{definition}[Complemento del Linguaggio Diagonale ($\overline{LD}$)]
Il complemento del linguaggio diagonale $\overline{LD}$ è l'insieme delle codifiche di Macchine di Turing $M_i$ tali che $M_i$ accetta la propria codifica $\langle M_i \rangle$.
\[ \overline{LD} = \{ \langle M_i \rangle \mid M_i \text{ accetta } \langle M_i \rangle \} \]
\end{definition}

\begin{theorem}
$\overline{LD} \in R_e$.
\end{theorem}
\begin{proof}
Per dimostrare che $\overline{LD} \in R_e$, dobbiamo costruire una Macchina di Turing $M_{\overline{LD}}$ che accetti $\overline{LD}$.

\textbf{Costruzione di $M_{\overline{LD}}$:}
\begin{enumerate}
    \item $M_{\overline{LD}}$ prende in input la codifica di una Macchina di Turing $\langle M_i \rangle$.
    \item $M_{\overline{LD}}$ crea una copia di $\langle M_i \rangle$ per usarla come stringa $w$. Forma quindi la coppia $\langle M_i, \langle M_i \rangle \rangle$.
    \item $M_{\overline{LD}}$ simula $M_i$ su $\langle M_i \rangle$ usando una Macchina Universale (MU).
    \item Se la simulazione di $M_i$ su $\langle M_i \rangle$ si arresta e accetta, allora $M_{\overline{LD}}$ accetta (risponde "sì").
    \item Se la simulazione di $M_i$ su $\langle M_i \rangle$ si arresta e rifiuta, o entra in loop, allora $M_{\overline{LD}}$ non accetta (risponde "no" o va in loop).
\end{enumerate}
\textbf{Analisi del comportamento di $M_{\overline{LD}}$:}
\begin{itemize}
    \item \textbf{Se $\langle M_i \rangle \in \overline{LD}$:}
    Per definizione, $M_i$ accetta $\langle M_i \rangle$. La simulazione della MU si arresterà e accetterà. Di conseguenza, $M_{\overline{LD}}$ accetterà.
    \item \textbf{Se $\langle M_i \rangle \notin \overline{LD}$:}
    Per definizione, $M_i$ non accetta $\langle M_i \rangle$. Ciò significa che $M_i$ o rifiuta o va in loop su $\langle M_i \rangle$.
    \begin{itemize}
        \item Se $M_i$ rifiuta $\langle M_i \rangle$, la simulazione della MU si arresterà e rifiuterà. $M_{\overline{LD}}$ non accetterà.
        \item Se $M_i$ va in loop su $\langle M_i \rangle$, la simulazione della MU andrà in loop. $M_{\overline{LD}}$ non accetterà.
    \end{itemize}
\end{itemize}
Poiché $M_{\overline{LD}}$ accetta esattamente le stringhe che appartengono a $\overline{LD}$, si conclude che $\overline{LD} \in R_e$.
\end{proof}

\begin{proposition}
$\overline{LD} \notin R$.
\end{proposition}
\begin{proof}
Se $\overline{LD}$ fosse in $R$, allora per la proprietà che $R$ è chiusa rispetto al complemento, anche $LD$ sarebbe in $R$. Ma sappiamo che $LD \notin R_e$, e quindi $LD \notin R$. Questo è una contraddizione.
\end{proof}

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=1.8cm, >=Stealth]
        \draw (0,0) circle (3cm) node[below] {$R_e$};
        \draw (0,0) circle (1.5cm) node[above] {$R$};
        \node at (2.5, 2.5) {$\overline{LU}$};
        \node at (2.5, -2.5) {$\overline{HALT}$};
        \node at (1.0, 1.0) {$LU$};
        \node at (1.0, -1.0) {$HALT$};
        \node at (-1.0, 1.0) {$\overline{LD}$};
        \node at (-1.0, -1.0) {$LD$}; % Errore di posizionamento nella figura del professore, LD è fuori Re. Correggo qui.
        
        % Aggiustamenti per chiarezza e correttezza (LD fuori Re)
        \node at (4.0, 0) {$LD$}; % LD è fuori Re
        \node at (-4.0, 0) {?}; % Altri linguaggi non in Re
    \end{tikzpicture}
    \caption{Relazioni tra classi di linguaggi R e $R_e$ e posizione di alcuni linguaggi}
    \label{fig:r_re_map}
\end{figure}

\section{Problema dell'Arresto (HALT)}
Introduciamo il famoso Problema dell'Arresto.
\begin{definition}[Problema dell'Arresto (HALT)]
Il linguaggio $HALT$ è l'insieme delle coppie $\langle M, w \rangle$, dove $M$ è una Macchina di Turing e $w$ è una stringa, tali che $M$ si arresta su $w$ (indipendentemente dal fatto che accetti o rifiuti).
\[ HALT = \{ \langle M, w \rangle \mid M \text{ si arresta su } w \} \]
\end{definition}
La differenza con $LU$ è sottile ma cruciale: $LU$ richiede l'accettazione, $HALT$ richiede solo l'arresto.

\begin{theorem}
$HALT \in R_e$.
\end{theorem}
\begin{proof}
Per dimostrare che $HALT \in R_e$, dobbiamo costruire una Macchina di Turing $M_{HALT}$ che accetti $HALT$.

\textbf{Costruzione di $M_{HALT}$:}
\begin{enumerate}
    \item $M_{HALT}$ prende in input la coppia $\langle M, w \rangle$.
    \item $M_{HALT}$ simula $M$ su $w$ usando una Macchina Universale (MU).
    \item Se la simulazione di $M$ su $w$ si arresta (sia in uno stato accettante che non accettante), allora $M_{HALT}$ accetta (risponde "sì").
    \item Se la simulazione di $M$ su $w$ entra in loop infinito, allora $M_{HALT}$ entra in loop (non risponde "sì").
\end{enumerate}
\textbf{Analisi del comportamento di $M_{HALT}$:}
\begin{itemize}
    \item \textbf{Se $\langle M, w \rangle \in HALT$:}
    Per definizione, $M$ si arresta su $w$. La simulazione della MU si arresterà. Di conseguenza, $M_{HALT}$ accetterà.
    \item \textbf{Se $\langle M, w \rangle \notin HALT$:}
    Per definizione, $M$ non si arresta su $w$ (va in loop). La simulazione della MU andrà in loop. Di conseguenza, $M_{HALT}$ non accetterà.
\end{itemize}
Poiché $M_{HALT}$ accetta esattamente le istanze di $HALT$, si conclude che $HALT \in R_e$.
\end{proof}

\begin{theorem}
$HALT \notin R$.
\end{theorem}
\begin{proof}
La dimostrazione procede per assurdo, utilizzando una riduzione da $LU$ (di cui sappiamo la non appartenenza a $R$).

\textbf{Assunzione per assurdo:} Supponiamo che $HALT \in R$.
\begin{enumerate}
    \item Se $HALT \in R$, allora esiste una Macchina di Turing $M_{HALT}^*$ che \textbf{decide} $HALT$. Ciò significa che $M_{HALT}^*$ si arresta sempre e dà una risposta corretta (sì/no).
    \item Costruiamo una nuova Macchina di Turing $M'$ che prende in input una coppia $\langle M, w \rangle$. Il comportamento di $M'$ è il seguente:
    \begin{enumerate}
        \item Riceve $\langle M, w \rangle$ come input.
        \item Dà in input $\langle M, w \rangle$ alla macchina $M_{HALT}^*$ (la cui esistenza è garantita dalla nostra assunzione).
        \item $M'$ verifica la risposta di $M_{HALT}^*$:
        \begin{itemize}
            \item \textbf{Se $M_{HALT}^*$ risponde "no"}: (Significa che $M$ non si arresta su $w$). Allora $M'$ risponde "no". (In questo caso, $M$ non può accettare $w$, quindi $\langle M, w \rangle \notin LU$).
            \item \textbf{Se $M_{HALT}^*$ risponde "sì"}: (Significa che $M$ si arresta su $w$). Allora $M'$ simula $M$ su $w$ usando una Macchina Universale (MU).
            \begin{itemize}
                \item Se la simulazione di $M$ su $w$ si arresta e accetta, allora $M'$ risponde "sì".
                \item Se la simulazione di $M$ su $w$ si arresta e rifiuta, allora $M'$ risponde "no".
            \end{itemize}
        \end{itemize}
    \end{enumerate}
\end{enumerate}
Ora analizziamo il linguaggio deciso da $M'$, $L(M')$:
\begin{itemize}
    \item \textbf{Se $M'$ risponde "sì":}
    Ciò accade solo se $M_{HALT}^*$ ha risposto "sì" (cioè $M$ si arresta su $w$) \textbf{e} la simulazione di $M$ su $w$ ha accettato.
    Questo significa che $M$ accetta $w$. Quindi, $\langle M, w \rangle \in LU$.
    \item \textbf{Se $M'$ risponde "no":}
    Ciò può accadere in due scenari:
    \begin{itemize}
        \item Scenario 1: $M_{HALT}^*$ ha risposto "no". Questo significa che $M$ non si arresta su $w$. Se $M$ non si arresta, non può accettare $w$. Quindi, $\langle M, w \rangle \notin LU$.
        \item Scenario 2: $M_{HALT}^*$ ha risposto "sì", ma la simulazione di $M$ su $w$ ha rifiutato. Questo significa che $M$ si è arrestata su $w$ ma non ha accettato $w$. Quindi, $\langle M, w \rangle \notin LU$.
    \end{itemize}
    In entrambi gli scenari, $M'$ risponde "no" se $\langle M, w \rangle \notin LU$.
\end{itemize}
Il comportamento di $M'$ è esattamente quello di una macchina che decide $LU$. Dunque, $L(M') = LU$.

Poiché $M'$ è costruita usando $M_{HALT}^*$ (che è un decider e si arresta sempre), e la parte di simulazione dopo la risposta "sì" di $M_{HALT}^*$ è garantita arrestarsi, $M'$ è anch'essa una Macchina di Turing che si arresta sempre, ovvero un decider.
Questo implicherebbe che $LU \in R$.

\textbf{Contraddizione:} Sappiamo che $LU \notin R$.
L'assunzione iniziale ($HALT \in R$) deve essere falsa.

\textbf{Conclusione:} $HALT \notin R$.
\end{proof}

\begin{proposition}
Il complemento di $HALT$, $\overline{HALT}$, non appartiene a $R$.
\end{proposition}
\begin{proof}
Se $\overline{HALT}$ fosse in $R$, allora per la proprietà che $R$ è chiusa rispetto al complemento, anche $HALT$ sarebbe in $R$. Ma abbiamo appena dimostrato che $HALT \notin R$. Questo è una contraddizione.
\end{proof}

\section{Problema dell'Arresto su Stringa Vuota (HALT$_\epsilon$)}
Questa è una variante specifica del problema dell'arresto.
\begin{definition}[Problema dell'Arresto su Stringa Vuota (HALT$_\epsilon$)]
Il linguaggio $HALT_\epsilon$ è l'insieme delle codifiche di Macchine di Turing $M$ tali che $M$ si arresta quando le viene data in input la stringa vuota $\epsilon$.
\[ HALT_\epsilon = \{ \langle M \rangle \mid M \text{ si arresta su } \epsilon \} \]
\end{definition}

\begin{theorem}
$HALT_\epsilon \in R_e$.
\end{theorem}
\begin{proof}
Per dimostrare che $HALT_\epsilon \in R_e$, dobbiamo costruire una Macchina di Turing $M_{HALT_\epsilon}$ che accetti $HALT_\epsilon$.

\textbf{Costruzione di $M_{HALT_\epsilon}$:}
\begin{enumerate}
    \item $M_{HALT_\epsilon}$ prende in input la codifica $\langle M \rangle$.
    \item $M_{HALT_\epsilon}$ simula $M$ sulla stringa vuota $\epsilon$ usando una Macchina Universale (MU).
    \item Se la simulazione di $M$ su $\epsilon$ si arresta (sia accettando che rifiutando), allora $M_{HALT_\epsilon}$ accetta (risponde "sì").
    \item Se la simulazione di $M$ su $\epsilon$ entra in loop infinito, allora $M_{HALT_\epsilon}$ entra in loop (non risponde "sì").
\end{enumerate}
\textbf{Analisi del comportamento di $M_{HALT_\epsilon}$:}
\begin{itemize}
    \item \textbf{Se $\langle M \rangle \in HALT_\epsilon$:}
    Per definizione, $M$ si arresta su $\epsilon$. La simulazione della MU si arresterà. Di conseguenza, $M_{HALT_\epsilon}$ accetterà.
    \item \textbf{Se $\langle M \rangle \notin HALT_\epsilon$:}
    Per definizione, $M$ non si arresta su $\epsilon$ (va in loop). La simulazione della MU andrà in loop. Di conseguenza, $M_{HALT_\epsilon}$ non accetterà.
\end{itemize}
Poiché $M_{HALT_\epsilon}$ accetta esattamente le istanze di $HALT_\epsilon$, si conclude che $HALT_\epsilon \in R_e$.
\end{proof}

\begin{theorem}
$HALT_\epsilon \notin R$.
\end{theorem}
\begin{proof}
La dimostrazione procede per assurdo, utilizzando una riduzione da $HALT$ (di cui sappiamo la non appartenenza a $R$).

\textbf{Assunzione per assurdo:} Supponiamo che $HALT_\epsilon \in R$.
\begin{enumerate}
    \item Se $HALT_\epsilon \in R$, allora esiste una Macchina di Turing $M_{HALT_\epsilon}^*$ che \textbf{decide} $HALT_\epsilon$. Ciò significa che $M_{HALT_\epsilon}^*$ si arresta sempre e dà una risposta corretta (sì/no).
    \item Costruiamo una nuova Macchina di Turing $M'$ che prende in input una coppia $\langle M, w \rangle$. Il comportamento di $M'$ è il seguente:
    \begin{enumerate}
        \item Riceve $\langle M, w \rangle$ come input.
        \item $M'$ costruisce (mediante un "modulo di reshaping") una nuova Macchina di Turing, che chiamiamo $M_{M,w}$ (o $M_w^{\text{tilde}}$), la cui codifica $\langle M_{M,w} \rangle$ viene passata al passo successivo.
        \item La Macchina $M_{M,w}$ è definita come segue:
        \begin{itemize}
            \item Quando $M_{M,w}$ viene avviata su un qualsiasi input (ad esempio, la stringa vuota $\epsilon$), ignora l'input.
            \item Cancella il suo nastro di input.
            \item Scrive la stringa $w$ (ottenuta dalla coppia $\langle M, w \rangle$ iniziale) sul proprio nastro.
            \item Simula la Macchina di Turing $M$ (ottenuta dalla coppia $\langle M, w \rangle$ iniziale) sul contenuto attuale del nastro, ovvero su $w$.
            \item $M_{M,w}$ accetta se $M$ accetta $w$, rifiuta se $M$ rifiuta $w$, va in loop se $M$ va in loop su $w$. In sintesi, $M_{M,w}$ si arresta su $\epsilon$ se e solo se $M$ si arresta su $w$.
        \end{itemize}
        \item $M'$ dà in input la codifica $\langle M_{M,w} \rangle$ alla macchina $M_{HALT_\epsilon}^*$.
        \item $M'$ adotta la risposta di $M_{HALT_\epsilon}^*$:
        \begin{itemize}
            \item Se $M_{HALT_\epsilon}^*$ risponde "sì", allora $M'$ risponde "sì".
            \item Se $M_{HALT_\epsilon}^*$ risponde "no", allora $M'$ risponde "no".
        \end{itemize}
    \end{enumerate}
\end{enumerate}
Ora analizziamo il linguaggio deciso da $M'$, $L(M')$:
\begin{itemize}
    \item \textbf{Se $M'$ risponde "sì":}
    Ciò significa che $M_{HALT_\epsilon}^*$ ha risposto "sì" sull'input $\langle M_{M,w} \rangle$.
    Poiché $M_{HALT_\epsilon}^*$ decide $HALT_\epsilon$, la risposta "sì" implica che $M_{M,w}$ si arresta su $\epsilon$.
    Per come abbiamo costruito $M_{M,w}$, questa si arresta su $\epsilon$ se e solo se $M$ si arresta su $w$.
    Quindi, $M$ si arresta su $w$. Ciò significa che $\langle M, w \rangle \in HALT$.
    \item \textbf{Se $M'$ risponde "no":}
    Ciò significa che $M_{HALT_\epsilon}^*$ ha risposto "no" sull'input $\langle M_{M,w} \rangle$.
    Poiché $M_{HALT_\epsilon}^*$ decide $HALT_\epsilon$, la risposta "no" implica che $M_{M,w}$ non si arresta su $\epsilon$.
    Per come abbiamo costruito $M_{M,w}$, questa non si arresta su $\epsilon$ se e solo se $M$ non si arresta su $w$.
    Quindi, $M$ non si arresta su $w$. Ciò significa che $\langle M, w \rangle \notin HALT$.
\end{itemize}
Il comportamento di $M'$ è esattamente quello di una macchina che decide $HALT$. Dunque, $L(M') = HALT$.

Poiché $M'$ è costruita usando $M_{HALT_\epsilon}^*$ (che è un decider e si arresta sempre), $M'$ è anch'essa una Macchina di Turing che si arresta sempre, ovvero un decider.
Questo implicherebbe che $HALT \in R$.

\textbf{Contraddizione:} Sappiamo che $HALT \notin R$.
L'assunzione iniziale ($HALT_\epsilon \in R$) deve essere falsa.

\textbf{Conclusione:} $HALT_\epsilon \notin R$.
\end{proof}

\begin{proposition}
Il complemento di $HALT_\epsilon$, $\overline{HALT_\epsilon}$, non appartiene a $R$.
\end{proposition}
\begin{proof}
Se $\overline{HALT_\epsilon}$ fosse in $R$, allora per la proprietà che $R$ è chiusa rispetto al complemento, anche $HALT_\epsilon$ sarebbe in $R$. Ma abbiamo appena dimostrato che $HALT_\epsilon \notin R$. Questo è una contraddizione.
\end{proof}

\section{Riepilogo e Conclusioni}
Abbiamo esplorato la decidibilità di diversi linguaggi fondamentali per la teoria della computazione:
\begin{itemize}
    \item $LD$: Non ricorsivamente enumerabile ($LD \notin R_e \implies LD \notin R$).
    \item $\overline{LD}$: Ricorsivamente enumerabile ($\overline{LD} \in R_e$), ma non ricorsivo ($\overline{LD} \notin R$).
    \item $LU$: Ricorsivamente enumerabile ($LU \in R_e$), ma non ricorsivo ($LU \notin R$).
    \item $HALT$: Ricorsivamente enumerabile ($HALT \in R_e$), ma non ricorsivo ($HALT \notin R$).
    \item $HALT_\epsilon$: Ricorsivamente enumerabile ($HALT_\epsilon \in R_e$), ma non ricorsivo ($HALT_\epsilon \notin R$).
\end{itemize}
Tutti i problemi di non appartenenza a $R$ sono stati dimostrati mediante riduzione ad altri problemi di cui era già nota la non decidibilità. Questo è un metodo standard in teoria della computazione per dimostrare l'indecidibilità.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[node distance=1.8cm, >=Stealth]
        % R_e boundary
        \draw[thick, blue] (0,0) circle (3.5cm) node[below right, blue] {$R_e$};
        
        % R boundary
        \draw[thick, green] (0,0) circle (1.8cm) node[above right, green] {$R$};
        
        % Languages inside R_e but outside R (Recursively Enumerable but not Recursive)
        \node[red] at (2.5, 2.5) {$LU$};
        \node[red] at (2.5, -2.5) {$HALT$};
        \node[red] at (-2.5, 2.5) {$\overline{LD}$};
        \node[red] at (-2.5, -2.5) {$HALT_\epsilon$};
        
        % Languages outside R_e (and therefore outside R)
        \node[purple] at (4.5, 0) {$LD$};
        \node[purple] at (-4.5, 0) {$\overline{HALT}$};
        \node[purple] at (0, 4.5) {$\overline{LU}$}; % Also outside R_e, by implication from LU
        \node[purple] at (0, -4.5) {$\overline{HALT_\epsilon}$}; % Also outside R_e, by implication from HALT_epsilon
    \end{tikzpicture}
    \caption{Mappa aggiornata delle classi di linguaggi $R$ e $R_e$ con i linguaggi discussi.}
    \label{fig:r_re_map_updated}
\end{figure}


% =====================================================
% --- START LECTURE 14 ---
% =====================================================

\chapter{Riduzioni e Indecidibilità del PCP}



\section{Introduzione alle Riduzioni}

Una \emph{riduzione} è uno strumento fondamentale in Teoria della Computazione per dimostrare la difficoltà computazionale di problemi.

\begin{definition}[Riduzione Formale]
Siano $A$ e $B$ due linguaggi (problemi di decisione). Una funzione $f: \Sigma^* \to \Sigma^*$ è una \textbf{riduzione calcolabile} di $A$ a $B$ (indicata $A \le_m B$) se:
\begin{enumerate}
    \item $f$ è calcolabile (esiste una Macchina di Turing che calcola $f$ in tempo finito per ogni input).
    \item Per ogni stringa $w \in \Sigma^*$, si ha che $w \in A \iff f(w) \in B$.
\end{enumerate}
Il significato di $w \in A \iff f(w) \in B$ è duplice:
\begin{itemize}
    \item Se $w \in A$, allora $f(w) \in B$.
    \item Se $w \notin A$, allora $f(w) \notin B$.
\end{itemize}
\end{definition}

\subsection{Scopo delle Riduzioni}

Le riduzioni sono utilizzate per dimostrare l'indecidibilità di problemi.
\begin{theorem}
Se $A \le_m B$ e $A$ è un problema indecidibile, allora anche $B$ è un problema indecidibile.
\end{theorem}
\begin{proof}
Supponiamo per contraddizione che $B$ sia decidibile. Allora esisterebbe una Macchina di Turing $M_B$ che decide $B$.
Poiché $f$ è calcolabile, possiamo costruire una Macchina di Turing $M_A$ che decide $A$ come segue:
\begin{enumerate}
    \item Input $w$.
    \item Calcola $f(w)$.
    \item Simula $M_B$ su $f(w)$.
    \item Accetta se $M_B$ accetta, rifiuta se $M_B$ rifiuta.
\end{enumerate}
Per la definizione di riduzione, $w \in A \iff f(w) \in B$. Quindi $M_A$ decide $A$. Ma questo contraddice l'ipotesi che $A$ sia indecidibile. Dunque, $B$ deve essere indecidibile.
\end{proof}

\begin{theorem}[Transitività delle Riduzioni]
Se $A \le_m B$ e $B \le_m C$, allora $A \le_m C$.
\end{theorem}
\begin{proof}
Siano $f$ la riduzione da $A$ a $B$ e $g$ la riduzione da $B$ a $C$.
Entrambe $f$ e $g$ sono calcolabili. Allora la composizione $g \circ f$ è anch'essa calcolabile.
Inoltre, $w \in A \iff f(w) \in B \iff g(f(w)) \in C$.
Quindi $g \circ f$ è una riduzione calcolabile da $A$ a $C$.
\end{proof}

\section{Problema di Corrispondenza di Post (PCP)}

Il Problema di Corrispondenza di Post (PCP) è un problema di decisione sulle stringhe.

\begin{definition}[PCP]
Un'istanza del PCP è data da due liste non-vuote di stringhe, $A = (A_1, A_2, \dots, A_k)$ e $B = (B_1, B_2, \dots, B_k)$, definite su un alfabeto $\Sigma$.
Il problema chiede se esista una sequenza di indici non-vuota $I = (i_1, i_2, \dots, i_m)$ con $m \ge 1$ e $1 \le i_j \le k$ per ogni $j$, tale che:
$$A_{i_1} A_{i_2} \dots A_{i_m} = B_{i_1} B_{i_2} \dots B_{i_m}$$
La risposta è "sì" se una tale sequenza esiste, "no" altrimenti.
\end{definition}

\begin{example}
Consideriamo le liste:
$A = (A_1 = 1, A_2 = 10111, A_3 = 10)$
$B = (B_1 = 111, B_2 = 10, B_3 = 0)$

Una possibile sequenza di indici è $I = (2, 1, 1, 3)$:
$A_2 A_1 A_1 A_3 = (10111)(1)(1)(10) = 101111110$
$B_2 B_1 B_1 B_3 = (10)(111)(111)(0) = 101111110$
Poiché le stringhe risultanti sono uguali, questa istanza ha una soluzione ("sì").
\end{example}

\subsection{Proprietà del PCP}
\begin{itemize}
    \item Il PCP è un problema \textbf{ricorsivamente enumerabile} (in R). Possiamo ideare un algoritmo che, data un'istanza, cerca tutte le possibili sequenze di indici di lunghezza crescente. Se trova una soluzione, accetta. Se non trova una soluzione in un tempo finito, potrebbe continuare a cercare indefinitamente.
    \item Il PCP è \textbf{indecidibile} (non in R). Non esiste un algoritmo che, per ogni istanza del PCP, termina e fornisce la risposta corretta ("sì" o "no"). Dimostreremo questo usando le riduzioni.
\end{itemize}

\section{Problema di Corrispondenza di Post Modificato (MPCP)}

Per dimostrare l'indecidibilità del PCP, useremo un problema intermedio, il MPCP.

\begin{definition}[MPCP]
Un'istanza del MPCP è data da due liste non-vuote di stringhe, $A = (A_1, A_2, \dots, A_k)$ e $B = (B_1, B_2, \dots, B_k)$, definite su un alfabeto $\Sigma$.
Il problema chiede se esista una sequenza di indici $I = (i_1, i_2, \dots, i_m)$ con $m \ge 1$ e $1 \le i_j \le k$ per ogni $j$, tale che:
\begin{enumerate}
    \item La sequenza deve iniziare con l'indice $1$: $i_1 = 1$.
    \item $A_{i_1} A_{i_2} \dots A_{i_m} = B_{i_1} B_{i_2} \dots B_{i_m}$
\end{enumerate}
La risposta è "sì" se una tale sequenza esiste, "no" altrimenti.
\end{definition}

L'unica differenza tra PCP e MPCP è la condizione che la sequenza di indici debba iniziare per $1$.

\section{Riduzione 1: $LU \le_m MPCP$}

Dimostriamo che il Linguaggio Universale ($LU$) si riduce al MPCP. Dato che $LU$ è indecidibile, questo implicherà che MPCP è indecidibile.

\subsection{Idea della Riduzione}
Un'istanza di $LU$ è una coppia $(M, w)$, dove $M$ è una Macchina di Turing e $w$ è una stringa.
Un'istanza di MPCP è una coppia di liste di stringhe $(A, B)$.
La funzione di riduzione $f$ deve trasformare $(M, w)$ in $(A, B)$.
L'idea centrale è simulare la computazione di $M$ su $w$ usando le stringhe delle liste $A$ e $B$. La computazione di una MT può essere rappresentata come una sequenza di \emph{descrizioni istantanee} (ID) separate da un simbolo speciale (e.g., $\#$).
$ID_1 \# ID_2 \# ID_3 \# \dots \# ID_k$
Vogliamo costruire le liste $A$ e $B$ in modo tale che, se $M$ accetta $w$, allora esista una sequenza di indici per MPCP che costruisce la stessa stringa.
Le stringhe di $B$ saranno sempre "un passo avanti" rispetto alle stringhe di $A$, simulando la prossima ID. Quando $M$ accetta, le stringhe di $A$ avranno la possibilità di "recuperare" e allinearsi con quelle di $B$.

\subsection{Costruzione delle Liste $A$ e $B$}
L'alfabeto del MPCP sarà $\Gamma \cup Q \cup \{\#, \$, *\}$, dove $\Gamma$ è l'alfabeto del nastro di $M$, $Q$ è l'insieme degli stati di $M$, e $\#, \$, *$ sono nuovi simboli.
Assumiamo che $M$ non scriva mai il simbolo blank e che il suo nastro sia semi-infinito a destra (non si muova mai a sinistra della posizione iniziale). Queste sono assunzioni standard che non limitano la generalità delle MT.

Le liste $A$ e $B$ sono costruite con le seguenti classi di coppie di stringhe $(A_i, B_i)$:

\begin{enumerate}
    \item \textbf{Coppia Iniziale (obbligatoria per MPCP):}
    Questa coppia inizia la simulazione e garantisce che $B$ sia un passo avanti.
    \begin{itemize}
        \item $A_1 = \#$
        \item $B_1 = \# q_0 w \#$ (dove $q_0$ è lo stato iniziale di $M$, $w$ è la stringa d'input, e $\#$ è un simbolo di confine).
    \end{itemize}

    \item \textbf{Coppie di Copia:}
    Permettono di copiare simboli di configurazione che non sono sotto la testina.
    Per ogni simbolo $x \in \Gamma \cup \{\#\}$:
    \begin{itemize}
        \item $A_i = x$
        \item $B_i = x$
    \end{itemize}

    \item \textbf{Coppie di Transizione:}
    Simulano il movimento della testina e la modifica del nastro secondo le regole di transizione di $M$. Queste regole sono generate per ogni $q \in Q \setminus F$ (stato non finale) e ogni $X \in \Gamma \cup \{\#\}$.
    \begin{itemize}
        \item \textbf{Spostamento a destra ($R$):} Se $\delta(q, X) = (p, Y, R)$:
            \begin{itemize}
                \item $A_i = qX$
                \item $B_i = Yp$
            \end{itemize}
            (Esempio: se $M$ legge $X$ nello stato $q$, scrive $Y$ e va nello stato $p$ muovendosi a destra, allora $qX$ in $A$ corrisponde a $Yp$ in $B$).
            \emph{Caso speciale: $X$ è un blank (simbolo di bordo \texttt{\#}).} Se $\delta(q, \#) = (p, Y, R)$:
            \begin{itemize}
                \item $A_i = q\#$
                \item $B_i = Yp\#$
            \end{itemize}
        \item \textbf{Spostamento a sinistra ($L$):} Se $\delta(q, X) = (p, Y, L)$:
            \begin{itemize}
                \item $A_i = ZqX$ (per ogni $Z \in \Gamma \cup \{\#\}$)
                \item $B_i = pZY$
            \end{itemize}
            (Esempio: se $M$ legge $X$ nello stato $q$, scrive $Y$ e va nello stato $p$ muovendosi a sinistra, allora la sequenza $ZqX$ in $A$ (dove $Z$ è il simbolo a sinistra di $q$) corrisponde a $pZY$ in $B$).
    \end{itemize}

    \item \textbf{Coppie di Accettazione/Recupero:}
    Queste coppie sono utilizzate solo quando $M$ entra in uno stato accettante $q_f \in F$. Permettono alla stringa concatenata di $A$ di "recuperare" la lunghezza della stringa concatenata di $B$.
    Per ogni $q_f \in F$ e ogni $X, Y \in \Gamma$:
    \begin{itemize}
        \item $A_i = Xq_f Y$
        \item $B_i = q_f Y$
    \end{itemize}
    Questo riduce la differenza di lunghezza tra $A_i$ e $B_i$ per $q_f$ e $Y$.
    \begin{itemize}
        \item $A_i = Xq_f \#$
        \item $B_i = q_f \#$
    \end{itemize}
    \begin{itemize}
        \item $A_i = q_f Y \#$
        \item $B_i = q_f \#$
    \end{itemize}
    \begin{itemize}
        \item $A_i = q_f \# \#$
        \item $B_i = q_f \#$
    \end{itemize}
    (Il professore ha menzionato $XQY \to QY$ come regola generale per $Q \in F$. Il principio è che la stringa di A diventa più lunga o uguale, permettendo di chiudere la partita.)

    \item \textbf{Coppia Finale:}
    Permette di completare il match una volta che la MT è in uno stato accettante e la differenza di lunghezza è stata recuperata.
    Per ogni $q_f \in F$:
    \begin{itemize}
        \item $A_i = q_f \# \#$ (due simboli \texttt{\#} per A)
        \item $B_i = \# \#$ (due simboli \texttt{\#} per B)
    \end{itemize}
\end{enumerate}

\subsection{Esempio di Simulazione}
Sia $M$ la Macchina di Turing definita come segue (dal diagramma della lezione):
Stati: $Q = \{q_1, q_2, q_3\}$ (dove $q_1$ è iniziale, $q_3$ è finale).
Alfabeto del nastro: $\Gamma = \{0, 1, \beta\}$ ($\beta$ è il simbolo blank).
Funzione di transizione (interpretata dal diagramma):
$\delta(q_1, 0) = (q_2, 1, R)$
$\delta(q_1, 1) = (q_1, 0, L)$
$\delta(q_2, 1) = (q_1, 0, R)$
$\delta(q_1, \beta) = (q_2, 1, L)$ (loop dal blank, torna indietro e cambia stato)
$\delta(q_2, \beta) = (q_3, 0, R)$ (accetta e si sposta a destra)

Sia la stringa d'input $w = 01$.
La computazione di $M$ su $w=01$ è:
1. $\#q_1 01\#$ (Configurazione iniziale)
2. $\#1q_2 1\#$ (Da $q_1$ legge $0$, scrive $1$, va in $q_2$, si sposta a $R$)
3. $\#10q_1 \#$ (Da $q_2$ legge $1$, scrive $0$, va in $q_1$, si sposta a $R$)
4. $\#1q_2 1\#$ (Da $q_1$ legge $\beta$, scrive $1$, va in $q_2$, si sposta a $L$)
5. $\#10q_3 \#$ (Da $q_2$ legge $\beta$, scrive $0$, va in $q_3$, si sposta a $R$) - Stato accettante $q_3$.

\paragraph{Come la riduzione costruisce la soluzione MPCP:}
\begin{itemize}
    \item Si inizia con la coppia iniziale: $A_1 = \#$, $B_1 = \#q_1 01\#$.
        La stringa concatenata di A (finora $\#$) è "dietro" quella di B (finora $\#q_1 01\#$).
    \item Per simulare $\#q_1 01\# \to \#1q_2 1\#$:
        \begin{itemize}
            \item Usiamo coppie di copia per $\#$ iniziale: $(A_i = \#, B_i = \#)$.
            \item Per la transizione $q_1 0 \to 1q_2$: usiamo la coppia $(A_i = q_1 0, B_i = 1q_2)$.
            \item Usiamo coppie di copia per $1\#$: $(A_i=1, B_i=1)$ e $(A_i=\#, B_i=\#)$.
        \end{itemize}
        A questo punto, la stringa concatenata di A sarà $\#q_1 01\#$, mentre quella di B sarà $\#q_1 01\# \#1q_2 1\#$. B è ancora un passo avanti.
    \item Questo processo continua. Le regole di copia e transizione assicurano che la stringa costruita da $B$ contenga la sequenza di ID, con la stringa costruita da $A$ che la segue con un ID di ritardo.
    \item Quando la computazione raggiunge uno stato finale (es. $q_3$ nell'esempio, $ID_5 = \#10q_3\#$), le coppie di accettazione (Classe 4) permettono alla stringa di $A$ di "catturare" la stringa di $B$. Ad esempio, per $Xq_f Y \to q_f Y$, $A$ contribuisce con 3 simboli e $B$ con 2, riducendo il gap.
    \item Infine, la coppia finale (Classe 5) permette di pareggiare completamente le stringhe. Se $A$ arriva a $\#ID_1\#ID_2\# \dots \#ID_k \#q_f \# \#$ e $B$ arriva a $\#ID_1\#ID_2\# \dots \#ID_k \# \# \#$, allora la coppia finale farà sì che $A$ e $B$ diventino uguali.
\end{itemize}

\subsection{Dimostrazione}
\begin{proof_sketch}
Si deve dimostrare che $(M, w)$ è un'istanza "sì" di $LU$ se e solo se l'istanza $(A, B)$ generata dalla riduzione è un'istanza "sì" di MPCP.

\paragraph{Parte 1: Se $M$ accetta $w$, allora l'istanza MPCP ha soluzione.}
Se $M$ accetta $w$, significa che esiste una sequenza di configurazioni $ID_1, ID_2, \dots, ID_k$ dove $ID_1 = q_0 w$, e $ID_k$ è una configurazione accettante.
Costruiamo la sequenza di indici per MPCP come segue:
Iniziamo sempre con l'indice $1$ (la coppia iniziale). Questo produce $A_{concat} = \#$ e $B_{concat} = \#ID_1\#$.
Poi, si scelgono le coppie di stringhe $(A_i, B_i)$ dalle classi 2 e 3 che simulano la transizione da $ID_j$ a $ID_{j+1}$. Ogni volta che si concatena una coppia di transizione, la stringa di $A$ "completa" la $ID_j$ e la stringa di $B$ "inizia" la $ID_{j+1}$.
Questo fa sì che $A_{concat}$ diventi $\#ID_1\#ID_2\# \dots \#ID_{k-1}\#$ e $B_{concat}$ diventi $\#ID_1\#ID_2\# \dots \#ID_k\#$.
Poiché $ID_k$ è una configurazione accettante, possiamo usare le coppie di Classe 4 per "rimuovere" i simboli vicino allo stato finale in $A$ e $B$ in modo disuguale (o meglio, $A$ apporta più simboli di $B$ per la stessa parte di configurazione), riducendo il divario di lunghezza. Infine, la coppia di Classe 5 permette di far terminare le stringhe con esattamente gli stessi simboli, facendo sì che $A_{concat} = B_{concat}$.
Dunque, esiste una soluzione per l'istanza MPCP.

\paragraph{Parte 2: Se l'istanza MPCP ha soluzione, allora $M$ accetta $w$.}
Supponiamo che l'istanza MPCP $(A, B)$ generata dalla riduzione abbia una soluzione.
Per la definizione di MPCP, questa soluzione deve iniziare con la coppia $(A_1, B_1)$, che è $(\#, \#q_0 w\#)$.
L'unico modo per le stringhe concatenate di $A$ e $B$ di rimanere allineate e infine eguagliarsi è che le coppie usate simulino correttamente le transizioni di $M$.
Qualsiasi sequenza di indici che non rispetti la simulazione delle transizioni (ad esempio, usando una coppia $(X,Y)$ senza che $Y$ sia il risultato della transizione di $X$) non permetterebbe mai alle stringhe di allinearsi correttamente a causa dell'alternanza di simboli di nastro e stato.
Poiché le stringhe di $B$ sono sempre un passo avanti (contengono $\#ID_j\#ID_{j+1}\#$ mentre $A$ ha solo $\#ID_j\#$), l'unico modo per $A$ di "recuperare" e far sì che le stringhe concatenate di $A$ e $B$ diventino uguali è tramite l'uso delle coppie di Classe 4 (Accettazione/Recupero) e Classe 5 (Finale).
Le coppie di Classe 4 e 5 possono essere utilizzate solo se la configurazione attuale di $M$ (simulata) contiene uno stato accettante.
Pertanto, se esiste una soluzione MPCP, la simulazione deve aver raggiunto una configurazione accettante, il che significa che $M$ accetta $w$.
\end{proof_sketch}

Poiché $LU$ è indecidibile e $LU \le_m MPCP$, concludiamo che $MPCP$ è \textbf{indecidibile}.

\section{Riduzione 2: $MPCP \le_m PCP$}

Ora dimostriamo che $MPCP$ si riduce al $PCP$. Dato che $MPCP$ è indecidibile, questo implicherà che $PCP$ è indecidibile.

\subsection{Idea della Riduzione}
Un'istanza di MPCP è una coppia di liste $(A, B)$. Un'istanza di PCP è una coppia di liste $(C, D)$.
La funzione di riduzione $f$ deve trasformare $(A, B)$ in $(C, D)$.
La sfida è che PCP non richiede che la soluzione inizi con un indice specifico, mentre MPCP sì (indice 1). Dobbiamo modificare le liste $(A, B)$ in $(C, D)$ in modo che qualsiasi soluzione PCP debba iniziare con la coppia modificata dall'indice 1 dell'MPCP.

\subsection{Costruzione delle Liste $C$ e $D$}
Introduciamo due nuovi simboli non presenti nell'alfabeto originale: $*$ (asterisco) e $\$$ (dollaro).

Le liste $C$ e $D$ sono costruite come segue:

\begin{enumerate}
    \item \textbf{Trasformazione delle Coppie Originali:}
    Per ogni coppia $(A_i, B_i)$ con $i \in \{1, \dots, k\}$:
    \begin{itemize}
        \item $C_i$: Ogni simbolo di $A_i$ è seguito da un asterisco.
              Esempio: se $A_i = s_1 s_2 \dots s_m$, allora $C_i = s_1 * s_2 * \dots s_m *$.
        \item $D_i$: Ogni simbolo di $B_i$ è preceduto da un asterisco.
              Esempio: se $B_i = t_1 t_2 \dots t_n$, allora $D_i = * t_1 * t_2 \dots * t_n$.
    \end{itemize}

    \item \textbf{Coppia Iniziale Forzata:}
    Per forzare la sequenza a iniziare con l'equivalente dell'indice 1 di MPCP:
    \begin{itemize}
        \item $C_0 = * C_1$ (la stringa $C_1$ (ottenuta dal $A_1$ originale) con un asterisco aggiunto all'inizio).
              Esempio: se $A_1 = 1$, $C_1 = 1*$. Allora $C_0 = *1*$.
        \item $D_0 = D_1$ (la stringa $D_1$ (ottenuta dal $B_1$ originale)).
              Esempio: se $B_1 = 111$, $D_1 = *1*1*1$. Allora $D_0 = *1*1*1$.
    \end{itemize}
    Nota: Tutte le stringhe in $C_i$ (per $i>0$) iniziano con un simbolo dell'alfabeto originale seguito da $^*$, mentre tutte le stringhe in $D_i$ iniziano con $^*$. L'unica eccezione è $C_0$ che inizia con $^*$. Questo forza la scelta di $C_0$ e $D_0$ come prima coppia della soluzione PCP.

    \item \textbf{Coppia Finale Forzata:}
    Per garantire che le stringhe concatenate possano eguagliarsi alla fine:
    \begin{itemize}
        \item $C_{k+1} = \$$ (un singolo simbolo dollaro)
        \item $D_{k+1} = *\$$ (asterisco seguito da dollaro)
    \end{itemize}
    Questo assicura che se le stringhe si allineano in modo corretto con gli asterischi e poi matchano con il dollaro, allora la soluzione è stata trovata.
\end{enumerate}

\subsection{Esempio di Costruzione}
Partiamo dall'istanza PCP dell'Esempio iniziale, ma trattandola come un'istanza MPCP (dove l'indice 1 deve essere il primo):
$A = (A_1 = 1, A_2 = 10111, A_3 = 10)$
$B = (B_1 = 111, B_2 = 10, B_3 = 0)$

Applichiamo la riduzione per ottenere $(C, D)$:
\begin{itemize}
    \item \textbf{Coppie trasformate (da indice 1 a 3):}
    $C_1 = 1*$
    $D_1 = *1*1*1$

    $C_2 = 1*0*1*1*1*$
    $D_2 = *1*0$

    $C_3 = 1*0*$
    $D_3 = *0$

    \item \textbf{Coppia Iniziale Forzata (indice 0):}
    $C_0 = *1*$
    $D_0 = *1*1*1$

    \item \textbf{Coppia Finale Forzata (indice 4):}
    $C_4 = \$$
    $D_4 = *\$$
\end{itemize}
L'istanza PCP risultante è $C=(C_0, C_1, C_2, C_3, C_4)$ e $D=(D_0, D_1, D_2, D_3, D_4)$.

\subsection{Dimostrazione}
\begin{proof_sketch}
Si deve dimostrare che l'istanza $(A, B)$ è "sì" per MPCP se e solo se l'istanza $(C, D)$ generata dalla riduzione è "sì" per PCP.

\paragraph{Parte 1: Se $(A, B)$ ha soluzione MPCP, allora $(C, D)$ ha soluzione PCP.}
Se $(A, B)$ ha una soluzione MPCP, esiste una sequenza di indici $I = (1, i_2, \dots, i_m)$ tale che $A_1 A_{i_2} \dots A_{i_m} = B_1 B_{i_2} \dots B_{i_m}$.
Costruiamo la soluzione PCP come segue: $I' = (0, 1, i_2, \dots, i_m, k+1)$.
La stringa $C_{concat}$ sarà $C_0 C_1 C_{i_2} \dots C_{i_m} C_{k+1}$.
La stringa $D_{concat}$ sarà $D_0 D_1 D_{i_2} \dots D_{i_m} D_{k+1}$.

Sostituendo le definizioni:
$C_{concat} = (* A_1') (A_1' \text{ con } * \text{dopo}) (A_{i_2}' \text{ con } * \text{dopo}) \dots (A_{i_m}' \text{ con } * \text{dopo}) (\$)$
$D_{concat} = (B_1' \text{ con } * \text{prima}) (B_1' \text{ con } * \text{prima}) (B_{i_2}' \text{ con } * \text{prima}) \dots (B_{i_m}' \text{ con } * \text{prima}) (*\$)$

Se $A_1 A_{i_2} \dots A_{i_m} = B_1 B_{i_2} \dots B_{i_m}$, allora è facile vedere che:
$A_1' A_{i_2}' \dots A_{i_m}' \text{ con asterischi finali}$
sarà uguale a
$B_1' B_{i_2}' \dots B_{i_m}' \text{ con asterischi iniziali}$.
L'aggiunta iniziale di $C_0 = *A_1'$ e $D_0 = B_1'$ (dove $B_1'$ ha l'asterisco iniziale) fa sì che il primo asterisco di $C_0$ si allinei con il primo asterisco di $D_0$. Poi, la corrispondenza simbolo-asterisco e asterisco-simbolo si mantiene. L'aggiunta finale di $\$$ e $*\$$ permette di concludere il match.

\paragraph{Parte 2: Se $(C, D)$ ha soluzione PCP, allora $(A, B)$ ha soluzione MPCP.}
Supponiamo che $(C, D)$ abbia una soluzione PCP, ovvero una sequenza di indici $I' = (j_1, j_2, \dots, j_p)$ tale che $C_{j_1} C_{j_2} \dots C_{j_p} = D_{j_1} D_{j_2} \dots D_{j_p}$.
\begin{itemize}
    \item \textbf{Forzatura dell'inizio:} L'unica stringa in $C$ che inizia con un asterisco è $C_0$. Tutte le stringhe in $D$ (tranne $D_{k+1}=*\$$) iniziano con un asterisco. Pertanto, per un match, la prima coppia usata deve essere $(C_0, D_0)$. Questo garantisce che $j_1 = 0$.
    \item \textbf{Allineamento degli asterischi:} Dopo l'inizio con $(C_0, D_0)$, tutte le stringhe di $C_i$ (per $i>0$) terminano con un asterisco, e tutte le stringhe di $D_i$ iniziano con un asterisco. Questo significa che un $C_i$ e un $D_j$ possono iniziare a corrispondere solo se l'ultimo carattere del precedente segmento di $C$ è un simbolo non-asterisco, e il primo carattere del successivo segmento di $D$ è un asterisco. Questo meccanismo di "passo-passo" impone che la sequenza di indici debba essere tale da mantenere una perfetta alternanza di simboli normali e asterischi per tutta la stringa.
    \item \textbf{Forzatura della fine:} Per terminare la stringa con un match, deve essere usata la coppia $(C_{k+1}, D_{k+1}) = (\$, *\$)$. Questo assicura che il match si concluda con un dollaro.
\end{itemize}
Data la struttura della riduzione, se le stringhe concatenate di $C$ e $D$ sono uguali, significa che la sequenza originale di $A$ (senza asterischi e dollari) deve essere uguale alla sequenza originale di $B$ (senza asterischi e dollari).
Poiché la soluzione PCP deve iniziare con $(C_0, D_0)$, questo implica che la soluzione MPCP inizia con la coppia $(A_1, B_1)$, soddisfacendo il requisito di MPCP.
Dunque, esiste una soluzione per l'istanza MPCP.
\end{proof_sketch}

Poiché $MPCP$ è indecidibile e $MPCP \le_m PCP$, concludiamo che $PCP$ è \textbf{indecidibile}.


% =====================================================
% --- START LECTURE 15 ---
% =====================================================

\chapter{Problemi Indecidibili e Classi di Calcolabilit\`a}



\section{Il Problema del Tassellamento (Tiling Problem)}

Il Tiling Problem è un problema indecidibile che, a prima vista, non sembra strettamente legato alle Macchine di Turing. La sua indecidibilità viene dimostrata tramite una riduzione da un problema di arresto.

\subsection{Definizione Intuitiva del Tiling Problem}
Si consideri il primo quadrante cartesiano (una superficie infinita divisa in celle unitarie) che si vuole ricoprire con piastrelle. Ogni piastrella ha la stessa dimensione di una cella.
Le piastrelle non possono essere ruotate o stirate. Esistono un numero finito di \textbf{tipi} di piastrelle, ma un numero infinito di piastrelle per ogni tipo.

Ogni piastrella è idealmente divisa in quattro porzioni, ciascuna con un colore. Le piastrelle possono essere affiancate (orizzontalmente o verticalmente) solo se i lati adiacenti hanno lo stesso colore.
Una piastrella specifica, chiamata $D_0$, deve essere posizionata nella cella di origine $(0,0)$.

Il problema è: dato un sistema di piastrelle (tipi e regole di affiancamento), è possibile ricoprire interamente il primo quadrante cartesiano?

\subsection{Definizione Formale del Tiling Problem}
\begin{definition}[Input del Tiling Problem]
L'input per il problema del Tiling è un sistema di tiling $T = (D, D_0, H, V)$, dove:
\begin{itemize}
    \item $D$: un insieme finito di \textbf{tipi} di piastrella.
    \item $D_0 \in D$: il tipo di piastrella che deve essere ammesso e posizionato nella cella di origine $(0,0)$.
    \item $H \subseteq D \times D$: l'insieme delle \textbf{regole di affiancamento orizzontale}. Una coppia $(d_1, d_2) \in H$ indica che una piastrella di tipo $d_1$ può essere posizionata immediatamente a sinistra di una piastrella di tipo $d_2$.
    \item $V \subseteq D \times D$: l'insieme delle \textbf{regole di affiancamento verticale}. Una coppia $(d_1, d_2) \in V$ indica che una piastrella di tipo $d_1$ può essere posizionata immediatamente sotto una piastrella di tipo $d_2$.
\end{itemize}
\end{definition}

\begin{definition}[Tiling]
Un \emph{tiling} (o tassellamento) per un sistema $T$ è una funzione $f: \mathbb{N} \times \mathbb{N} \to D$ tale che:
\begin{itemize}
    \item $f(0,0) = D_0$ (la piastrella iniziale è nel punto di origine).
    \item Per ogni $m, n \in \mathbb{N}$: $(f(m,n), f(m+1,n)) \in H$ (rispetto delle regole di affiancamento orizzontale).
    \item Per ogni $m, n \in \mathbb{N}$: $(f(m,n), f(m,n+1)) \in V$ (rispetto delle regole di affiancamento verticale).
\end{itemize}
Il problema del Tiling chiede se per un dato sistema $T$ esista una tale funzione $f$.
\end{definition}

\subsection{Indecidibilit\`a del Tiling Problem: Riduzione da $\text{HALT}_\epsilon^C$}
Dimostriamo che il Tiling Problem è indecidibile, riducendo $\text{HALT}_\epsilon^C$ (il problema di determinare se una Macchina di Turing si \textbf{non} arresta sulla stringa vuota $\epsilon$) al Tiling Problem.

\begin{theorem}
$\text{HALT}_\epsilon^C \le_m \text{Tiling}$. Poiché $\text{HALT}_\epsilon^C$ è indecidibile, ne segue che anche $\text{Tiling}$ è indecidibile.
\end{theorem}

\subsubsection{Idea della Riduzione}
L'idea è codificare le computazioni di una Macchina di Turing $M$ sul nastro semi-infinito (senza perdere generalità) utilizzando le piastrelle. Ogni "riga" di piastrelle nel tiling (ovvero, le piastrelle con lo stesso valore di $n$) rappresenterà una configurazione della macchina di Turing ad un determinato passo. I bordi orizzontali tra le righe di piastrelle codificheranno le configurazioni successive della MT.

La MT $M$ non si arresta su $\epsilon$ se e solo se è possibile piastrellare l'intero quadrante.

\subsubsection{Costruzione della Macchina di Turing $N$ (funzione $G$)}
Sia $M = (Q, \Sigma, \Gamma, \delta, q_0, q_{acc}, q_{rej})$ una Macchina di Turing (MT) data in input a $G$. La funzione $G$ costruirà un sistema di tiling $T=(D, D_0, H, V)$.
Useremo "etichette" per i colori delle porzioni di piastrella, che saranno simboli del nastro, stati della MT o combinazioni di essi. Il nastro è semi-infinito a destra.

Le piastrelle avranno tipi $d \in D$ definiti per simulare il comportamento di $M$:

\begin{enumerate}
    \item \textbf{Piastrelle di Inerzia (Simboli del Nastro Lontani dalla Testina):}
    Queste piastrelle rappresentano le porzioni del nastro in cui la testina non è presente e quindi il contenuto del nastro rimane invariato.
    Per ogni simbolo $X \in \Gamma$, definiamo una piastrella del tipo:
    \[
    \begin{tikzpicture}[scale=0.8]
    \draw (0,0) -- (1,0) -- (1,1) -- (0,1) -- cycle;
    \draw (0,0) -- (1,1);
    \draw (1,0) -- (0,1);
    \node at (0.25, 0.75) {}; % Top-left
    \node at (0.75, 0.75) {$X$}; % Top-right
    \node at (0.25, 0.25) {$X$}; % Bottom-left
    \node at (0.75, 0.25) {}; % Bottom-right
    \end{tikzpicture}
    \]
    Queste piastrelle assicurano che un simbolo $X$ sul bordo inferiore venga replicato sul bordo superiore, simulando l'inerzia del nastro. Il "colore" del bordo superiore-destro deve combaciare con quello inferiore-sinistro della piastrella adiacente, e così via.

    \item \textbf{Piastrelle di Transizione (Spostamento della Testina a Destra):}
    Queste piastrelle simulano una transizione $\delta(q, X) = (p, Y, R)$. Una tale transizione significa che la MT, nello stato $q$ leggendo $X$, si sposta nello stato $p$, scrive $Y$ e sposta la testina a destra.
    Queste transizioni coinvolgono due celle del nastro.
    Per ogni $q, p \in Q \setminus \{q_{acc}, q_{rej}\}$ (stati non terminali) e per ogni $X, Y, Z \in \Gamma$:
    \[
    \begin{tikzpicture}[scale=0.8]
    \draw (0,0) -- (1,0) -- (1,1) -- (0,1) -- cycle;
    \draw (0,0) -- (1,1);
    \draw (1,0) -- (0,1);
    \node at (0.25, 0.75) {}; % Top-left
    \node at (0.75, 0.75) {$Y$}; % Top-right
    \node at (0.25, 0.25) {$qX$}; % Bottom-left
    \node at (0.75, 0.25) {}; % Bottom-right
    \end{tikzpicture}
    \quad
    \begin{tikzpicture}[scale=0.8]
    \draw (0,0) -- (1,0) -- (1,1) -- (0,1) -- cycle;
    \draw (0,0) -- (1,1);
    \draw (1,0) -- (0,1);
    \node at (0.25, 0.75) {}; % Top-left
    \node at (0.75, 0.75) {$PZ$}; % Top-right
    \node at (0.25, 0.25) {$Z$}; % Bottom-left
    \node at (0.75, 0.25) {}; % Bottom-right
    \end{tikzpicture}
    \]
    (Le etichette interne alle piastrelle sono per chiarezza espositiva; i "colori" effettivi sono sulle porzioni.)
    Le etichette dei "colori" sui bordi saranno:
    \begin{itemize}
        \item Bordo inferiore della prima piastrella: $qX$ (stato $q$ e simbolo letto $X$).
        \item Bordo superiore della prima piastrella: $Y$ (simbolo scritto $Y$).
        \item Bordo inferiore della seconda piastrella: $Z$ (simbolo successivo $Z$).
        \item Bordo superiore della seconda piastrella: $pZ$ (stato $p$ e testina su $Z$).
        \item Bordo verticale tra le due piastrelle: $p \to R$ (simbola il movimento a destra dello stato $p$).
    \end{itemize}

    \item \textbf{Piastrelle di Transizione (Spostamento della Testina a Sinistra):}
    Simulano una transizione $\delta(q, X) = (p, Y, L)$. La MT, nello stato $q$ leggendo $X$, si sposta nello stato $p$, scrive $Y$ e sposta la testina a sinistra.
    Per ogni $q, p \in Q \setminus \{q_{acc}, q_{rej}\}$ e per ogni $X, Y, Z \in \Gamma$:
    \[
    \begin{tikzpicture}[scale=0.8]
    \draw (0,0) -- (1,0) -- (1,1) -- (0,1) -- cycle;
    \draw (0,0) -- (1,1);
    \draw (1,0) -- (0,1);
    \node at (0.25, 0.75) {}; % Top-left
    \node at (0.75, 0.75) {$PZ$}; % Top-right
    \node at (0.25, 0.25) {$Z$}; % Bottom-left
    \node at (0.75, 0.25) {}; % Bottom-right
    \end{tikzpicture}
    \quad
    \begin{tikzpicture}[scale=0.8]
    \draw (0,0) -- (1,0) -- (1,1) -- (0,1) -- cycle;
    \draw (0,0) -- (1,1);
    \draw (1,0) -- (0,1);
    \node at (0.25, 0.75) {}; % Top-left
    \node at (0.75, 0.75) {$Y$}; % Top-right
    \node at (0.25, 0.25) {$qX$}; % Bottom-left
    \node at (0.75, 0.25) {}; % Bottom-right
    \end{tikzpicture}
    \]
    Le etichette dei "colori" sui bordi saranno:
    \begin{itemize}
        \item Bordo inferiore della prima piastrella: $Z$ (simbolo precedente $Z$).
        \item Bordo superiore della prima piastrella: $pZ$ (stato $p$ e testina su $Z$).
        \item Bordo inferiore della seconda piastrella: $qX$ (stato $q$ e simbolo letto $X$).
        \item Bordo superiore della seconda piastrella: $Y$ (simbolo scritto $Y$).
        \item Bordo verticale tra le due piastrelle: $p \to L$ (simbola il movimento a sinistra dello stato $p$).
    \end{itemize}

    \item \textbf{Piastrella Iniziale ($D_0$):}
    Questa piastrella forza la prima riga a codificare la configurazione iniziale di $M$ su $\epsilon$. $q_0$ è lo stato iniziale e $B$ è il simbolo di blank.
    La piastrella $D_0$ è:
    \[
    \begin{tikzpicture}[scale=0.8]
    \draw (0,0) -- (1,0) -- (1,1) -- (0,1) -- cycle;
    \draw (0,0) -- (1,1);
    \draw (1,0) -- (0,1);
    \node at (0.25, 0.75) {}; % Top-left
    \node at (0.75, 0.75) {$B$}; % Top-right
    \node at (0.25, 0.25) {$q_0B$}; % Bottom-left
    \node at (0.75, 0.25) {}; % Bottom-right
    \end{tikzpicture}
    \]
    I colori sui bordi della piastrella $D_0$ impongono la configurazione iniziale. Le piastrelle adiacenti a destra saranno quelle di tipo 1 che codificano $B$ (blank).
\end{enumerate}

\subsubsection{Dimostrazione del Teorema}
Dobbiamo dimostrare che $M \in \text{HALT}_\epsilon^C \iff T \text{ ammette un tiling}$.

\textbf{Direzione 1: $M \in \text{HALT}_\epsilon^C \implies T \text{ ammette un tiling}$}
Se $M$ non si arresta su $\epsilon$, significa che la sua computazione su $\epsilon$ è infinita, generando una sequenza infinita di configurazioni: $C_0, C_1, C_2, \dots$.
Il sistema di tiling $T$ è stato costruito appositamente per simulare queste configurazioni.
\begin{itemize}
    \item La piastrella $D_0$ posizionata in $(0,0)$ forza il bordo superiore della prima riga a codificare la configurazione iniziale $C_0$.
    \item Le piastrelle di tipo 1 (inerzia) coprono tutte le posizioni del nastro che non contengono la testina.
    \item Le piastrelle di tipo 2 e 3 (transizione) replicano l'effetto della funzione di transizione di $M$. Per ogni passo di computazione di $M$, è possibile trovare una riga di piastrelle che codifica la configurazione successiva $C_{i+1}$ a partire dalla configurazione $C_i$ (codificata dalla riga inferiore).
\end{itemize}
Poiché $M$ non si arresta, genera infinite configurazioni. Grazie alla costruzione delle piastrelle, possiamo posizionare infinite righe di piastrelle, una per ogni configurazione, e ciascuna riga sufficientemente lunga per coprire le celle necessarie della configurazione. Di conseguenza, è possibile piastrellare l'intero quadrante.

\textbf{Direzione 2: $T \text{ ammette un tiling} \implies M \in \text{HALT}_\epsilon^C$}
Supponiamo che $T$ ammetta un tiling $f$.
\begin{itemize}
    \item Il tiling $f$ impone che la prima riga di piastrelle ($n=0$) rappresenti la configurazione iniziale $C_0$ di $M$ su $\epsilon$, grazie a $D_0$.
    \item La costruzione delle piastrelle (tipi 1, 2, 3) è tale che ogni riga di piastrelle $f(m,n)$ può essere validamente posizionata sopra una riga $f(m,n-1)$ solo se $f(m,n)$ codifica una configurazione che è il risultato dell'applicazione di un passo di transizione di $M$ alla configurazione codificata da $f(m,n-1)$.
    \item Crucialmente, le piastrelle di transizione sono state definite solo per stati \textbf{non terminali}. Non esistono piastrelle che possano simulare una transizione verso uno stato di accettazione o rifiuto, né che possano estendere una computazione che ha già raggiunto uno stato finale.
\end{itemize}
Se esiste un tiling che copre l'intero quadrante (infinitamente in alto), significa che $M$ può generare una sequenza infinita di configurazioni senza mai raggiungere uno stato terminale. Pertanto, $M$ non si arresta su $\epsilon$.

\subsection{Posizionamento di Tiling nelle Classi di Calcolabilit\`a}
Il problema del Tiling è indecidibile. In particolare, è un linguaggio in $\text{coRE}$.
\begin{itemize}
    \item \textbf{Perché non è in R?} L'abbiamo appena dimostrato tramite riduzione da $\text{HALT}_\epsilon^C$. Se Tiling fosse in R, allora $\text{HALT}_\epsilon^C$ sarebbe in R, il che è falso.
    \item \textbf{Perché è in coRE?} Per dimostrare che Tiling è in coRE, dobbiamo mostrare che il suo complemento (cioè, il problema di determinare se un sistema di tiling \emph{non} ammette un tiling) è in R. Questo significa che se non è possibile piastrellare, dobbiamo essere in grado di dirlo in tempo finito.
    Per un dato sistema di tiling, si può esplorare un numero finito di configurazioni di piastrellamento fino ad una certa profondità e larghezza. Se non si riesce a completare il piastrellamento entro un certo limite, o se si raggiunge una configurazione che blocca ulteriori estensioni, si può dire "no". Poiché il numero di tipi di piastrelle è finito, si può sistematicamente esplorare tutte le possibilità (ad esempio, con un algoritmo di backtracking). Se l'intero spazio di ricerca (finito) viene esplorato senza trovare un tiling completo, si può rispondere "no" in tempo finito.
\end{itemize}

\section{Classi di Calcolabilit\`a: R e coRE}
Finora, i problemi indecidibili che abbiamo incontrato mostrano una certa "asimmetria".

\subsection{Richiamo: R e RE}
\begin{definition}[Classe R]
Un linguaggio $L$ appartiene alla classe \textbf{R} (ricorsivo o decidibile) se esiste una Macchina di Turing che si arresta su ogni input e decide se l'input appartiene a $L$. In altre parole, possiamo rispondere "SÌ" in tempo finito se l'input è nel linguaggio, e "NO" in tempo finito se non lo è.
\end{definition}

\begin{definition}[Classe RE]
Un linguaggio $L$ appartiene alla classe \textbf{RE} (ricorsivamente enumerabile) se esiste una Macchina di Turing che accetta tutti gli input in $L$ e può non arrestarsi per gli input non in $L$. In altre parole, possiamo rispondere "SÌ" in tempo finito se l'input è nel linguaggio, ma non abbiamo garanzie di risposta (potremmo loopare) se non lo è.
\end{definition}

\subsection{Introduzione: coRE}
\begin{definition}[Classe coRE]
Un linguaggio $L$ appartiene alla classe \textbf{coRE} se il suo complemento $\overline{L}$ appartiene a RE.
Se $L \in \text{coRE}$, significa che esiste una Macchina di Turing che si arresta e rifiuta tutti gli input in $L$ (ovvero, accetta tutti gli input non in $L$). In altre parole, possiamo rispondere "NO" in tempo finito se l'input non è nel linguaggio, ma non abbiamo garanzie di risposta (potremmo loopare) se lo è.
\end{definition}

\begin{remark}
Molti dei problemi indecidibili che abbiamo visto finora (ad esempio, $\overline{L_U}$, $\overline{L_{NE}}$, $\overline{\text{HALT}}$, $\overline{\text{HALT}_\epsilon}$, Tiling) si trovano in coRE ma non in RE.
\begin{itemize}
    \item $L_U \in \text{RE}$, $\overline{L_U} \in \text{coRE}$.
    \item $L_{NE} \in \text{RE}$, $\overline{L_{NE}} \in \text{coRE}$.
    \item $\text{HALT} \in \text{RE}$, $\overline{\text{HALT}} \in \text{coRE}$.
    \item $\text{HALT}_\epsilon \in \text{RE}$, $\overline{\text{HALT}_\epsilon} \in \text{coRE}$.
    \item $\text{Tiling} \in \text{coRE}$ (come discusso sopra).
\end{itemize}
Questi linguaggi sono "sbilanciati": hanno un "lato facile" (o il SÌ o il NO può essere confermato in tempo finito).
\end{remark}

\subsection{Relazioni tra le Classi}

\begin{theorem}
$\text{RE} \cap \text{coRE} = \text{R}$.
\end{theorem}
\begin{proof}
\textbf{Parte 1: $\text{RE} \cap \text{coRE} \subseteq \text{R}$}
Sia $L$ un linguaggio tale che $L \in \text{RE} \cap \text{coRE}$.
Poiché $L \in \text{RE}$, esiste una MT $M_1$ che accetta $L$. Per ogni $w \in L$, $M_1(w)$ si arresta e accetta. Per $w \notin L$, $M_1(w)$ o si arresta e rifiuta o entra in loop.
Poiché $L \in \text{coRE}$, per definizione $\overline{L} \in \text{RE}$. Quindi esiste una MT $M_2$ che accetta $\overline{L}$. Per ogni $w \notin L$ (cioè $w \in \overline{L}$), $M_2(w)$ si arresta e accetta. Per $w \in L$, $M_2(w)$ o si arresta e rifiuta o entra in loop.

Per dimostrare che $L \in \text{R}$, costruiamo una nuova MT $M_{decider}$ che decide $L$:
\begin{minted}[mathescape=true, framesep=2mm, breaklines, fontsize=\small]{text}
M_decider(w):
  Simula M_1(w) e M_2(w) in parallelo.
  Se M_1(w) accetta, allora M_decider accetta.
  Se M_2(w) accetta, allora M_decider rifiuta.
\end{minted}
Poiché $w$ deve essere o in $L$ o in $\overline{L}$, una delle due simulazioni si arresterà e accetterà. $M_{decider}$ si arresterà sempre e darà una risposta (accetta se $w \in L$, rifiuta se $w \notin L$). Dunque $L \in \text{R}$.

\textbf{Parte 2: $\text{R} \subseteq \text{RE} \cap \text{coRE}$}
Sia $L \in \text{R}$.
Poiché $L \in \text{R}$, esiste una MT $M$ che decide $L$. Questa MT si arresta sempre. Dunque $M$ è anche una macchina che accetta $L$, quindi $L \in \text{RE}$.
Inoltre, se $M$ decide $L$, possiamo costruire una macchina $M'$ che decide $\overline{L}$ semplicemente invertendo le risposte di $M$. Poiché $M'$ decide $\overline{L}$, $\overline{L} \in \text{R}$. E dato che $\text{R} \subseteq \text{RE}$, allora $\overline{L} \in \text{RE}$. Per definizione, questo significa che $L \in \text{coRE}$.
Quindi, se $L \in \text{R}$, allora $L \in \text{RE}$ e $L \in \text{coRE}$, da cui $L \in \text{RE} \cap \text{coRE}$.
\end{proof}

\begin{theorem}
$\text{R} \neq \text{RE}$ e $\text{R} \neq \text{coRE}$.
\end{theorem}
\begin{proof}
Sappiamo che $\text{HALT} \in \text{RE}$ ma $\text{HALT} \notin \text{R}$. Questo dimostra che $\text{RE} \neq \text{R}$.
Inoltre, sappiamo che $\overline{\text{HALT}} \in \text{coRE}$ ma $\overline{\text{HALT}} \notin \text{R}$. Questo dimostra che $\text{coRE} \neq \text{R}$.
\end{proof}

\begin{theorem}
$\text{RE} \neq \text{coRE}$.
\end{theorem}
\begin{proof}
Supponiamo per assurdo che $\text{RE} = \text{coRE}$.
Allora, poiché $\text{HALT} \in \text{RE}$ (e per l'assunto $\text{RE} = \text{coRE}$), avremmo $\text{HALT} \in \text{coRE}$.
Se $\text{HALT} \in \text{coRE}$, allora per definizione $\overline{\text{HALT}} \in \text{RE}$.
Ma $\text{HALT} \in \text{RE}$ e $\overline{\text{HALT}} \in \text{RE}$ implica, per il teorema precedente, che $\text{HALT} \in \text{RE} \cap \text{coRE} = \text{R}$.
Questo è una contraddizione, poiché sappiamo che $\text{HALT} \notin \text{R}$.
Dunque, l'assunto $\text{RE} = \text{coRE}$ è falso, e $\text{RE} \neq \text{coRE}$.
\end{proof}

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[node distance=2cm, auto, thick, scale=1.0]
        % Disegna gli insiemi
        \fill[blue!10, opacity=0.7] (0,0) ellipse (3.5cm and 2.5cm);
        \fill[red!10, opacity=0.7] (2,0) ellipse (3.5cm and 2.5cm);
        \node (RE) at (0.5, 2.2) {\textbf{RE}};
        \node (coRE) at (3.5, 2.2) {\textbf{coRE}};

        % Punti per i linguaggi
        \node at (-1.5, 0.5) {$L_U$};
        \node at (-1.5, -0.5) {$L_{NE}$};
        \node at (-1.5, 1.5) {$\text{HALT}$};
        \node at (-1.5, -1.5) {$\text{HALT}_\epsilon$};

        \node at (5.5, 0.5) {$\overline{L_U}$};
        \node at (5.5, -0.5) {$\overline{L_{NE}}$};
        \node at (5.5, 1.5) {$\overline{\text{HALT}}$};
        \node at (5.5, -1.5) {$\overline{\text{HALT}_\epsilon}$};
        \node at (4.5, 0.0) {$\text{Tiling}$};

        % Intersezione R
        \node (R) at (2,0) {\textbf{R}};

        % Problema fuori
        \node (HALT_ALL) at (8,0) {$\text{HALT}_\forall$};
        \node (HALT_ALL_COMPLEMENT) at (-4,0) {$\overline{\text{HALT}_\forall}$};
    \end{tikzpicture}
    \caption{Diagramma delle classi di calcolabilit\`a RE, coRE e R.}
\end{figure}

\section{Problemi Esterni a RE e coRE}
I problemi indecidibili che abbiamo visto finora appartengono a RE o a coRE (ovvero, hanno un "lato facile" per cui si può dire SÌ o NO in tempo finito). La domanda è: esistono problemi che non appartengono né a RE né a coRE? Ovvero, problemi per i quali non siamo in grado di dare garanzie di risposta (SÌ o NO) in tempo finito?

\begin{definition}[$\text{HALT}_\forall$]
Sia $\text{HALT}_\forall = \{ \langle M \rangle \mid M \text{ si arresta su ogni input } w \}$.
\end{definition}

\begin{remark}
Intuitivamente:
\begin{itemize}
    \item \textbf{Perché $\text{HALT}_\forall \notin \text{RE}$?} Per determinare se una macchina si arresta su *ogni* input, dovremmo simulare la macchina su tutti gli input possibili, che sono infiniti. Se la macchina si arresta su tutti, non potremmo mai finire di verificarlo.
    \item \textbf{Perché $\text{HALT}_\forall \notin \text{coRE}$?} Se $\text{HALT}_\forall \in \text{coRE}$, allora $\overline{\text{HALT}_\forall} \in \text{RE}$. Ma $\overline{\text{HALT}_\forall} = \{ \langle M \rangle \mid M \text{ non si arresta su almeno un input } w \}$. Per determinare se $M$ non si arresta su *almeno un* input, dovremmo cercare questo input. Se $M$ non si arresta su un certo $w$, la simulazione di $M$ su $w$ non terminerebbe, e quindi non potremmo dire "SÌ" in tempo finito.
\end{itemize}
Quindi, $\text{HALT}_\forall$ sembra essere un candidato a non stare né in RE né in coRE.

Per dimostrarlo formalmente, useremo il seguente Lemma sulle riduzioni:
\end{remark}

\begin{lemma}
Siano $A$ e $B$ due linguaggi.
\begin{itemize}
    \item Se $A \le_m B$ e $B \in \text{R}$, allora $A \in \text{R}$. (Già dimostrato)
    \item Se $A \le_m B$ e $A \notin \text{RE}$, allora $B \notin \text{RE}$. (Già dimostrato)
    \item Se $A \le_m B$ e $A \notin \text{coRE}$, allora $B \notin \text{coRE}$.
\end{itemize}
\end{lemma}
\begin{proof}[Dimostrazione (ultimo punto)]
Supponiamo per assurdo che $A \le_m B$ e $B \in \text{coRE}$, ma $A \notin \text{coRE}$.
Se $B \in \text{coRE}$, allora $\overline{B} \in \text{RE}$.
Poiché $A \le_m B$, esiste una funzione calcolabile $f$ tale che $w \in A \iff f(w) \in B$.
Questo implica che $w \notin A \iff f(w) \notin B$, ovvero $w \in \overline{A} \iff f(w) \in \overline{B}$.
Quindi $\overline{A} \le_m \overline{B}$.
Dato che $\overline{B} \in \text{RE}$ e $\overline{A} \le_m \overline{B}$, per la seconda parte del Lemma ($A \le_m B$ e $B \in \text{RE} \implies A \in \text{RE}$), ne segue che $\overline{A} \in \text{RE}$.
Se $\overline{A} \in \text{RE}$, per definizione $A \in \text{coRE}$. Questo contraddice la nostra assunzione $A \notin \text{coRE}$.
Quindi l'assunzione è falsa, e se $A \le_m B$ e $A \notin \text{coRE}$, allora $B \notin \text{coRE}$.
\end{proof}

\subsubsection{Dimostrare $\text{HALT}_\forall \notin \text{coRE}$}
Per dimostrare $\text{HALT}_\forall \notin \text{coRE}$, useremo la riduzione $\text{HALT}_\epsilon \le_m \text{HALT}_\forall$.
Sappiamo che $\text{HALT}_\epsilon \in \text{RE}$ ma $\text{HALT}_\epsilon \notin \text{coRE}$ (infatti $\overline{\text{HALT}_\epsilon} \in \text{RE}$).

Sia $f$ la funzione di riduzione che prende una MT $M$ e produce una MT $N = f(M)$.
\begin{minted}[mathescape=true, framesep=2mm, breaklines, fontsize=\small]{python}
# Pseudocodice per la macchina N = f(M)
def N(input_w):
    # N ignora completamente il proprio input_w
    # e si concentra solo sulla simulazione di M su stringa vuota.
    
    # Simula M sulla stringa vuota (epsilon)
    # Si assume che una simulazione di M su epsilon possa essere eseguita
    # e che N possa osservarne il comportamento.
    if M.halts_on_epsilon():
        # Se M si arresta su epsilon, N si arresta (ad esempio, accetta)
        return "Accetta"
    else:
        # Se M non si arresta su epsilon, N non si arresta
        # N entra in un loop infinito
        while True:
            pass # Loop infinito
\end{minted}

\begin{proof}
Dobbiamo mostrare che $M \in \text{HALT}_\epsilon \iff N \in \text{HALT}_\forall$.

\textbf{Direzione 1: $M \in \text{HALT}_\epsilon \implies N \in \text{HALT}_\forall$}
Se $M \in \text{HALT}_\epsilon$, significa che $M$ si arresta sulla stringa vuota $\epsilon$.
Per costruzione, la macchina $N$, indipendentemente dal suo input $w$, simula $M$ su $\epsilon$. Poiché $M$ si arresta su $\epsilon$, la simulazione di $M$ su $\epsilon$ termina in un numero finito di passi, e $N$ si arresta.
Questo è vero per ogni $w$ dato a $N$. Dunque, $N$ si arresta su ogni input $w$. Per definizione, $N \in \text{HALT}_\forall$.

\textbf{Direzione 2: $M \notin \text{HALT}_\epsilon \implies N \notin \text{HALT}_\forall$}
Se $M \notin \text{HALT}_\epsilon$, significa che $M$ non si arresta sulla stringa vuota $\epsilon$.
Per costruzione, la macchina $N$, indipendentemente dal suo input $w$, simula $M$ su $\epsilon$. Poiché $M$ non si arresta su $\epsilon$, la simulazione di $M$ su $\epsilon$ non termina mai, e $N$ entra in un loop infinito.
Questo è vero per ogni $w$ dato a $N$. Dunque, $N$ non si arresta su nessun input $w$. Per definizione, $N \notin \text{HALT}_\forall$.

La riduzione $\text{HALT}_\epsilon \le_m \text{HALT}_\forall$ è valida.
Poiché $\text{HALT}_\epsilon \notin \text{coRE}$, per il Lemma, segue che $\text{HALT}_\forall \notin \text{coRE}$.
\end{proof}

\subsubsection{Dimostrare $\text{HALT}_\forall \notin \text{RE}$}
Per dimostrare $\text{HALT}_\forall \notin \text{RE}$, useremo la riduzione $\overline{\text{HALT}_\epsilon} \le_m \text{HALT}_\forall$.
Sappiamo che $\overline{\text{HALT}_\epsilon} \notin \text{RE}$.

Sia $f$ la funzione di riduzione che prende una MT $M$ e produce una MT $N = f(M)$.
\begin{minted}[mathescape=true, framesep=2mm, breaklines, fontsize=\small]{python}
# Pseudocodice per la macchina N = f(M)
def N(input_w):
    # Usa la lunghezza di input_w come un contatore di passi.
    t = len(input_w)
    
    # Simula M sulla stringa vuota (epsilon) per un massimo di t passi.
    simulazione_m_terminata = M.simulate_on_epsilon(max_steps=t)
    
    if simulazione_m_terminata:
        # Se M si arresta su epsilon entro t passi, N entra in un loop infinito.
        while True:
            pass # Loop infinito
    else:
        # Se M non si arresta su epsilon entro t passi, N si arresta (ad esempio, rifiuta).
        return "Rifiuta"
\end{minted}

\begin{proof}
Dobbiamo mostrare che $M \in \overline{\text{HALT}_\epsilon} \iff N \in \text{HALT}_\forall$.

\textbf{Direzione 1: $M \in \overline{\text{HALT}_\epsilon} \implies N \in \text{HALT}_\forall$}
Se $M \in \overline{\text{HALT}_\epsilon}$, significa che $M$ non si arresta sulla stringa vuota $\epsilon$.
Poiché $M$ non si arresta mai su $\epsilon$, la simulazione di $M$ su $\epsilon$ per un numero finito di passi $t = |w|$ (la lunghezza dell'input di $N$) non raggiungerà mai uno stato di arresto.
Di conseguenza, per qualsiasi input $w$ dato a $N$, la condizione \texttt{simulazione\_m\_terminata} sarà sempre falsa. Quindi $N$ si arresterà sempre (rifiutando).
Dunque, $N$ si arresta su ogni input $w$. Per definizione, $N \in \text{HALT}_\forall$.

\textbf{Direzione 2: $M \notin \overline{\text{HALT}_\epsilon} \implies N \notin \text{HALT}_\forall$}
Se $M \notin \overline{\text{HALT}_\epsilon}$, significa che $M$ si arresta sulla stringa vuota $\epsilon$.
Sia $k$ il numero di passi dopo i quali $M$ si arresta su $\epsilon$.
\begin{itemize}
    \item Se $N$ riceve un input $w$ tale che $|w| < k$:
    $N$ simulerà $M$ su $\epsilon$ per $|w|$ passi. Poiché $|w| < k$, $M$ non si sarà ancora arrestata. La condizione \texttt{simulazione\_m\_terminata} sarà falsa, e $N$ si arresterà (rifiutando).
    \item Se $N$ riceve un input $w$ tale che $|w| \ge k$:
    $N$ simulerà $M$ su $\epsilon$ per $|w|$ passi. Poiché $|w| \ge k$, $M$ si arresterà entro $k$ passi. La condizione \texttt{simulazione\_m\_terminata} sarà vera, e $N$ entrerà in un loop infinito.
\end{itemize}
Poiché $N$ si arresta su alcuni input (quelli con lunghezza $< k$) e non si arresta su altri input (quelli con lunghezza $\ge k$), $N$ non si arresta su *ogni* input.
Dunque, $N \notin \text{HALT}_\forall$.

La riduzione $\overline{\text{HALT}_\epsilon} \le_m \text{HALT}_\forall$ è valida.
Poiché $\overline{\text{HALT}_\epsilon} \notin \text{RE}$, per il Lemma, segue che $\text{HALT}_\forall \notin \text{RE}$.
\end{proof}

\subsection{Conclusione: $\text{HALT}_\forall$ è Esterno a RE e coRE}
Abbiamo dimostrato che $\text{HALT}_\forall \notin \text{RE}$ e $\text{HALT}_\forall \notin \text{coRE}$.
Ciò significa che $\text{HALT}_\forall$ è un problema ancora più "difficile" dei problemi indecidibili che abbiamo visto finora. Non esiste un algoritmo che possa confermare l'appartenenza di una macchina a $\text{HALT}_\forall$ in tempo finito (perché non sta in RE), né un algoritmo che possa confermare la non appartenenza in tempo finito (perché non sta in coRE).
Il suo complemento $\overline{\text{HALT}_\forall}$ si trova anch'esso fuori da RE e coRE.


% =====================================================
% --- START LECTURE 16 ---
% =====================================================

\chapter{Teorema di Rice}



\section{Introduzione al Teorema di Rice}

Abbiamo precedentemente studiato linguaggi come $L_e = \{\langle M \rangle \mid L(M) = \emptyset\}$ (il linguaggio delle macchine di Turing il cui linguaggio è vuoto) e $L_{ne} = \{\langle M \rangle \mid L(M) \neq \emptyset\}$ (il linguaggio delle macchine di Turing il cui linguaggio non è vuoto). Abbiamo dimostrato che questi linguaggi sono indecidibili. Tuttavia, la loro indecidibilità non è un caso isolato, ma rientra in un risultato molto più generale: il Teorema di Rice. Questo teorema si applica a linguaggi che contengono le codifiche di macchine di Turing le cui proprietà soddisfano determinati criteri.

\subsection{Definizione di Proprietà di Macchine di Turing}

Intuitivamente, una macchina di Turing ha una certa proprietà se possiede determinate caratteristiche. Per formalizzare questo concetto:

\begin{definition}[Proprietà di Macchine di Turing]
Una \textbf{proprietà $P$} di macchine di Turing è un insieme di codifiche di macchine di Turing.
Una macchina di Turing $M$ si dice che ha la proprietà $P$ se e solo se la sua codifica $\langle M \rangle$ appartiene a $P$.
\end{definition}

Associato a una proprietà $P$, definiamo il \textbf{linguaggio della proprietà $P$} come:
\[ L_P = \{ \langle M \rangle \mid M \text{ ha la proprietà } P \} = \{ \langle M \rangle \mid \langle M \rangle \in P \} \]
In pratica, $L_P$ è semplicemente la proprietà $P$ stessa quando vista come un linguaggio.

\subsection{Classificazione delle Proprietà}

Le proprietà delle macchine di Turing possono essere categorizzate in due grandi famiglie:
\begin{enumerate}
    \item \textbf{Proprietà Strutturali (o Sintattiche)}: Riguardano la struttura interna della macchina di Turing o il suo comportamento computazionale, ma non direttamente il linguaggio che essa riconosce.
    \item \textbf{Proprietà Semantiche (o di Linguaggi)}: Riguardano esclusivamente il linguaggio riconosciuto dalla macchina di Turing, indipendentemente dalla sua implementazione specifica o dal suo comportamento interno (purché sia funzionalmente equivalente).
\end{enumerate}

Il Teorema di Rice si applica specificamente alle proprietà semantiche.

\begin{definition}[Proprietà Semantica]
Una proprietà $P$ di macchine di Turing è detta \textbf{semantica} se per ogni coppia di macchine di Turing $M_1$ e $M_2$:
\[ L(M_1) = L(M_2) \implies (\langle M_1 \rangle \in P \iff \langle M_2 \rangle \in P) \]
Ciò significa che l'appartenenza di una macchina $M$ a una proprietà semantica $P$ dipende unicamente dal linguaggio $L(M)$ che essa riconosce. Se due macchine riconoscono lo stesso linguaggio, o entrambe possiedono la proprietà $P$ o nessuna delle due la possiede. Per questa ragione, le proprietà semantiche sono anche chiamate \textbf{proprietà di linguaggi}.
\end{definition}

\begin{example}[Proprietà Strutturale]
Sia $P_1$ la proprietà: \textit{la macchina di Turing $M$ ha esattamente 5 stati}.
$P_1 = \{ \langle M \rangle \mid M \text{ ha 5 stati} \}$.
Questa è una proprietà strutturale. Per dimostrare che non è semantica, troviamo un controesempio:
Siano $M_1$ e $M_2$ due macchine di Turing. $M_1$ ha 5 stati e riconosce un linguaggio $L$. $M_2$ è costruita da $M_1$ aggiungendo uno stato irraggiungibile (o una transizione superflua, ecc.), cosicché $M_2$ abbia 6 stati ma riconosca lo stesso linguaggio $L$. In questo caso, $L(M_1) = L(M_2)$, ma $\langle M_1 \rangle \in P_1$ mentre $\langle M_2 \rangle \notin P_1$. Dunque $P_1$ non è semantica.
\end{example}

\begin{example}[Proprietà Semantica]
Sia $P_2$ la proprietà: \textit{il linguaggio riconosciuto da $M$ contiene solo stringhe di lunghezza pari}.
$P_2 = \{ \langle M \rangle \mid \forall s \in L(M), |s| \text{ è pari} \}$.
Questa è una proprietà semantica. Se $L(M_1) = L(M_2)$, allora o tutte le stringhe di $L(M_1)$ (e quindi di $L(M_2)$) hanno lunghezza pari, o nessuna (o alcune) ce l'hanno. L'appartenenza a $P_2$ dipende solo dal linguaggio.
\end{example}

\begin{example}[Proprietà Semantica: $L_e$]
Il linguaggio $L_e = \{\langle M \rangle \mid L(M) = \emptyset\}$ è l'insieme delle macchine di Turing il cui linguaggio è vuoto. Questo è un esempio di proprietà semantica, poiché dipende solo dal linguaggio riconosciuto (in questo caso, il linguaggio vuoto).
\end{example}

\subsection{Proprietà Banali}

\begin{definition}[Proprietà Banale]
Una proprietà $P$ è detta \textbf{banale} se:
\begin{enumerate}
    \item Non contiene alcuna macchina di Turing: $P = \emptyset$.
    \item Contiene tutte le macchine di Turing: $P = \{\langle M \rangle \mid M \text{ è una macchina di Turing}\}$.
\end{enumerate}
Se $P$ è una proprietà di linguaggi (cioè semantica), allora $P$ è banale se e solo se $P = \emptyset$ (nessun linguaggio ha la proprietà) o $P = RE$ (tutti i linguaggi ricorsivamente enumerabili hanno la proprietà).
\end{definition}

Le proprietà banali sono sempre \textbf{decidibili}. Se una proprietà è banale nel senso che $P = \emptyset$, possiamo sempre rispondere "no" per qualsiasi macchina data. Se $P$ contiene tutte le macchine, possiamo sempre rispondere "sì". Il problema di decidere se una macchina possiede una proprietà banale è quindi banale esso stesso.

\section{Teorema di Rice}

Il Teorema di Rice è un risultato fondamentale nella teoria della computabilità, che generalizza l'indecidibilità di problemi come $L_e$ e $L_{ne}$.

\begin{theorem}[Teorema di Rice]
Ogni proprietà non banale dei linguaggi ricorsivamente enumerabili (RE) è indecidibile.
In altre parole, se $P$ è una proprietà semantica (proprietà di linguaggi) tale che $P \neq \emptyset$ e $P \neq RE$, allora il linguaggio $L_P = \{\langle M \rangle \mid L(M) \in P\}$ è indecidibile.
\end{theorem}

\subsection{Dimostrazione del Teorema di Rice}
Sia $P$ una proprietà semantica non banale dei linguaggi RE. Vogliamo dimostrare che $L_P$ è indecidibile. Procediamo con una riduzione dal Problema di Halting Universale, $L_u = \{\langle M,w \rangle \mid M \text{ accetta } w\}$, che sappiamo essere indecidibile.

La dimostrazione si divide in due casi, a seconda che il linguaggio vuoto $\emptyset$ appartenga o meno alla proprietà $P$.

\subsubsection{Caso 1: Il linguaggio vuoto non appartiene a $P$ ($\emptyset \notin P$)}
Poiché $P$ è una proprietà non banale, e $\emptyset \notin P$, deve esistere almeno un linguaggio $L \in P$ tale che $L \neq \emptyset$. (Se tutti i linguaggi in $P$ fossero vuoti, e $\emptyset \notin P$, allora $P$ sarebbe $\emptyset$, contraddicendo l'ipotesi che $P$ sia non banale).
Dato che $L \in RE$, deve esistere una macchina di Turing $M_L$ tale che $L(M_L) = L$.

Costruiamo una macchina di Turing $N$ a partire da una coppia $\langle M,w \rangle$ (input per $L_u$) e da $M_L$. La macchina $N$ è una nuova macchina (la cui codifica è l'output della nostra riduzione) che prende un input $x$. La sua logica di funzionamento è la seguente:

\textbf{Costruzione della macchina $N_{M,w}$ (che chiamiamo $N$ per semplicità):}
\begin{enumerate}
    \item Su input $x$:
    \item Ignora l'input $x$ e simula $M$ sull'input $w$.
    \item Se la simulazione di $M$ su $w$ accetta:
        \begin{enumerate}
            \item Inizia a simulare $M_L$ sull'input $x$.
            \item Se $M_L$ accetta $x$, allora $N$ accetta $x$.
            \item Se $M_L$ rifiuta $x$, allora $N$ rifiuta $x$.
        \end{enumerate}
    \item Se la simulazione di $M$ su $w$ non accetta (ovvero, rifiuta o loopa):
        \begin{enumerate}
            \item $N$ non accetta (rifiuta o loopa).
        \end{enumerate}
\end{enumerate}

Ora analizziamo il comportamento di $N$ per dimostrare che la riduzione funziona:

\textbf{i) Se $\langle M,w \rangle \in L_u$ (cioè, $M$ accetta $w$):}
In questo caso, la simulazione di $M$ su $w$ al passo 2 della costruzione di $N$ terminerà e accetterà. Di conseguenza, $N$ procederà sempre al passo 3 e simulerà $M_L$ su $x$. Questo significa che $N$ accetta $x$ se e solo se $M_L$ accetta $x$.
Quindi, $L(N) = L(M_L) = L$.
Poiché abbiamo stabilito che $L \in P$, ne consegue che $\langle N \rangle \in L_P$.

\textbf{ii) Se $\langle M,w \rangle \notin L_u$ (cioè, $M$ non accetta $w$):}
In questo caso, la simulazione di $M$ su $w$ al passo 2 della costruzione di $N$ non terminerà accettando (o rifiuterà, o loopa). Di conseguenza, $N$ non raggiungerà mai il passo 3 e quindi non accetterà mai alcun input $x$.
Quindi, $L(N) = \emptyset$.
Poiché abbiamo assunto $\emptyset \notin P$, ne consegue che $\langle N \rangle \notin L_P$.

Questa costruzione definisce una funzione calcolabile $f: \langle M,w \rangle \mapsto \langle N \rangle$ tale che $\langle M,w \rangle \in L_u \iff \langle N \rangle \in L_P$. Poiché $L_u$ è indecidibile e $L_u \le_m L_P$, concludiamo che $L_P$ è indecidibile.

\subsubsection{Caso 2: Il linguaggio vuoto appartiene a $P$ ($\emptyset \in P$)}
Se $\emptyset \in P$, consideriamo la proprietà $\overline{P}$, definita come il complemento di $P$ rispetto all'insieme di tutti i linguaggi RE: $\overline{P} = RE \setminus P$.
\begin{itemize}
    \item $\overline{P}$ è una proprietà semantica, poiché se $P$ lo è, anche il suo complemento lo è.
    \item Poiché $\emptyset \in P$, ne consegue che $\emptyset \notin \overline{P}$.
    \item Se $P$ è non banale, allora $\overline{P}$ è anch'essa non banale. (Se $P = \emptyset$, $\overline{P} = RE$. Se $P = RE$, $\overline{P} = \emptyset$. In entrambi i casi, $\overline{P}$ sarebbe banale. Ma abbiamo assunto che $P$ è non banale, quindi anche $\overline{P}$ è non banale).
\end{itemize}
Riassumendo, $\overline{P}$ è una proprietà semantica non banale e $\emptyset \notin \overline{P}$. Questo è esattamente il Caso 1 che abbiamo appena dimostrato. Quindi, il linguaggio $L_{\overline{P}} = \{\langle M \rangle \mid L(M) \in \overline{P}\}$ è indecidibile.

Ora, supponiamo per assurdo che $L_P$ sia decidibile. Allora esisterebbe una macchina di Turing decisore $D_P$ che decide $L_P$.
Utilizzando $D_P$, potremmo costruire una macchina di Turing $D_{\overline{P}}$ che decide $L_{\overline{P}}$ nel modo seguente:
\textbf{Costruzione di $D_{\overline{P}}$ su input $\langle M \rangle$:}
\begin{enumerate}
    \item Esegui $D_P$ su $\langle M \rangle$.
    \item Se $D_P$ accetta $\langle M \rangle$, allora $D_{\overline{P}}$ rifiuta $\langle M \rangle$.
    \item Se $D_P$ rifiuta $\langle M \rangle$, allora $D_{\overline{P}}$ accetta $\langle M \rangle$.
\end{enumerate}
Questa macchina $D_{\overline{P}}$ deciderebbe $L_{\overline{P}}$. Tuttavia, abbiamo appena dimostrato che $L_{\overline{P}}$ è indecidibile. Abbiamo raggiunto una contraddizione.
Pertanto, la nostra supposizione iniziale che $L_P$ fosse decidibile deve essere falsa. Concludiamo quindi che $L_P$ è indecidibile anche nel Caso 2.

Combinando i due casi, il Teorema di Rice è dimostrato. Ogni proprietà non banale dei linguaggi RE è indecidibile.

\section{Applicazioni del Teorema di Rice}
Il Teorema di Rice fornisce un potente strumento per dimostrare l'indecidibilità di un'ampia classe di problemi. Per applicarlo, è sufficiente verificare che la proprietà in questione sia:
\begin{enumerate}
    \item Una proprietà di linguaggi (cioè semantica).
    \item Non banale.
\end{enumerate}
Se entrambe le condizioni sono soddisfatte, allora il problema di decidere se una macchina di Turing possiede tale proprietà è indecidibile.

\begin{example}[Indecidibilità di $L_e$ e $L_{ne}$]
\begin{enumerate}
    \item $L_e = \{\langle M \rangle \mid L(M) = \emptyset\}$:
    \begin{itemize}
        \item \textbf{Proprietà semantica?}: Sì, dipende solo dal linguaggio $L(M)$.
        \item \textbf{Non banale?}: Sì. Contiene il linguaggio vuoto (quindi non è $\emptyset$). Non contiene, ad esempio, $\Sigma^*$ (quindi non è $RE$).
    \end{itemize}
    Dato che è semantica e non banale, per il Teorema di Rice, $L_e$ è indecidibile.
    \item $L_{ne} = \{\langle M \rangle \mid L(M) \neq \emptyset\}$:
    \begin{itemize}
        \item \textbf{Proprietà semantica?}: Sì, dipende solo dal linguaggio $L(M)$.
        \item \textbf{Non banale?}: Sì. Contiene, ad esempio, $\Sigma^*$ (quindi non è $\emptyset$). Non contiene $\emptyset$ (quindi non è $RE$).
    \end{itemize}
    Dato che è semantica e non banale, per il Teorema di Rice, $L_{ne}$ è indecidibile.
\end{enumerate}
\end{example}

\begin{example}[Decidere se $L(M)$ è finito]
Sia $L_{finito} = \{\langle M \rangle \mid L(M) \text{ è finito}\}$.
\begin{itemize}
    \item \textbf{Proprietà semantica?}: Sì, la finitezza di un linguaggio è una sua proprietà intrinseca.
    \item \textbf{Non banale?}: Sì. Contiene linguaggi finiti (e.g., $L(M)=\emptyset$ o $L(M)=\{a\}$), quindi non è $\emptyset$. Non contiene linguaggi infiniti (e.g., $L(M)=\Sigma^*$), quindi non è $RE$.
\end{itemize}
Per il Teorema di Rice, $L_{finito}$ è indecidibile.
\end{example}

\begin{example}[Decidere se $L(M)$ è infinito]
Sia $L_{infinito} = \{\langle M \rangle \mid L(M) \text{ è infinito}\}$.
\begin{itemize}
    \item \textbf{Proprietà semantica?}: Sì.
    \item \textbf{Non banale?}: Sì. Contiene linguaggi infiniti (e.g., $L(M)=\Sigma^*$), quindi non è $\emptyset$. Non contiene linguaggi finiti (e.g., $L(M)=\emptyset$), quindi non è $RE$.
\end{itemize}
Per il Teorema di Rice, $L_{infinito}$ è indecidibile.
\end{example}

\begin{example}[Decidere se $L(M)$ è riconosciuto solo da macchine con 5 stati]
Sia $P_{solo5stati} = \{\langle M \rangle \mid L(M) \text{ è riconosciuto solo da macchine con 5 stati}\}$.
\begin{itemize}
    \item \textbf{Proprietà di linguaggi?}: Sì, è una proprietà semantica.
    \item \textbf{Non banale?}: No, è banale. Ogni linguaggio ricorsivamente enumerabile che può essere riconosciuto da una macchina di Turing con 5 stati, può essere riconosciuto anche da una macchina con più di 5 stati (basta aggiungere stati irraggiungibili). Quindi, nessun linguaggio può essere riconosciuto \emph{solo} da macchine con 5 stati. Pertanto, $P_{solo5stati} = \emptyset$. Essendo $\emptyset$, è una proprietà banale.
\end{itemize}
Poiché è una proprietà banale, $P_{solo5stati}$ è \textbf{decidibile} (la risposta è sempre "no"). Il Teorema di Rice non si applica per dimostrare l'indecidibilità in questo caso.
\end{example}

\subsection{Altri Esempi specifici}

Consideriamo il linguaggio $L = \{w\#A \mid w \in \{0,1\}^+, A=w \lor A=w^R \}$. Questo linguaggio è RE. Di seguito, una descrizione di una macchina di Turing a 2 nastri che riconosce $L$:
\begin{itemize}
    \item Inizialmente, il nastro 1 contiene $w\#A$.
    \item La macchina copia $w$ sul nastro 2.
    \item Quando incontra '\#', si sposta all'inizio di $w$ sul nastro 2.
    \item Non deterministicamente, la macchina può scegliere tra due rami:
        \begin{enumerate}
            \item \textbf{Verifica $A=w$}: Confronta $A$ sul nastro 1 con $w$ sul nastro 2, leggendo entrambi da sinistra a destra. Se corrispondono e si arriva alla fine, accetta.
            \item \textbf{Verifica $A=w^R$}: Confronta $A$ sul nastro 1 da sinistra a destra con $w$ sul nastro 2 da destra a sinistra. Se corrispondono e si arriva alla fine, accetta.
        \end{enumerate}
\end{itemize}

\begin{example}[Decidere se $L(M)=L$]
Sia $P_L = \{\langle M \rangle \mid L(M) = L\}$, dove $L$ è il linguaggio definito sopra.
\begin{itemize}
    \item \textbf{Proprietà di macchine?}: Sì.
    \item \textbf{Proprietà semantica?}: Sì, dipende solo dal fatto che il linguaggio riconosciuto sia esattamente $L$.
    \item \textbf{Non banale?}: Sì. Contiene il linguaggio $L$ (quindi non è $\emptyset$). Non contiene, ad esempio, il linguaggio $\Sigma^*$ (quindi non è $RE$).
\end{itemize}
Per il Teorema di Rice, $P_L$ è \textbf{indecidibile}.
\end{example}

\begin{example}[Decidere se ogni stringa di $L(M)$ è accettata in al più 100 passi]
Sia $P_{100steps} = \{\langle M \rangle \mid \forall s \in L(M), M \text{ accetta } s \text{ in } \le 100 \text{ passi}\}$.
\begin{itemize}
    \item \textbf{Proprietà di macchine?}: Sì.
    \item \textbf{Proprietà semantica?}: No. Dipende dal comportamento computazionale (numero di passi). Controesempio: una macchina $M_1$ accetta "a" in 10 passi. Un'altra macchina $M_2$ potrebbe accettare "a" in 200 passi (es. loopando inutilmente prima di accettare). $L(M_1) = L(M_2) = \{a\}$. Ma $M_1 \in P_{100steps}$ e $M_2 \notin P_{100steps}$.
    \item \textbf{Banale?}: No. Contiene macchine che accettano stringhe corte in pochi passi (e.g., una macchina che accetta solo $\epsilon$ in 5 passi). Non è $RE$ (poiché non tutte le macchine soddisfano la proprietà).
\end{itemize}
Poiché $P_{100steps}$ non è semantica, il Teorema di Rice \textbf{non si applica}. Questo problema è in realtà \textbf{decidibile}. Una macchina per questo problema può simulare $M$ su tutte le stringhe di lunghezza $\le 100$ per 100 passi. Se $M$ accetta una stringa più lunga di 100, o non accetta una stringa in $L(M)$ entro 100 passi, allora rifiuta. Altrimenti accetta.
\end{example}

\begin{example}[Decidere se $M$ non accetta stringhe di $L$ di lunghezza 100]
Sia $P_{no100} = \{\langle M \rangle \mid L(M) \cap \{s \mid |s|=100\} = \emptyset\}$, dove $L$ è il linguaggio definito in precedenza. In altre parole, $M$ non accetta alcuna stringa del linguaggio $L$ che abbia lunghezza 100.
\begin{itemize}
    \item \textbf{Proprietà di macchine?}: Sì.
    \item \textbf{Proprietà semantica?}: Sì, dipende esclusivamente dal linguaggio $L(M)$ e dalla sua intersezione con l'insieme delle stringhe di lunghezza 100.
    \item \textbf{Non banale?}: Sì.
    \begin{itemize}
        \item Non è $\emptyset$: una macchina che accetta solo $0\#0$ (lunghezza 3) appartiene a $P_{no100}$, poiché non accetta alcuna stringa di lunghezza 100.
        \item Non è $RE$: una macchina che accetta una stringa di $L$ di lunghezza 100 (se esiste) non appartiene a $P_{no100}$. Se esistono stringhe di $L$ di lunghezza 100, allora non tutte le macchine RE appartengono a $P_{no100}$.
    \end{itemize}
\end{itemize}
Poiché $P_{no100}$ è semantica e non banale, per il Teorema di Rice, è \textbf{indecidibile}.
\end{example}


% =====================================================
% --- START LECTURE 17 ---
% =====================================================

\chapter{Complessità Strutturale}



\section{Introduzione alla Complessità Strutturale}

Finora, il corso si è occupato principalmente di decidibilità, ovvero di stabilire se un dato problema (linguaggio) è risolvibile o meno (se appartiene a $R$, $RE$, $coRE$, o nessuno dei due).
L'ultima lezione ha introdotto problemi indecidibili, alcuni più "difficili" di altri (es. il complemento di $HALT$ è in $coRE$, ma $HALT$ non è in $R$). Tali problemi rientrano nel campo della teoria della ricorsione e della logica matematica.

Da questo momento in poi, l'attenzione si sposterà all'interno della classe $R$ (problemi ricorsivi o decidibili).
\begin{center}
\begin{tikzpicture}[scale=0.8]
    \draw (0,0) ellipse (3cm and 2cm);
    \node at (2.5,1.5) {$R$};
    \fill[pattern=north east lines, pattern color=blue!30] (0,0) ellipse (1.5cm and 1cm);
    \node at (0,0) {\tiny (focus attuale)};
\end{tikzpicture}
\end{center}
L'obiettivo è categorizzare i problemi decidibili in base alla loro \emph{complessità computazionale}, ovvero quanto tempo e/o spazio è richiesto per risolverli. Questo studio prende il nome di \textbf{Complessità Strutturale}.

Storicamente, lo studio della decidibilità ha preceduto quello della complessità. Negli anni '30, con i lavori di Turing, l'interesse era capire cosa le macchine astratte (come le Macchine di Turing) potessero risolvere. Con l'avvento dei primi computer reali negli anni '40 (ENIAC, EDVAC), divenne evidente che alcuni problemi erano più veloci da risolvere di altri, spingendo la necessità di una teoria formale per quantificare questa differenza.
I lavori seminali sulla complessità computazionale delle Macchine di Turing furono pubblicati nel 1965 da Hartmanis, Stearns e Lewis.

\section{Nozioni di Complessità Temporale}

Definiamo formalmente il tempo di esecuzione per le Macchine di Turing.

\begin{definition}[Computation Time]
Sia $M$ una Macchina di Turing e $w$ una stringa in input per $M$.
\begin{itemize}
    \item Se $M$ è \textbf{deterministica}: il \textbf{computation time} di $M$ su $w$ è il numero di passi che $M$ esegue prima di arrestarsi su $w$.
    \item Se $M$ è \textbf{non-deterministica}: il \textbf{computation time} di $M$ su $w$ è la lunghezza del \emph{branch di computazione più lungo} (il cammino più lungo nell'albero di computazione di $M$ su $w$).
\end{itemize}
\end{definition}

\begin{definition}[Time Function]
Una funzione $t: \mathbb{N} \to \mathbb{N}$ è detta \textbf{time function} se è non-decrescente e strettamente positiva.
\begin{itemize}
    \item \textbf{Non-decrescente}: Se l'input è più grande, il tempo richiesto non diminuisce.
    \item \textbf{Strettamente positiva}: Il tempo richiesto è sempre maggiore di zero.
\end{itemize}
\end{definition}

\begin{definition}[Running Time di una Macchina di Turing]
Sia $t(n)$ una time function. Una Macchina di Turing $M$ ha \textbf{running time} $t(n)$ se, per tutte le stringhe $w$ in input (a parte un numero finito di esse), il computation time di $M$ su $w$ non eccede $t(|w|)$ (dove $|w|$ è la lunghezza di $w$).
\end{definition}
In altre parole, il running time è una stima funzionale del tempo di esecuzione di una macchina, in funzione della dimensione dell'input. Questo concetto è simile alla complessità degli algoritmi che avete studiato.

\subsection{Notazione Asintotica}
Per esprimere i running time in modo indipendente dalle costanti e concentrarsi sul tasso di crescita, usiamo la notazione asintotica.

\begin{definition}[Big O ($O$)]
Date due funzioni $f, g: \mathbb{N} \to \mathbb{N}$, diciamo che $f(n) \in O(g(n))$ (o $f(n)$ è $O(g(n))$) se esistono costanti positive $c$ e $n_0$ tali che per ogni $n \ge n_0$, si ha $f(n) \le c \cdot g(n)$.
\end{definition}
\textbf{Intuizione:} $f(n)$ cresce al più velocemente quanto $g(n)$ (cioè $g(n)$ è un \emph{upper bound} asintotico per $f(n)$).

\begin{center}
\begin{tikzpicture}[scale=0.7]
    \draw[->] (0,0) -- (6,0) node[below] {$n$};
    \draw[->] (0,0) -- (0,5) node[left] {$f(n), c \cdot g(n)$};
    
    % Draw f(n)
    \draw[smooth,samples=100,domain=0:6] plot(\x, {0.5*sin(deg(2*\x))+0.5*\x+1});
    \node at (5.5, 3) [above right] {$f(n)$};
    
    % Draw c*g(n)
    \draw[smooth,samples=100,domain=0:6, red] plot(\x, {0.8*\x+2});
    \node at (5.5, 4) [above right] {$c \cdot g(n)$};
    
    % Mark n0
    \draw[dashed] (3,0) node[below] {$n_0$} -- (3,3.5);
    \draw[dotted,red] (3,3.5) -- (0,3.5);
\end{tikzpicture}
\end{center}

\begin{definition}[Big Omega ($\Omega$)]
Date due funzioni $f, g: \mathbb{N} \to \mathbb{N}$, diciamo che $f(n) \in \Omega(g(n))$ (o $f(n)$ è $\Omega(g(n))$) se esistono costanti positive $c$ e $n_0$ tali che per ogni $n \ge n_0$, si ha $f(n) \ge c \cdot g(n)$.
\end{definition}
\textbf{Intuizione:} $f(n)$ cresce almeno velocemente quanto $g(n)$ (cioè $g(n)$ è un \emph{lower bound} asintotico per $f(n)$).

\begin{center}
\begin{tikzpicture}[scale=0.7]
    \draw[->] (0,0) -- (6,0) node[below] {$n$};
    \draw[->] (0,0) -- (0,5) node[left] {$f(n), c \cdot g(n)$};
    
    % Draw f(n)
    \draw[smooth,samples=100,domain=0:6] plot(\x, {0.8*\x+2});
    \node at (5.5, 4) [above right] {$f(n)$};
    
    % Draw c*g(n)
    \draw[smooth,samples=100,domain=0:6, red] plot(\x, {0.5*sin(deg(2*\x))+0.5*\x+1});
    \node at (5.5, 3) [above right] {$c \cdot g(n)$};
    
    % Mark n0
    \draw[dashed] (3,0) node[below] {$n_0$} -- (3,3.5);
    \draw[dotted,red] (3,3.5) -- (0,3.5);
\end{tikzpicture}
\end{center}

\begin{definition}[Big Theta ($\Theta$)]
Date due funzioni $f, g: \mathbb{N} \to \mathbb{N}$, diciamo che $f(n) \in \Theta(g(n))$ (o $f(n)$ è $\Theta(g(n))$) se $f(n) \in O(g(n))$ e $f(n) \in \Omega(g(n))$.
\end{definition}
\textbf{Intuizione:} $f(n)$ e $g(n)$ hanno lo stesso tasso di crescita asintotico.

\begin{theorem}[Linear Speedup (menzionato)]
Per ogni Macchina di Turing $M$ che opera in tempo $t(n)$, e per ogni costante $c > 0$, esiste un'altra Macchina di Turing $M'$ che opera in tempo $t(n)/c$.
Questo teorema implica che le costanti moltiplicative nel running time non sono significative quando si parla di classi di complessità, poiché si possono sempre "compattare" più simboli sul nastro per accelerare la computazione.
\end{theorem}

\subsection{Complessità Temporale dei Problemi}
I concetti di Big O e Big Omega ci permettono di definire la complessità temporale dei problemi stessi, non solo degli algoritmi.

\begin{definition}[Time Complexity Upper Bound di un Problema]
Il \textbf{time complexity upper bound} di un problema $P$ è $O(f(n))$ se \emph{esiste almeno un algoritmo} che risolve $P$ con un running time $O(f(n))$.
\end{definition}
\begin{example}
Il problema dell'ordinamento di array:
\begin{itemize}
    \item È $O(n^2)$? Sì, (es. Bubble Sort).
    \item È $O(2^n)$? Sì, (es. Bubble Sort è anche $O(2^n)$, è un upper bound molto lasco ma valido).
    \item È $O(n \log n)$? Sì, (es. Merge Sort).
    \item È $O(n)$? No, si può dimostrare che non esiste un algoritmo di ordinamento basato su confronti che sia $O(n)$.
\end{itemize}
\end{example}

\begin{definition}[Time Complexity Lower Bound di un Problema]
Il \textbf{time complexity lower bound} di un problema $P$ è $\Omega(f(n))$ se \emph{tutti gli algoritmi} che risolvono $P$ hanno un running time $\Omega(f(n))$.
\end{definition}
\begin{example}
Il problema dell'ordinamento di array:
\begin{itemize}
    \item È $\Omega(n)$? Sì, tutti gli algoritmi devono almeno leggere tutti gli elementi.
    \item È $\Omega(n \log n)$? Sì, è dimostrato che questo è il lower bound per algoritmi basati su confronti.
    \item È $\Omega(n^2)$? No, perché esistono algoritmi (es. Merge Sort) che sono $O(n \log n)$, quindi non tutti gli algoritmi sono $\Omega(n^2)$.
\end{itemize}
\end{example}

\subsection{Problemi Trattabili vs. Intratttabili}
Basandoci sulla complessità temporale, i problemi possono essere classificati in:
\begin{itemize}
    \item \textbf{Problemi Trattabili (o Facili)}: Sono problemi il cui time complexity upper bound è \emph{polinomiale}.
    \item \textbf{Problemi Intratttabili (o Difficili)}: Sono problemi per i quali \emph{non esiste} (o non si è ancora dimostrato che esista) un algoritmo con running time polinomiale. Questi problemi generalmente richiedono tempo esponenziale.
\end{itemize}
\textbf{Nota:} Un algoritmo $O(n^{1000})$ è teoricamente polinomiale, ma impraticabile per grandi $n$. Tuttavia, algoritmi esponenziali (es. $O(2^n)$) diventano ingestibili molto più rapidamente con l'aumentare di $n$ rispetto a qualsiasi polinomio.

\section{Classi di Complessità Temporale}

Il prossimo passo è definire classi formali di problemi basate sulla loro complessità temporale, all'interno di $R$.

\begin{definition}[Classe DTIME($t(n)$)]
Sia $t(n)$ una time function. La classe di complessità \textbf{DTIME($t(n)$)} è l'insieme di tutti i linguaggi $L$ tali che esiste una Macchina di Turing \emph{deterministica} $M$ che decide $L$ con un running time $O(t(n))$.
\end{definition}
La 'D' in DTIME sta per "Deterministica".

\begin{definition}[Classe P (Polynomial Time)]
La classe \textbf{P} è l'unione di tutte le classi DTIME($n^c$) per ogni costante $c \ge 1$:
\[ P = \bigcup_{c \ge 1} \text{DTIME}(n^c) \]
\end{definition}
\textbf{Nota Importante}: La classe P contiene \emph{esclusivamente problemi di decisione}. Problemi di calcolo (es. sommare due numeri) o di ottimizzazione (es. ordinare un array) non appartengono a P per definizione, sebbene gli algoritmi per risolverli possano avere complessità polinomiale.

\subsection{Esempi di Problemi in P}

\begin{example}[Reachability (Raggiungibilità)]
\begin{itemize}
    \item \textbf{Problema di Decisione}: Dato un grafo diretto $G=(V, E)$ e due nodi $s, t \in V$, esiste un percorso da $s$ a $t$ in $G$?
    \item \textbf{Linguaggio}: $L_{\text{Reach}} = \{ (G, s, t) \mid G \text{ è un grafo diretto, } s, t \in V(G), \text{ e esiste un percorso da } s \text{ a } t \text{ in } G \}$.
    \item \textbf{Algoritmo}: È possibile risolvere Reachability usando algoritmi come la Breadth-First Search (BFS) o la Depth-First Search (DFS).
        \begin{itemize}
            \item \textbf{Funzionamento intuitivo di BFS/DFS}: Si parte dal nodo $s$, si visitano tutti i nodi adiacenti, poi i nodi adiacenti dei nodi visitati, e così via, segnando i nodi già visitati per evitare cicli. Se $t$ viene raggiunto, la risposta è "sì".
            \item \textbf{Complessità}: La BFS/DFS ha una complessità temporale di $O(V+E)$ (dove $V$ è il numero di vertici ed $E$ è il numero di archi), che è polinomiale nella dimensione del grafo (input).
        \end{itemize}
    \item \textbf{Conclusione}: Poiché esiste un algoritmo deterministico con complessità polinomiale, $L_{\text{Reach}} \in P$.
\end{itemize}
\end{example}

\begin{example}[PRIMES]
\begin{itemize}
    \item \textbf{Problema di Decisione}: Dato un intero $n$ (rappresentato in binario), è $n$ un numero primo?
    \item \textbf{Linguaggio}: $L_{\text{Primes}} = \{ \langle n \rangle \mid n \in \mathbb{N} \text{ e } n \text{ è primo} \}$, dove $\langle n \rangle$ è la rappresentazione binaria di $n$.
    \item \textbf{Algoritmo Naïf (Test di Divisione)}: Per verificare se $n$ è primo, si può tentare di dividerlo per tutti gli interi da $2$ fino a $\sqrt{n}$.
        \begin{itemize}
            \item \textbf{Numero di Divisioni}: Circa $\sqrt{n}$.
            \item \textbf{Costo di ogni Divisione}: Per numeri grandi, una divisione non è una operazione a costo costante, ma richiede tempo polinomiale nella lunghezza dei numeri coinvolti.
            \item \textbf{Complessità Totale}: Il numero di divisioni è $\sqrt{n}$. Se l'input è la stringa binaria $\langle n \rangle$, la sua lunghezza è $L = |\langle n \rangle| \approx \log_2 n$. Quindi $n \approx 2^L$. Il numero di divisioni è $\sqrt{n} = \sqrt{2^L} = 2^{L/2}$.
            \item \textbf{Conclusione sul Naïf}: Questo algoritmo è \emph{esponenziale} nella lunghezza dell'input ($L$), non polinomiale. \emph{Attenzione}: un errore comune è confondere il valore dell'input con la sua lunghezza. Contare fino a $n$ o iterare $n$ volte è esponenziale se $n$ è dato in binario.
        \end{itemize}
    \item \textbf{Risultato Moderno}: Nonostante l'algoritmo naïf sia esponenziale, il problema PRIMES è stato dimostrato essere in P nel 2002 (Algoritmo AKS, Agrawal, Kayal, Saxena). La sua complessità è $O(L^k)$ per un $k$ piccolo (es. $O(L^6)$ o $O(L^{12})$), rendendolo un problema polinomiale. Tuttavia, in pratica si preferiscono algoritmi randomizzati più veloci (es. Miller-Rabin).
\end{itemize}
\end{example}

\section{Complessità Temporale Non-Deterministica}
Alcuni problemi, pur sembrando difficili con algoritmi deterministici, beneficiano enormemente del non-determinismo.

\begin{example}[SAT (Boolean Satisfiability)]
\begin{itemize}
    \item \textbf{Formule in Forma Normale Congiuntiva (CNF)}: Una formula CNF è una congiunzione di clausole, dove ogni clausola è una disgiunzione di letterali. Un letterale è una variabile booleana o la sua negazione (es. $(x_1 \lor x_2 \lor \neg x_3) \land (\neg x_2 \lor x_4) \land (\neg x_3 \lor x_5 \lor \neg x_6)$).
    \item \textbf{Problema di Decisione}: Data una formula $\phi$ in CNF, è $\phi$ soddisfacibile (esiste un assegnamento di verità alle variabili che rende $\phi$ vera)?
    \item \textbf{Linguaggio}: $L_{\text{SAT}} = \{ \phi \mid \phi \text{ è una formula CNF e } \phi \text{ è soddisfacibile} \}$.
    \item \textbf{Verifica di un Assegnamento}: Se ci viene dato un assegnamento di verità, verificare se soddisfa la formula richiede tempo polinomiale (lineare nella lunghezza della formula).
    \item \textbf{Algoritmo Naïf Deterministico}: Testare tutti i possibili assegnamenti di verità. Se ci sono $n$ variabili, ci sono $2^n$ assegnamenti. Ognuno richiede tempo polinomiale per la verifica.
    \item \textbf{Complessità Naïf}: $O(2^n \cdot \text{poly}(|\phi|))$. Questo è esponenziale.
\end{itemize}
\end{example}

\begin{example}[Independent Set (Insieme Indipendente)]
\begin{itemize}
    \item \textbf{Definizione}: Dato un grafo non diretto $G=(V, E)$, un \emph{independent set} è un sottoinsieme di vertici $S \subseteq V$ tale che nessun vertice in $S$ è collegato da un arco a un altro vertice in $S$.
    \item \textbf{Problema di Decisione}: Dato un grafo $G$ e un intero $k$, esiste un independent set in $G$ di dimensione almeno $k$?
    \item \textbf{Linguaggio}: $L_{\text{IS}} = \{ (G, k) \mid G \text{ è un grafo e esiste un independent set di taglia } \ge k \text{ in } G \}$.
    \item \textbf{Verifica di un Insieme}: Dato un sottoinsieme di vertici $S$, verificare se è un independent set di taglia $\ge k$ richiede tempo polinomiale (es. $O(V^2)$ o $O(V+E)$).
    \item \textbf{Algoritmo Naïf Deterministico}: Testare tutti i possibili sottoinsiemi di vertici di dimensione $\ge k$. Il numero di tali sottoinsiemi può essere $\binom{V}{k}$, che è esponenziale in $V$.
    \item \textbf{Complessità Naïf}: Esponenziale.
\end{itemize}
\end{example}

\subsection{Il Potere del Non-Determinismo}
I problemi come SAT e Independent Set, pur essendo esponenziali per algoritmi deterministici naïf, diventano molto più "facili" se si introduce il non-determinismo.

\textbf{Algoritmo Non-Deterministico per SAT}:
\begin{enumerate}
    \item \textbf{Fase di Guess (Indovina)}: Una Macchina di Turing non-deterministica "indovina" (o genera non-deterministicamente) un assegnamento di verità per le variabili della formula $\phi$. Questo può essere fatto in tempo \emph{lineare} ($O(n)$ dove $n$ è il numero di variabili). Concettualmente, la macchina crea un branch per ogni possibile scelta di valore (vero/falso) per ogni variabile.
    \item \textbf{Fase di Check (Verifica)}: Successivamente, la macchina verifica in modo deterministico se l'assegnamento indovinato soddisfa effettivamente la formula $\phi$. Come visto, questa fase richiede tempo \emph{polinomiale} nella lunghezza di $\phi$.
\end{enumerate}
Il computation time per questo algoritmo non-deterministico è la somma del tempo di guess e del tempo di check, risultando in un tempo \emph{polinomiale}.

\textbf{Algoritmo Non-Deterministico per Independent Set}:
\begin{enumerate}
    \item \textbf{Fase di Guess (Indovina)}: Una Macchina di Turing non-deterministica "indovina" un sottoinsieme di vertici $S \subseteq V$. Questo può essere fatto in tempo \emph{lineare} ($O(V)$).
    \item \textbf{Fase di Check (Verifica)}: La macchina verifica in modo deterministico se il sottoinsieme $S$ indovinato è un independent set e se la sua dimensione è almeno $k$. Questo richiede tempo \emph{polinomiale} (es. $O(V^2)$).
\end{enumerate}
Anche in questo caso, il computation time è polinomiale.

\begin{definition}[Classe NTIME($t(n)$)]
Sia $t(n)$ una time function. La classe di complessità \textbf{NTIME($t(n)$)} è l'insieme di tutti i linguaggi $L$ tali che esiste una Macchina di Turing \emph{non-deterministica} $M$ che decide $L$ con un running time $O(t(n))$.
\end{definition}
La 'N' in NTIME sta per "Non-Deterministica".

\begin{definition}[Classe NP (Nondeterministic Polynomial Time)]
La classe \textbf{NP} è l'unione di tutte le classi NTIME($n^c$) per ogni costante $c \ge 1$:
\[ NP = \bigcup_{c \ge 1} \text{NTIME}(n^c) \]
\end{definition}
\textbf{Nota Importante}: NP sta per \emph{Nondeterministic Polynomial} e non per "Non Polinomiale". La classe NP contiene tutti i problemi di decisione che possono essere risolti da una macchina di Turing non-deterministica in tempo polinomiale.
Esempi di problemi in NP includono SAT, Independent Set, Knapsack, Hamiltonian Cycle, Traveling Salesperson Problem e molti altri problemi che sono centrali nell'informatica teorica e pratica.

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Classe} & \textbf{Tipo di Macchina} & \textbf{Running Time} \\
\hline
DTIME($t(n)$) & Deterministica & $O(t(n))$ \\
P & Deterministica & $O(n^c)$ per qualche $c \ge 1$ \\
NTIME($t(n)$) & Non-deterministica & $O(t(n))$ \\
NP & Non-deterministica & $O(n^c)$ per qualche $c \ge 1$ \\
\hline
\end{tabular}
\end{center}

La relazione tra P e NP è una delle questioni più importanti e irrisolte dell'informatica: $P \stackrel{?}{=} NP$.


% =====================================================
% --- START LECTURE 18 ---
% =====================================================

\chapter{Classi di Complessità e Riduzioni}



\section{Introduzione alle Classi di Complessità}

Abbiamo iniziato a definire classi di complessità che sono sottoclassi della classe $\mathbf{R}$ (Problemi Decidibili). Ci chiediamo quali problemi siano facili e quali difficili.

Abbiamo introdotto i concetti di \emph{computation time} e \emph{running time} per Macchine di Turing (MT) e le funzioni di tempo. Da ciò, abbiamo definito le classi:
\begin{itemize}
    \item \textbf{Time($f(n)$)}: L'insieme di tutti i linguaggi decidibili da una Macchina di Turing deterministica in tempo $O(f(n))$.
    \item \textbf{NTime($f(n)$)}: L'insieme di tutti i linguaggi decidibili da una Macchina di Turing non deterministica in tempo $O(f(n))$.
\end{itemize}

Queste classi ci permettono di definire il concetto di risolvibilità in tempo polinomiale, esponenziale, ecc., sia in modalità deterministica che non deterministica.

\subsection{Classi P e NP}

\begin{definition}[Classe P]
La classe \textbf{P} è l'insieme di tutti i problemi di decisione decidibili da una Macchina di Turing deterministica in tempo polinomiale.
Formalmente: $\mathbf{P} = \bigcup_{k \ge 1} \text{Time}(n^k)$.
\end{definition}

\begin{definition}[Classe NP]
La classe \textbf{NP} è l'insieme di tutti i problemi di decisione decidibili da una Macchina di Turing non deterministica in tempo polinomiale.
Formalmente: $\mathbf{NP} = \bigcup_{k \ge 1} \text{NTime}(n^k)$.
\end{definition}

\textbf{Nota Importante:} NP sta per \emph{Non-deterministic Polynomial}, non per \emph{Non-Polynomial}.

\subsubsection{Relazione tra P e NP: Il Problema P vs NP}

È intuitivo che $\mathbf{P} \subseteq \mathbf{NP}$. Tutto ciò che può essere deciso in tempo polinomiale deterministico può anche essere deciso in tempo polinomiale non deterministico (una MT deterministica è un caso particolare di MT non deterministica).

La relazione inversa, ovvero se $\mathbf{NP} \subseteq \mathbf{P}$ (il che implicherebbe $\mathbf{P} = \mathbf{NP}$), è un problema tuttora aperto e uno dei più importanti problemi non risolti dell'informatica teorica (e uno dei problemi del millennio).

Attualmente, sappiamo simulare una Macchina di Turing non deterministica con una deterministica con un overhead esponenziale. Questo significa che un linguaggio in $\mathbf{NP}$ può essere deciso da una MT deterministica in tempo esponenziale. Questo non lo colloca automaticamente in $\mathbf{P}$. Non è stato ancora dimostrato che non esista un metodo più efficiente (polinomiale) per tale simulazione. La congettura comune è che $\mathbf{P} \ne \mathbf{NP}$.

\subsection{Riduzione Polinomiale}

Il concetto di riduzione che abbiamo già visto viene specificato per il contesto delle classi di complessità.

\begin{definition}[Riduzione Polinomiale]
Siano $A$ e $B$ due linguaggi. Una \textbf{riduzione polinomiale} da $A$ a $B$ è una funzione $f: \Sigma^* \to \Sigma^*$ tale che:
\begin{enumerate}
    \item $f$ è calcolabile da una Macchina di Turing deterministica in tempo polinomiale.
    \item Per ogni stringa $w \in \Sigma^*$, $w \in A \iff f(w) \in B$.
\end{enumerate}
La riduzione è denotata con $A \le_p B$.
\end{definition}
Il vincolo cruciale è che la trasformazione $f$ (implementata da un trasduttore) deve avere un tempo di esecuzione polinomiale rispetto alla taglia dell'input.

\section{NP-Hardness e NP-Completezza}

Questi concetti sono fondamentali per classificare la difficoltà dei problemi all'interno e al di fuori di NP.

\begin{definition}[NP-Hardness]
Un linguaggio $L$ è \textbf{NP-hard} se per ogni linguaggio $L' \in \mathbf{NP}$, esiste una riduzione polinomiale da $L'$ a $L$ ($L' \le_p L$).
\end{definition}
Intuitivamente, un linguaggio NP-hard è \emph{almeno altrettanto difficile} quanto qualsiasi problema in NP.

\begin{definition}[NP-Completezza]
Un linguaggio $L$ è \textbf{NP-complete} se:
\begin{enumerate}
    \item $L \in \mathbf{NP}$
    \item $L$ è NP-hard.
\end{enumerate}
\end{definition}
In altre parole, un linguaggio NP-complete è un problema che appartiene a NP ed è tra i più difficili problemi in NP.

\subsubsection{Differenza tra NP-Hard e NP-Complete}
La distinzione è cruciale:
\begin{itemize}
    \item Un problema NP-hard potrebbe non appartenere a NP. Ad esempio, il Linguaggio Universale ($A_{TM}$) è NP-hard (è difficile almeno quanto ogni problema in NP), ma non è NP-complete perché non è decidibile (e quindi non appartiene a NP).
    \item Un problema NP-complete è necessariamente in NP.
\end{itemize}

\subsection{Teorema: Relazione tra $L \in \text{NP-Complete}$ e P vs NP}

\begin{theorem}
Sia $L$ un linguaggio NP-complete. Allora, $L \in \mathbf{P}$ se e solo se $\mathbf{P} = \mathbf{NP}$.
\end{theorem}

\begin{proof}
Dobbiamo dimostrare la doppia implicazione.

\textbf{Direzione 1: Se $\mathbf{P} = \mathbf{NP}$, allora $L \in \mathbf{P}$.}
Poiché $L$ è NP-complete, per definizione $L \in \mathbf{NP}$. Se $\mathbf{P} = \mathbf{NP}$, allora è immediato che $L \in \mathbf{P}$.

\textbf{Direzione 2: Se $L \in \mathbf{P}$, allora $\mathbf{P} = \mathbf{NP}$.}
Sappiamo già che $\mathbf{P} \subseteq \mathbf{NP}$. Per dimostrare che $\mathbf{P} = \mathbf{NP}$, ci resta da dimostrare che $\mathbf{NP} \subseteq \mathbf{P}$.
Assumiamo $L \in \mathbf{P}$.
Poiché $L$ è NP-complete, per definizione $L$ è NP-hard.
Questo significa che per ogni linguaggio $L' \in \mathbf{NP}$, esiste una riduzione polinomiale $f$ da $L'$ a $L$ ($L' \le_p L$).
Consideriamo una Macchina di Turing $M_{L'}$ che decide $L'$ utilizzando la riduzione a $L$. $M_{L'}$ è costruita come segue:
\begin{enumerate}
    \item Input: una stringa $w$.
    \item Calcola $y = f(w)$. Sia $M_f$ la MT che calcola $f$.
    \item Simula la Macchina di Turing $M_L$ (che decide $L$) con input $y$.
    \item $M_{L'}$ accetta se $M_L$ accetta, e $M_{L'}$ rifiuta se $M_L$ rifiuta.
\end{enumerate}

Analizziamo il tempo di esecuzione di $M_{L'}$:
\begin{enumerate}
    \item \textbf{Calcolo di $y = f(w)$:} Poiché $f$ è una riduzione polinomiale, $M_f$ opera in tempo $O(|w|^c)$ per una costante $c \ge 1$.
    \item \textbf{Dimensione di $y$:} Poiché $M_f$ opera in tempo $O(|w|^c)$, la dimensione dell'output $y$ non può essere maggiore di $O(|w|^c)$. Ovvero, $|y| \le O(|w|^c)$. (Una macchina non può scrivere più simboli di quanti passi compie).
    \item \textbf{Simulazione di $M_L$ su $y$:} Poiché abbiamo assunto $L \in \mathbf{P}$, $M_L$ opera in tempo polinomiale. Sia $O(|input|^d)$ il suo tempo di esecuzione, per una costante $d \ge 1$.
    Quindi, $M_L$ opera in tempo $O(|y|^d) = O((|w|^c)^d) = O(|w|^{c \cdot d})$.
\end{enumerate}
Il tempo totale di esecuzione di $M_{L'}$ è $O(|w|^c) + O(|w|^{c \cdot d})$. Poiché $c$ e $d$ sono costanti, $c \cdot d$ è anch'essa una costante. Quindi, il tempo totale è polinomiale rispetto a $|w|$.
Questo significa che per ogni linguaggio $L' \in \mathbf{NP}$, possiamo costruire una Macchina di Turing deterministica che lo decide in tempo polinomiale.
Pertanto, $\mathbf{NP} \subseteq \mathbf{P}$.
Combinando con $\mathbf{P} \subseteq \mathbf{NP}$, otteniamo $\mathbf{P} = \mathbf{NP}$.
\end{proof}

Questo teorema implica che per risolvere il problema P vs NP, è sufficiente trovare un algoritmo polinomiale per un singolo problema NP-complete, oppure dimostrare che tale algoritmo non esiste.

\section{Proprietà delle Riduzioni Polinomiali}

\subsection{Transitivit\`a delle Riduzioni Polinomiali}

\begin{theorem}
Siano $A, B, C$ tre linguaggi. Se $A \le_p B$ e $B \le_p C$, allora $A \le_p C$.
\end{theorem}

\begin{proof}
\begin{itemize}
    \item $A \le_p B$: Esiste una riduzione polinomiale $f: \Sigma^* \to \Sigma^*$ con tempo $O(|w|^c)$ per $c \ge 1$.
    \item $B \le_p C$: Esiste una riduzione polinomiale $g: \Sigma^* \to \Sigma^*$ con tempo $O(|w|^d)$ per $d \ge 1$.
\end{itemize}
Vogliamo dimostrare $A \le_p C$.
Consideriamo la funzione composta $h(w) = g(f(w))$.
\begin{enumerate}
    \item \textbf{Correttezza:} $w \in A \iff f(w) \in B \iff g(f(w)) \in C$. Quindi, $w \in A \iff h(w) \in C$. La correttezza logica è mantenuta.
    \item \textbf{Tempo di calcolo:}
    \begin{itemize}
        \item Il calcolo di $f(w)$ richiede $O(|w|^c)$ tempo. La dimensione dell'output $f(w)$ è $O(|w|^c)$.
        \item Il calcolo di $g(f(w))$ viene eseguito su input di dimensione $O(|w|^c)$. Poiché $g$ richiede tempo $O(|input|^d)$, il tempo per calcolare $g(f(w))$ è $O((|w|^c)^d) = O(|w|^{c \cdot d})$.
    \end{itemize}
    Il tempo totale per calcolare $h(w)$ è $O(|w|^c) + O(|w|^{c \cdot d})$, che è polinomiale.
\end{enumerate}
Quindi, $h$ è una riduzione polinomiale da $A$ a $C$, e $A \le_p C$.
\end{proof}

\subsection{Utilizzo della Transitivit\`a per Dimostrare NP-Hardness}

\begin{theorem}
Sia $A$ un linguaggio NP-hard. Se $A \le_p B$, allora $B$ è NP-hard.
\end{theorem}

\begin{proof}
Per dimostrare che $B$ è NP-hard, dobbiamo mostrare che per ogni linguaggio $L' \in \mathbf{NP}$, $L' \le_p B$.
Sappiamo che $A$ è NP-hard (per ipotesi). Questo significa che per ogni $L' \in \mathbf{NP}$, $L' \le_p A$.
Per ipotesi, sappiamo anche che $A \le_p B$.
Combinando queste due riduzioni usando la transitività (Teorema precedente): $L' \le_p A$ e $A \le_p B \implies L' \le_p B$.
Poiché $L'$ era un linguaggio generico in NP, abbiamo dimostrato che ogni linguaggio in NP può essere ridotto polinomialmente a $B$. Quindi $B$ è NP-hard.
\end{proof}

Questo teorema è di fondamentale importanza: una volta dimostrato che un problema è NP-hard (il primo è stato SAT), possiamo dimostrare l'NP-hardness di altri problemi tramite riduzioni "a catena", partendo da un problema NP-hard già noto.

\subsection{Illustrazione Grafica delle Classi di Complessità}

Immaginiamo una rappresentazione visuale delle classi:
\begin{itemize}
    \item $\mathbf{P}$ come un insieme interno (problemi "facili").
    \item $\mathbf{NP}$ come un insieme più grande che contiene $\mathbf{P}$.
    \item I problemi $\mathbf{NP}$-complete sono i problemi "più difficili" all'interno di $\mathbf{NP}$, formando una specie di "bordo" o "frontiera" di $\mathbf{NP}$. Se uno di questi fosse in $\mathbf{P}$, allora $\mathbf{P}$ e $\mathbf{NP}$ collasserebbero.
    \item I problemi $\mathbf{NP}$-hard sono tutti i problemi "almeno difficili quanto" $\mathbf{NP}$. Essi includono i problemi $\mathbf{NP}$-complete e possono anche includere problemi al di fuori di $\mathbf{NP}$ (es. problemi indecidibili), che sono ancora più difficili.
\end{itemize}

\begin{tikzpicture}[scale=0.8, >=Latex]
    % Draw NP boundary
    \draw[thick, fill=blue!10, draw=blue!50] (0,0) ellipse (3.5cm and 2.0cm) node[below=2.0cm] {\textbf{NP}};
    % Draw P boundary
    \draw[thick, fill=green!10, draw=green!50] (0,0) ellipse (1.5cm and 0.8cm) node[below=0.8cm] {\textbf{P}};

    % NP-Complete - on the boundary of NP
    \node at (3.2, 0.5) {\textbf{NP-Complete}};
    \node at (-3.2, -0.5) {}; % Placeholder to balance

    % NP-Hard - outside and on the boundary
    \node at (5, 1.5) {\textbf{NP-Hard}};
    \node at (4, -1) {};
    \node at (0, 2.5) {};

    % Arrows indicating "at least as hard as"
    \draw[->] (3.5, 0) -- (4.5, 0.5); % From NP boundary to outside NP-Hard
    \draw[->] (3.5, 0) -- (2, 2.2); % From NP boundary to outside NP-Hard
    \draw[->] (-3.5, 0) -- (-4.5, -0.5); % From NP boundary to outside NP-Hard
    \draw[->] (-3.5, 0) -- (-2, -2.2); % From NP boundary to outside NP-Hard
\end{tikzpicture}


\section{Problemi NP-Complete: SAT e 3-SAT}

Il problema \textbf{SAT} (Boolean Satisfiability Problem) è l'insieme delle formule booleane in Forma Normale Congiuntiva (CNF) che sono soddisfacibili. SAT è stato il primo problema dimostrato essere NP-complete (Teorema di Cook-Levin, 1971). Ci fidiamo di questo risultato per ora.

\subsection{Definizione di 3-SAT}

\begin{definition}[Formula 3-CNF]
Una formula booleana è in \textbf{3-CNF} se è in CNF e ogni sua clausola contiene \emph{al più} tre letterali.
\end{definition}

\begin{definition}[Problema 3-SAT]
Il problema \textbf{3-SAT} è l'insieme di tutte le formule booleane in 3-CNF che sono soddisfacibili.
\end{definition}

\begin{example}
Una formula in 3-CNF potrebbe essere:
$F = (x_1 \lor x_2 \lor \neg x_3) \land (\neg x_2 \lor x_4) \land (x_3 \lor x_5)$
Ogni clausola ha al più 3 letterali (la seconda ne ha 2, le altre 3).
\end{example}

Nonostante la restrizione sulla struttura delle clausole, 3-SAT è anch'esso un problema NP-complete.

\subsection{Dimostrazione che 3-SAT è NP-Complete}

Per dimostrare che 3-SAT è NP-complete, dobbiamo provare due cose:
\begin{enumerate}
    \item 3-SAT $\in \mathbf{NP}$
    \item 3-SAT è NP-hard
\end{enumerate}

\subsubsection{3-SAT $\in \mathbf{NP}$}
Un problema è in NP se può essere deciso da una macchina di Turing non deterministica in tempo polinomiale.
Per 3-SAT, una MT non deterministica può:
\begin{enumerate}
    \item \textbf{Guess (indovinare):} Indovinare un'assegnazione di verità per tutte le variabili booleane nella formula. Questo può essere fatto in tempo polinomiale (lineare nel numero di variabili, che è al più lineare nella lunghezza della formula).
    \item \textbf{Check (verificare):} Verificare se l'assegnazione indovinata soddisfa la formula. Questo comporta la valutazione di ogni clausola. Per una formula in 3-CNF, ogni clausola ha al più 3 letterali, quindi la valutazione di una clausola è costante. Verificare tutte le clausole richiede tempo polinomiale (lineare nel numero di clausole).
\end{enumerate}
Poiché entrambe le fasi (guess e check) richiedono tempo polinomiale, 3-SAT $\in \mathbf{NP}$.

\subsubsection{3-SAT è NP-hard (mediante riduzione SAT $\le_p$ 3-SAT)}

Per dimostrare che 3-SAT è NP-hard, useremo il teorema precedente e ridurremo SAT a 3-SAT ($SAT \le_p 3SAT$). Poiché SAT è NP-complete (quindi NP-hard), se $SAT \le_p 3SAT$, allora 3-SAT è NP-hard.

\textbf{Obiettivo:} Data una formula booleana $\phi$ in CNF (un'istanza di SAT), costruire una formula $\psi$ in 3-CNF (un'istanza di 3-SAT) tale che $\phi$ è soddisfacibile se e solo se $\psi$ è soddisfacibile. La trasformazione deve essere polinomiale.

\textbf{Descrizione della Trasformazione:}
Sia $\phi = C_1 \land C_2 \land \dots \land C_m$ una formula in CNF, dove ogni $C_i$ è una clausola.
Il processo di trasformazione da $\phi$ a $\psi$ è iterativo. Costruiamo una serie di formule intermedie $\phi^{(0)}, \phi^{(1)}, \dots, \phi^{(k)} = \psi$, dove $\phi^{(0)} = \phi$.
Ad ogni passo, $\phi^{(j+1)}$ è ottenuta da $\phi^{(j)}$ riscrivendo le clausole che hanno più di tre letterali.

Consideriamo una generica clausola $C_i = (L_1 \lor L_2 \lor \dots \lor L_k)$, dove $L_j$ sono letterali.
\begin{enumerate}
    \item \textbf{Caso 1: $k \le 3$ (la clausola ha al più 3 letterali).}
    La clausola $C_i$ viene copiata direttamente in $\phi^{(j+1)}$ senza modifiche.
    \item \textbf{Caso 2: $k > 3$ (la clausola ha più di 3 letterali).}
    Questa clausola $C_i$ viene sostituita da un insieme di clausole più piccole, introducendo nuove variabili ausiliarie.
    Sostituiamo $C_i = (L_1 \lor L_2 \lor L_3 \lor \dots \lor L_k)$ con due nuove clausole:
    $C'_i = (L_1 \lor L_2 \lor H_i)$
    $C''_i = (H_i \lor L_3 \lor \dots \lor L_k)$
    dove $H_i$ è una nuova variabile booleana non presente in $\phi$ o in altre variabili ausiliarie già introdotte.
    \textbf{Attenzione:} La seconda clausola $C''_i$ ha $k-2+1 = k-1$ letterali (se $k-2 \ge 0$). Quindi, $C''_i$ potrebbe ancora avere più di 3 letterali se $k-1 > 3$.
\end{enumerate}

Questo processo viene ripetuto. Partiamo da $\phi = \phi^{(0)}$. Per ogni clausola in $\phi^{(j)}$ che ha più di 3 letterali, applichiamo la trasformazione del Caso 2 per ottenere clausole per $\phi^{(j+1)}$. Continuiamo questo processo finché tutte le clausole nella formula risultante hanno al più 3 letterali. Questa formula finale sarà $\psi$.

\textbf{Analisi del Tempo di Trasformazione:}
Ogni passo della trasformazione riduce la lunghezza delle clausole "lunghe".
Una clausola con $k$ letterali viene sostituita da una clausola con 3 letterali e una clausola con $k-1$ letterali.
Questo processo garantisce che la dimensione delle clausole si riduca progressivamente. Il numero di iterazioni è al più proporzionale alla lunghezza massima delle clausole in $\phi$.
Il numero di clausole nella formula finale $\psi$ sarà al più polinomiale nel numero di clausole originali di $\phi$ (e nella lunghezza di $\phi$). Il numero di nuove variabili introdotte è anch'esso polinomiale.
Pertanto, la trasformazione è calcolabile in tempo polinomiale.

\textbf{Correttezza della Trasformazione: $\phi$ è soddisfacibile $\iff \psi$ è soddisfacibile.}

\textbf{Direzione 1: $\phi$ è soddisfacibile $\implies \psi$ è soddisfacibile.}
Supponiamo che $\phi$ sia soddisfacibile. Esiste quindi un'assegnazione di verità $\sigma$ per le variabili di $\phi$ che rende $\phi$ vera.
Vogliamo mostrare che esiste un'assegnazione di verità $\tau$ per le variabili di $\psi$ (che include le variabili di $\phi$ più le variabili ausiliarie $H_i$) che rende $\psi$ vera.
Definiamo $\tau$ in modo che valuti le variabili di $\phi$ esattamente come $\sigma$. Per le variabili ausiliarie $H_i$, le valuteremo in modo opportuno.

Consideriamo una clausola $C_i = (L_1 \lor L_2 \lor \dots \lor L_k)$ di $\phi$.
Se $k \le 3$, $C_i$ è copiata in $\psi$. Poiché $\sigma$ soddisfa $C_i$, anche $\tau$ soddisferà $C_i$ (dato che $\tau$ replica $\sigma$).
Se $k > 3$, $C_i$ è sostituita da $C'_i = (L_1 \lor L_2 \lor H_i)$ e $C''_i = (H_i \lor L_3 \lor \dots \lor L_k)$.
Poiché $\sigma$ soddisfa $C_i$, almeno un letterale $L_j$ in $C_i$ è vero sotto $\sigma$.
\begin{itemize}
    \item Se $L_1$ o $L_2$ è vero sotto $\sigma$: Assegniamo $H_i = \text{Falso}$. Allora $C'_i$ è vera (per $L_1$ o $L_2$) e $C''_i$ diventa $(\text{Falso} \lor L_3 \lor \dots \lor L_k)$. Poiché $\sigma$ soddisfaceva $C_i$, se $L_1$ o $L_2$ erano Falsi, ci doveva essere un altro $L_j$ (con $j \ge 3$) vero. Se questo è il caso, $C''_i$ sarà vera.
    \item Se $L_1$ e $L_2$ sono Falsi sotto $\sigma$: Allora deve esserci un $L_j$ con $j \ge 3$ che è vero sotto $\sigma$. Assegniamo $H_i = \text{Vero}$. Allora $C'_i$ diventa $(\text{Falso} \lor \text{Falso} \lor \text{Vero})$, che è vero. E $C''_i$ diventa $(\text{Vero} \lor L_3 \lor \dots \lor L_k)$, che è vero (dato che $H_i$ è Vero).
\end{itemize}
In entrambi i casi, possiamo assegnare un valore a $H_i$ in modo che $C'_i \land C''_i$ sia vera. Questo processo si applica a cascata per tutte le trasformazioni intermedie.
Dunque, se $\phi$ è soddisfacibile, $\psi$ è soddisfacibile.

\textbf{Direzione 2: $\psi$ è soddisfacibile $\implies \phi$ è soddisfacibile.}
Supponiamo che $\psi$ sia soddisfacibile. Esiste quindi un'assegnazione di verità $\tau$ per le variabili di $\psi$ (che include le variabili di $\phi$ e le ausiliarie) che rende $\psi$ vera.
Vogliamo mostrare che l'assegnazione $\sigma$ ottenuta da $\tau$ ignorando le variabili ausiliarie (cioè restringendo $\tau$ alle sole variabili di $\phi$) rende $\phi$ vera.

Consideriamo una clausola $C_i = (L_1 \lor L_2 \lor \dots \lor L_k)$ di $\phi$.
Se $k \le 3$, $C_i$ è identica in $\phi$ e $\psi$. Poiché $C_i$ è vera sotto $\tau$, lo è anche sotto $\sigma$ (dato che $\sigma$ è la restrizione di $\tau$).
Se $k > 3$, $C_i$ è sostituita da $C'_i = (L_1 \lor L_2 \lor H_i)$ e $C''_i = (H_i \lor L_3 \lor \dots \lor L_k)$.
Poiché $\tau$ soddisfa $\psi$, sia $C'_i$ che $C''_i$ sono vere sotto $\tau$.
\begin{itemize}
    \item Se $H_i$ è vera sotto $\tau$: Allora $C'_i$ è vera perché $H_i$ è vera. E $C''_i$ è vera perché $H_i$ è vera.
    \item Se $H_i$ è falsa sotto $\tau$: Allora $C'_i$ deve essere vera per $L_1$ o $L_2$. E $C''_i$ deve essere vera per almeno un $L_j$ ($j \ge 3$).
\end{itemize}
In entrambi i casi, per garantire che $C'_i \land C''_i$ sia vera sotto $\tau$, deve essere vero che $(L_1 \lor L_2 \lor \dots \lor L_k)$ è vero sotto $\tau$ (e quindi sotto $\sigma$ per le sole variabili di $\phi$).
Infatti, $L_1 \lor L_2 \lor \dots \lor L_k \equiv (L_1 \lor L_2 \lor H_i) \land (H_i \lor L_3 \lor \dots \lor L_k)$ non è una tautologia. La trasformazione mantiene la soddisfacibilità se e solo se $H_i$ è considerata una variabile.
Il punto chiave è che se $C'_i \land C''_i$ è vero, allora deve essere vero $(L_1 \lor L_2 \lor \dots \lor L_k)$.
Se $\tau(H_i)=Vero$, allora $L_1 \lor L_2 \lor Vero$ è Vero e $Vero \lor L_3 \lor \dots \lor L_k$ è Vero. Non ci dice nulla sugli $L_j$.
Se $\tau(H_i)=Falso$, allora $L_1 \lor L_2 \lor Falso$ è Vero (quindi $L_1 \lor L_2$ è Vero) e $Falso \lor L_3 \lor \dots \lor L_k$ è Vero (quindi $L_3 \lor \dots \lor L_k$ è Vero).
In entrambi i sottocasi, abbiamo che $(L_1 \lor L_2 \lor \dots \lor L_k)$ è vero.
Questo si propaga a ritroso attraverso le iterazioni della trasformazione.
Dunque, se $\psi$ è soddisfacibile, anche $\phi$ è soddisfacibile.

Avendo dimostrato che $SAT \le_p 3SAT$ e sapendo che SAT è NP-hard, concludiamo che 3-SAT è NP-hard.
Poiché 3-SAT $\in \mathbf{NP}$ e 3-SAT è NP-hard, allora 3-SAT è NP-complete.

\subsection{Problema 2-SAT}
Si può dimostrare che il problema \textbf{2-SAT} (formule in CNF dove ogni clausola ha esattamente due letterali) appartiene alla classe \textbf{P}. Nonostante la somiglianza con 3-SAT, 2-SAT è un problema molto più semplice, risolvibile in tempo polinomiale. Questo sottolinea come anche piccole restrizioni sulla struttura del problema possano avere un impatto enorme sulla sua complessità.

\subsection{Esercizio: EXACT 3-SAT}
\begin{definition}[Problema EXACT 3-SAT]
Il problema \textbf{EXACT 3-SAT} è l'insieme di tutte le formule booleane in CNF in cui ogni clausola contiene \emph{esattamente} tre letterali, e la formula è soddisfacibile.
\end{definition}

\textbf{Esercizio per casa:} Dimostrare che EXACT 3-SAT è NP-complete.
\textbf{Suggerimento:} Ridurre 3-SAT a EXACT 3-SAT.


% =====================================================
% --- START LECTURE 19 ---
% =====================================================

\chapter{Problemi NP-Completi}



\section{Introduzione ai Problemi NP-Completi}

Dopo aver definito la nozione di NP-Completezza, in questa lezione esamineremo diversi problemi noti per essere NP-Completi. L'obiettivo è comprendere cosa significhi per un problema essere NP-Hard e NP-Completo, e come dimostrarlo attraverso riduzioni polinomiali.

\subsection{Richiami su Exact 3-SAT}

Il problema \emph{Exact 3-SAT} (o \emph{3-SAT Esatto}) è una variante di 3-SAT in cui ogni clausola della formula booleana deve contenere \emph{esattamente} tre letterali. Ieri era stata lasciata la domanda di dimostrarne l'NP-Hardness.

\textbf{Dimostrazione dell'NP-Hardness di Exact 3-SAT:}
Si parte da una formula 3-SAT generica, dove le clausole hanno "al più" tre letterali. Dobbiamo trasformarla in una formula equivalente in cui ogni clausola ha "esattamente" tre letterali.
La trasformazione procede clausola per clausola:
\begin{itemize}
    \item Se una clausola ha esattamente tre letterali, la si copia così com'è.
    \item Se una clausola ha meno di tre letterali (es. uno o due), si prendono a caso uno o più letterali già presenti in quella clausola e li si replicano fino a raggiungere esattamente tre letterali.
\end{itemize}
Questa trasformazione è polinomiale e mantiene l'equivalenza semantica della formula. Di conseguenza, se 3-SAT è NP-Hard, anche Exact 3-SAT lo è. Per questa ragione, d'ora in avanti, quando si parlerà di 3-SAT ci si potrà riferire indistintamente alla variante con "al più" o "esattamente" tre letterali, a seconda della convenienza per la riduzione.

\section{Independent Set (IS)}

L'Independent Set è il primo problema NP-Completo che esaminiamo in dettaglio.

\begin{definition}[Independent Set]
Dato un grafo non orientato $G=(V, E)$, un \emph{Independent Set} (IS) $S \subseteq V$ è un sottoinsieme dei suoi nodi tale per cui non esiste alcun arco tra nessuna coppia di nodi in $S$. Formalmente:
\[ \forall u, v \in S, \quad (u, v) \notin E \]
Ogni grafo ammette un Independent Set (es. il set vuoto o un singolo nodo). I problemi interessanti riguardano la ricerca di Independent Set "grandi".
\end{definition}

\begin{definition}[Independent Set (Problema di Decisione)]
Il problema di decisione \emph{Independent Set} è definito come l'insieme delle coppie $\langle G, K \rangle$ tali che $G$ è un grafo non orientato, $K$ è un numero intero, ed esiste un Independent Set in $G$ di taglia (cardinalità) almeno $K$.
\end{definition}

\subsection{Membership in NP}

\begin{proposition}
Il problema \emph{Independent Set} appartiene alla classe NP.
\end{proposition}

\begin{proof*}
Per dimostrare che IS $\in$ NP, dobbiamo mostrare che esiste una Macchina di Turing Non-Deterministica (MTND) che decide un'istanza in tempo polinomiale.
Una MTND può risolvere IS come segue:
\begin{enumerate}
    \item \textbf{Guess (Non-Deterministic Choice):} La MTND "indovina" (o sceglie in modo non-deterministico) un sottoinsieme $S'$ di nodi di $V$. Questo può essere fatto in tempo polinomiale (ad esempio, per ogni nodo, decide se includerlo o meno in $S'$).
    \item \textbf{Check (Deterministic Verification):} La MTND verifica deterministicamente due condizioni:
    \begin{itemize}
        \item La cardinalità di $S'$ è almeno $K$: $|S'| \ge K$.
        \item Non esistono archi tra coppie di nodi in $S'$: Per ogni coppia di nodi $u, v \in S'$ ($u \ne v$), verifica che $(u, v) \notin E$.
    \end{itemize}
    Se entrambe le condizioni sono soddisfatte, la MTND accetta l'istanza. Altrimenti, la rifiuta.
\end{enumerate}
Entrambi i passaggi (guess e check) possono essere eseguiti in tempo polinomiale rispetto alla dimensione dell'input (numero di nodi e archi del grafo). Quindi, Independent Set $\in$ NP.
\end{proof*}

\subsection{Dimostrazione NP-Hardness: $3SAT \le_p IS$}

Per dimostrare che Independent Set è NP-Hard, effettuiamo una riduzione polinomiale da 3-SAT a Independent Set.

\begin{theorem}
$3SAT \le_p IS$. Di conseguenza, \emph{Independent Set} è NP-Hard.
\end{theorem}

\begin{proof*}
Sia $\phi = C_1 \land C_2 \land \dots \land C_m$ un'istanza di 3-SAT, dove ogni $C_i$ è una clausola con esattamente tre letterali (possiamo usare la variante Exact 3-SAT). Vogliamo costruire una coppia $\langle G, K \rangle$ tale che $\phi$ è soddisfacibile se e solo se $G$ ha un Independent Set di taglia almeno $K$.

\textbf{Costruzione della Trasformazione ($f$):}
\begin{enumerate}
    \item \textbf{Nodi ($V'$):} Per ogni letterale in ogni clausola, creiamo un nodo nel grafo $G$. Se $C_i = (l_{i1} \lor l_{i2} \lor l_{i3})$, creiamo tre nodi distinti $v_{i1}, v_{i2}, v_{i3}$ per questa clausola.
    Quindi, se $\phi$ ha $m$ clausole, $G$ avrà $3m$ nodi.
    \item \textbf{Archi ($E'$):} Gli archi sono di due tipi:
    \begin{itemize}
        \item \textbf{Archi di Clausola:} Per ogni clausola $C_i$, aggiungiamo un arco tra ogni coppia di nodi corrispondenti ai letterali della stessa clausola. Ad esempio, per $C_i = (l_{i1} \lor l_{i2} \lor l_{i3})$, aggiungiamo gli archi $(v_{i1}, v_{i2})$, $(v_{i1}, v_{i3})$, $(v_{i2}, v_{i3})$. Questo forma un triangolo (una cricca di taglia 3) per ogni clausola.
        \item \textbf{Archi di Contraddizione:} Aggiungiamo un arco tra due nodi $v_{ij}$ e $v_{kl}$ se i loro letterali corrispondenti $l_{ij}$ e $l_{kl}$ sono opposti (ad esempio, $x_1$ e $\neg x_1$).
    \end{itemize}
    \item \textbf{Valore $K$:} Il valore $K$ per l'Independent Set è il numero di clausole in $\phi$, ovvero $K=m$.
\end{enumerate}
La costruzione è chiaramente polinomiale in $m$ e nel numero di variabili.

\textbf{Esempio di Trasformazione:}
Sia $\phi = (x_1 \lor x_2 \lor \neg x_3) \land (\neg x_2 \lor x_3 \lor x_4) \land (\neg x_3 \lor \neg x_4 \lor x_5)$.
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[node distance=2cm, thick]
        % Clause 1
        \node[circle, draw] (c1_1) at (0,3) {$x_1$};
        \node[circle, draw] (c1_2) at (1.5,3) {$x_2$};
        \node[circle, draw] (c1_3) at (0.75,1.5) {$\neg x_3$};
        \draw (c1_1) -- (c1_2) -- (c1_3) -- (c1_1);

        % Clause 2
        \node[circle, draw] (c2_1) at (4,3) {$\neg x_2$};
        \node[circle, draw] (c2_2) at (5.5,3) {$x_3$};
        \node[circle, draw] (c2_3) at (4.75,1.5) {$x_4$};
        \draw (c2_1) -- (c2_2) -- (c2_3) -- (c2_1);

        % Clause 3
        \node[circle, draw] (c3_1) at (8,3) {$\neg x_3$};
        \node[circle, draw] (c3_2) at (9.5,3) {$\neg x_4$};
        \node[circle, draw] (c3_3) at (8.75,1.5) {$x_5$};
        \draw (c3_1) -- (c3_2) -- (c3_3) -- (c3_1);

        % Contradiction Edges
        \draw[dashed, red] (c1_2) -- (c2_1); % x_2 and not x_2
        \draw[dashed, red] (c1_3) -- (c2_2); % not x_3 and x_3
        \draw[dashed, red] (c1_3) -- (c3_1); % not x_3 and not x_3 (can be simplified if same literal in different clauses)
        \draw[dashed, red] (c2_2) -- (c3_1); % x_3 and not x_3
        \draw[dashed, red] (c2_3) -- (c3_2); % x_4 and not x_4
    \end{tikzpicture}
    \caption{Grafo $G$ costruito da $\phi$}
    \label{fig:is_reduction_graph}
\end{figure}

\textbf{Correttezza della Riduzione:}
Dobbiamo dimostrare che $\phi$ è soddisfacibile se e solo se $G$ ha un Independent Set di taglia $m$.

\subsubsection{$\implies$ (Se $\phi$ è soddisfacibile, allora $G$ ha un IS di taglia $m$)}
Supponiamo che $\phi$ sia soddisfacibile. Allora esiste un assegnamento di verità $\sigma$ che soddisfa $\phi$.
Costruiamo un insieme $S_\sigma$ nel grafo $G$ come segue: per ogni clausola $C_i$ di $\phi$, dato che $\sigma$ soddisfa $C_i$, esiste almeno un letterale in $C_i$ che è vero sotto $\sigma$. Scegliamo uno di questi letterali veri (arbitrariamente se ce ne sono più di uno) e aggiungiamo il nodo corrispondente a $S_\sigma$.
Poiché ci sono $m$ clausole e scegliamo esattamente un nodo per ogni clausola, la taglia di $S_\sigma$ sarà $|S_\sigma|=m$.

Ora, dobbiamo dimostrare che $S_\sigma$ è un Independent Set.
Assumiamo per contraddizione che $S_\sigma$ non sia un Independent Set. Questo significa che esistono due nodi $u, v \in S_\sigma$ tali che $(u, v) \in E'$.
Per costruzione degli archi, questi due nodi $u, v$ possono essere collegati in due modi:
\begin{enumerate}
    \item $u$ e $v$ provengono dalla stessa clausola: Questo è impossibile, perché abbiamo scelto solo un nodo per ogni clausola, e i nodi della stessa clausola sono sempre interconnessi. Se avessimo scelto due nodi dalla stessa clausola, questi sarebbero collegati, ma $S_\sigma$ è costruito selezionando un unico nodo per clausola.
    \item $u$ e $v$ provengono da clausole diverse, ma i loro letterali sono opposti: Se $u$ e $v$ sono collegati e provengono da clausole diverse, ciò implica che i loro letterali corrispondenti $l_u$ e $l_v$ sono opposti (es. $x$ e $\neg x$). Ma per costruzione di $S_\sigma$, sia $l_u$ che $l_v$ devono essere veri sotto l'assegnamento $\sigma$. Questo è impossibile, poiché $\sigma$ è un assegnamento di verità consistente (non può assegnare vero sia a $x$ che a $\neg x$).
\end{enumerate}
Entrambi i casi portano a una contraddizione. Pertanto, $S_\sigma$ deve essere un Independent Set di taglia $m$.

\subsubsection{$\impliedby$ (Se $G$ ha un IS di taglia $m$, allora $\phi$ è soddisfacibile)}
Supponiamo che $G$ abbia un Independent Set $S$ di taglia $m$.
Per costruzione, ogni "triangolo" di nodi corrispondente a una clausola $C_i$ forma una cricca di taglia 3. Poiché $S$ è un Independent Set, non può contenere più di un nodo da ciascuno di questi triangoli (altrimenti non sarebbe un IS, dato che tutti i nodi in un triangolo sono interconnessi).
Poiché $|S|=m$ (il numero di clausole), questo significa che $S$ deve contenere \emph{esattamente} un nodo da ciascuna delle $m$ triplette di nodi (triangoli) del grafo.

Ora, costruiamo un assegnamento di verità $\sigma_S$ per le variabili di $\phi$ basato su $S$:
\begin{itemize}
    \item Per ogni variabile $x_j$, se un nodo corrispondente al letterale $x_j$ è in $S$, allora $\sigma_S(x_j) = \text{Vero}$.
    \item Per ogni variabile $x_j$, se un nodo corrispondente al letterale $\neg x_j$ è in $S$, allora $\sigma_S(x_j) = \text{Falso}$.
    \item Se una variabile $x_j$ non ha né $x_j$ né $\neg x_j$ in $S$, le si può assegnare un valore arbitrario (es. Vero).
\end{itemize}
Dobbiamo dimostrare che $\sigma_S$ è un assegnamento consistente. Non può assegnare sia Vero che Falso alla stessa variabile, perché se così fosse, significherebbe che sia $x_j$ che $\neg x_j$ sono rappresentati da nodi in $S$. Ma per costruzione del grafo, nodi corrispondenti a letterali opposti sono collegati da un arco. Se $x_j$ e $\neg x_j$ fossero entrambi in $S$, $S$ non sarebbe un Independent Set, il che contraddice l'ipotesi. Quindi $\sigma_S$ è consistente.

Infine, dobbiamo dimostrare che $\sigma_S$ soddisfa $\phi$.
Per ogni clausola $C_i$, sappiamo che $S$ contiene esattamente un nodo $v$ proveniente dalla tripletta di nodi di $C_i$. Sia $l$ il letterale corrispondente a $v$. Per costruzione di $\sigma_S$, il valore di verità di $l$ sarà Vero sotto $\sigma_S$. Questo significa che ogni clausola $C_i$ contiene almeno un letterale vero, e quindi $\phi$ è soddisfatta da $\sigma_S$.

Poiché la trasformazione è polinomiale e la dimostrazione di equivalenza è valida in entrambi i versi, abbiamo dimostrato che $3SAT \le_p IS$. Dato che 3-SAT è NP-Hard, anche Independent Set è NP-Hard. Con IS $\in$ NP (dimostrato sopra), concludiamo che \textbf{Independent Set è NP-Completo}.
\end{proof*}

\section{Vertex Cover (VC)}

\begin{definition}[Vertex Cover]
Dato un grafo non orientato $G=(V, E)$, un \emph{Vertex Cover} (VC) $C \subseteq V$ è un sottoinsieme dei suoi nodi tale per cui ogni arco $(u,v) \in E$ ha almeno un endpoint in $C$. Formalmente:
\[ \forall (u, v) \in E, \quad u \in C \lor v \in C \]
Ogni grafo ammette un Vertex Cover (es. l'intero insieme di nodi $V$). I problemi interessanti riguardano la ricerca di Vertex Cover "piccoli".
\end{definition}

\begin{definition}[Vertex Cover (Problema di Decisione)]
Il problema di decisione \emph{Vertex Cover} è definito come l'insieme delle coppie $\langle G, K \rangle$ tali che $G$ è un grafo non orientato, $K$ è un numero intero, ed esiste un Vertex Cover in $G$ di taglia (cardinalità) al più $K$.
\end{definition}

\subsection{Membership in NP}

\begin{proposition}
Il problema \emph{Vertex Cover} appartiene alla classe NP.
\end{proposition}

\begin{proof*}
Simile a Independent Set:
\begin{enumerate}
    \item \textbf{Guess:} La MTND indovina un sottoinsieme $C'$ di nodi di $V$.
    \item \textbf{Check:} La MTND verifica deterministicamente:
    \begin{itemize}
        \item La cardinalità di $C'$ è al più $K$: $|C'| \le K$.
        \item Ogni arco è coperto da $C'$: Per ogni arco $(u, v) \in E$, verifica che $u \in C'$ oppure $v \in C'$.
    \end{itemize}
\end{enumerate}
Entrambi i passaggi sono polinomiali. Quindi, Vertex Cover $\in$ NP.
\end{proof*}

\subsection{Dimostrazione NP-Hardness: $IS \le_p VC$}

Per dimostrare che Vertex Cover è NP-Hard, effettuiamo una riduzione polinomiale da Independent Set a Vertex Cover. Questa riduzione si basa su un'importante proprietà di dualità.

\begin{lemma}[Dualità IS-VC]
Sia $G=(V, E)$ un grafo non orientato. Un sottoinsieme $S \subseteq V$ è un Independent Set di $G$ se e solo se il suo complemento $V \setminus S$ è un Vertex Cover di $G$.
\end{lemma}

\begin{proof*}
\begin{itemize}
    \item[$\implies$] Supponiamo che $S$ sia un Independent Set di $G$.
    Dobbiamo dimostrare che $C = V \setminus S$ è un Vertex Cover di $G$.
    Assumiamo per contraddizione che $C$ non sia un Vertex Cover. Questo significa che esiste almeno un arco $(u, v) \in E$ tale che nessuno dei suoi endpoint è in $C$. Se $u \notin C$ e $v \notin C$, allora per definizione di complemento, $u \in S$ e $v \in S$. Ma se $u, v \in S$ e $(u, v) \in E$, allora $S$ non sarebbe un Independent Set, il che contraddice la nostra ipotesi iniziale. Quindi la nostra assunzione è falsa, e $C$ deve essere un Vertex Cover.

    \item[$\impliedby$] Supponiamo che $C = V \setminus S$ sia un Vertex Cover di $G$.
    Dobbiamo dimostrare che $S$ è un Independent Set di $G$.
    Assumiamo per contraddizione che $S$ non sia un Independent Set. Questo significa che esiste almeno un arco $(u, v) \in E$ tale che entrambi i suoi endpoints $u, v$ sono in $S$. Ma se $u \in S$ e $v \in S$, allora $u \notin C$ e $v \notin C$. Questo significa che l'arco $(u, v)$ non è coperto da $C$, il che contraddice la nostra ipotesi che $C$ sia un Vertex Cover. Quindi la nostra assunzione è falsa, e $S$ deve essere un Independent Set.
\end{itemize}
Il lemma è dimostrato.
\end{proof*}

\begin{theorem}
$IS \le_p VC$. Di conseguenza, \emph{Vertex Cover} è NP-Hard.
\end{theorem}

\begin{proof*}
Sia $\langle G, K \rangle$ un'istanza di Independent Set. Vogliamo costruire una coppia $\langle H, L \rangle$ tale che $G$ ha un Independent Set di taglia almeno $K$ se e solo se $H$ ha un Vertex Cover di taglia al più $L$.

\textbf{Costruzione della Trasformazione ($f$):}
\begin{enumerate}
    \item \textbf{Grafo $H$:} $H = G$. Il grafo rimane invariato.
    \item \textbf{Valore $L$:} $L = |V| - K$, dove $|V|$ è il numero totale di nodi in $G$ (e quindi in $H$).
\end{enumerate}
Questa trasformazione è chiaramente polinomiale (ricopiare un grafo e fare una sottrazione).

\textbf{Correttezza della Riduzione:}

\subsubsection{$\implies$ (Se $G$ ha un IS di taglia almeno $K$, allora $H$ ha un VC di taglia al più $L$)}
Supponiamo che $\langle G, K \rangle$ sia un'istanza "sì" di Independent Set. Questo significa che esiste un Independent Set $S$ in $G$ tale che $|S| \ge K$.
Consideriamo il complemento di $S$ rispetto a $V$, ovvero $C = V \setminus S$.
Per il Lemma di Dualità IS-VC, $C$ è un Vertex Cover di $G$. Poiché $H=G$, $C$ è anche un Vertex Cover di $H$.
Calcoliamo la taglia di $C$: $|C| = |V| - |S|$.
Dato che $|S| \ge K$, ne segue che $-|S| \le -K$.
Quindi, $|C| = |V| - |S| \le |V| - K$.
Per costruzione, $L = |V| - K$. Dunque, $|C| \le L$.
Questo significa che $H$ ha un Vertex Cover $C$ di taglia al più $L$, quindi $\langle H, L \rangle$ è un'istanza "sì" di Vertex Cover.

\subsubsection{$\impliedby$ (Se $H$ ha un VC di taglia al più $L$, allora $G$ ha un IS di taglia almeno $K$)}
Supponiamo che $\langle H, L \rangle$ sia un'istanza "sì" di Vertex Cover. Questo significa che esiste un Vertex Cover $C$ in $H$ tale che $|C| \le L$.
Consideriamo il complemento di $C$ rispetto a $V_H$ (i nodi di $H$), ovvero $S = V_H \setminus C$.
Per il Lemma di Dualità IS-VC, $S$ è un Independent Set di $H$. Poiché $G=H$, $S$ è anche un Independent Set di $G$.
Calcoliamo la taglia di $S$: $|S| = |V_H| - |C|$.
Dato che $|C| \le L$, ne segue che $-|C| \ge -L$.
Quindi, $|S| = |V_H| - |C| \ge |V_H| - L$.
Per costruzione, $L = |V| - K$, e $|V_H| = |V|$. Sostituendo:
$|S| \ge |V| - (|V| - K) = |V| - |V| + K = K$.
Questo significa che $G$ ha un Independent Set $S$ di taglia almeno $K$, quindi $\langle G, K \rangle$ è un'istanza "sì" di Independent Set.

Poiché la trasformazione è polinomiale e la dimostrazione di equivalenza è valida in entrambi i versi, abbiamo dimostrato che $IS \le_p VC$. Dato che Independent Set è NP-Hard, anche Vertex Cover è NP-Hard. Con VC $\in$ NP (dimostrato sopra), concludiamo che \textbf{Vertex Cover è NP-Completo}.
\end{proof*}

\section{Clique}

\begin{definition}[Clique]
Dato un grafo non orientato $G=(V, E)$, una \emph{Clique} (o Cricca) $Q \subseteq V$ è un sottoinsieme dei suoi nodi tale per cui ogni coppia di nodi distinti in $Q$ è collegata da un arco. In altre parole, $Q$ forma un sottografo completo. Formalmente:
\[ \forall u, v \in Q, u \ne v \implies (u, v) \in E \]
Ogni grafo ammette una Clique (es. il set vuoto o un singolo nodo). I problemi interessanti riguardano la ricerca di Clique "grandi".
\end{definition}

\begin{definition}[Clique (Problema di Decisione)]
Il problema di decisione \emph{Clique} è definito come l'insieme delle coppie $\langle G, K \rangle$ tali che $G$ è un grafo non orientato, $K$ è un numero intero, ed esiste una Clique in $G$ di taglia (cardinalità) almeno $K$.
\end{definition}

\subsection{Membership in NP}

\begin{proposition}
Il problema \emph{Clique} appartiene alla classe NP.
\end{proposition}

\begin{proof*}
\begin{enumerate}
    \item \textbf{Guess:} La MTND indovina un sottoinsieme $Q'$ di nodi di $V$.
    \item \textbf{Check:} La MTND verifica deterministicamente:
    \begin{itemize}
        \item La cardinalità di $Q'$ è almeno $K$: $|Q'| \ge K$.
        \item Tutti i nodi in $Q'$ sono interconnessi: Per ogni coppia di nodi distinti $u, v \in Q'$, verifica che $(u, v) \in E$.
    \end{itemize}
\end{enumerate}
Entrambi i passaggi sono polinomiali. Quindi, Clique $\in$ NP.
\end{proof*}

\subsection{Dimostrazione NP-Hardness: $IS \le_p Clique$}

Per dimostrare che Clique è NP-Hard, effettuiamo una riduzione polinomiale da Independent Set a Clique. Questa riduzione si basa sul concetto di grafo complemento.

\begin{definition}[Grafo Complemento]
Dato un grafo non orientato $G=(V, E)$, il suo \emph{grafo complemento} $\bar{G}=(V, \bar{E})$ è un grafo con lo stesso insieme di nodi $V$, ma con l'insieme di archi $\bar{E}$ tale che $(u, v) \in \bar{E}$ se e solo se $(u, v) \notin E$ (per $u \ne v$). In altre parole, gli archi in $\bar{G}$ sono esattamente gli archi che non esistono in $G$.
\end{definition}

\begin{lemma}[Relazione IS-Clique nel grafo complemento]
Sia $G=(V, E)$ un grafo non orientato. Un sottoinsieme $S \subseteq V$ è un Independent Set di $G$ se e solo se $S$ è una Clique in $\bar{G}$.
\end{lemma}

\begin{proof*}
\begin{itemize}
    \item[$\implies$] Supponiamo che $S$ sia un Independent Set di $G$.
    Dobbiamo dimostrare che $S$ è una Clique in $\bar{G}$.
    Per definizione di Independent Set, per ogni coppia di nodi distinti $u, v \in S$, non esiste un arco $(u, v)$ in $E$.
    Per definizione di grafo complemento, se $(u, v) \notin E$, allora $(u, v) \in \bar{E}$.
    Quindi, per ogni coppia di nodi distinti $u, v \in S$, esiste un arco $(u, v)$ in $\bar{E}$. Questo significa che $S$ è una Clique in $\bar{G}$.

    \item[$\impliedby$] Supponiamo che $S$ sia una Clique in $\bar{G}$.
    Dobbiamo dimostrare che $S$ è un Independent Set di $G$.
    Per definizione di Clique, per ogni coppia di nodi distinti $u, v \in S$, esiste un arco $(u, v)$ in $\bar{E}$.
    Per definizione di grafo complemento, se $(u, v) \in \bar{E}$, allora $(u, v) \notin E$.
    Quindi, per ogni coppia di nodi distinti $u, v \in S$, non esiste un arco $(u, v)$ in $E$. Questo significa che $S$ è un Independent Set in $G$.
\end{itemize}
Il lemma è dimostrato.
\end{proof*}

\begin{theorem}
$IS \le_p Clique$. Di conseguenza, \emph{Clique} è NP-Hard.
\end{theorem}

\begin{proof*}
Sia $\langle G, K \rangle$ un'istanza di Independent Set. Vogliamo costruire una coppia $\langle H, L \rangle$ tale che $G$ ha un Independent Set di taglia almeno $K$ se e solo se $H$ ha una Clique di taglia almeno $L$.

\textbf{Costruzione della Trasformazione ($f$):}
\begin{enumerate}
    \item \textbf{Grafo $H$:} $H = \bar{G}$. $H$ è il grafo complemento di $G$.
    \item \textbf{Valore $L$:} $L = K$. Il valore $K$ viene copiato.
\end{enumerate}
La costruzione del grafo complemento può essere fatta in tempo polinomiale (iterando su tutte le possibili coppie di nodi e controllando l'esistenza di un arco in $G$). Quindi la trasformazione è polinomiale.

\textbf{Correttezza della Riduzione:}

\subsubsection{$\implies$ (Se $G$ ha un IS di taglia almeno $K$, allora $H$ ha una Clique di taglia almeno $L$)}
Supponiamo che $\langle G, K \rangle$ sia un'istanza "sì" di Independent Set. Questo significa che esiste un Independent Set $S$ in $G$ tale che $|S| \ge K$.
Per il Lemma di Relazione IS-Clique nel grafo complemento, $S$ è una Clique in $\bar{G}$.
Poiché $H=\bar{G}$, $S$ è una Clique in $H$.
La taglia di $S$ è $|S| \ge K$. Per costruzione, $L=K$.
Quindi, $H$ ha una Clique $S$ di taglia almeno $L$, il che significa che $\langle H, L \rangle$ è un'istanza "sì" di Clique.

\subsubsection{$\impliedby$ (Se $H$ ha una Clique di taglia almeno $L$, allora $G$ ha un IS di taglia almeno $K$)}
Supponiamo che $\langle H, L \rangle$ sia un'istanza "sì" di Clique. Questo significa che esiste una Clique $S$ in $H$ tale che $|S| \ge L$.
Per il Lemma di Relazione IS-Clique nel grafo complemento, $S$ è un Independent Set in $\bar{H}$.
Per costruzione, $H=\bar{G}$, quindi $\bar{H} = \overline{\bar{G}} = G$.
Pertanto, $S$ è un Independent Set di $G$.
La taglia di $S$ è $|S| \ge L$. Per costruzione, $L=K$.
Quindi, $G$ ha un Independent Set $S$ di taglia almeno $K$, il che significa che $\langle G, K \rangle$ è un'istanza "sì" di Independent Set.

Poiché la trasformazione è polinomiale e la dimostrazione di equivalenza è valida in entrambi i versi, abbiamo dimostrato che $IS \le_p Clique$. Dato che Independent Set è NP-Hard, anche Clique è NP-Hard. Con Clique $\in$ NP (dimostrato sopra), concludiamo che \textbf{Clique è NP-Completo}.
\end{proof*}

\section{Dominating Set (DS)}

\begin{definition}[Dominating Set]
Dato un grafo non orientato $G=(V, E)$, un \emph{Dominating Set} (DS) $D \subseteq V$ è un sottoinsieme dei suoi nodi tale per cui ogni nodo $v \in V \setminus D$ è adiacente ad almeno un nodo in $D$. In altre parole, ogni nodo fuori da $D$ è "dominato" da un nodo in $D$. Formalmente:
\[ \forall v \in V \setminus D, \exists u \in D \text{ tale che } (u, v) \in E \]
\end{definition}

\begin{example}
Consideriamo il grafo $G=(V, E)$ con $V=\{1,2,3,4,5\}$ ed $E=\{(1,2), (1,3), (2,3), (3,4), (4,5)\}$.
\begin{center}
\begin{tikzpicture}[node distance=1.5cm, thick]
    \node[circle, draw] (1) at (0,0) {1};
    \node[circle, draw] (2) at (1.5,1) {2};
    \node[circle, draw] (3) at (1.5,-1) {3};
    \node[circle, draw] (4) at (3,0) {4};
    \node[circle, draw] (5) at (4.5,0) {5};

    \draw (1) -- (2);
    \draw (1) -- (3);
    \draw (2) -- (3);
    \draw (3) -- (4);
    \draw (4) -- (5);
\end{tikzpicture}
\end{center}
Il sottoinsieme $D=\{3,5\}$ è un Dominating Set.
\begin{itemize}
    \item Nodo 1: è adiacente a 3 ($\in D$).
    \item Nodo 2: è adiacente a 3 ($\in D$).
    \item Nodo 4: è adiacente a 3 ($\in D$) e 5 ($\in D$).
\end{itemize}
Tutti i nodi fuori da $D$ sono dominati.
\end{example}

\textbf{Relazione tra Vertex Cover e Dominating Set:}
Un Vertex Cover è sempre un Dominating Set. Questo perché se ogni arco $(u,v)$ ha almeno un endpoint in $C$ (VC), allora ogni nodo $x \notin C$ deve per forza avere tutti i suoi vicini in $C$ (altrimenti l'arco che lo collega a un vicino fuori da $C$ non sarebbe coperto). Di conseguenza, ogni nodo $x \notin C$ è dominato da almeno un nodo in $C$.
Tuttavia, il viceversa non è vero. Nell'esempio precedente, $D=\{3,5\}$ è un Dominating Set di taglia 2. Ma non è un Vertex Cover, perché l'arco $(1,2)$ non è coperto (né 1 né 2 sono in $D$).
Quindi, un Vertex Cover è un caso più "stringente" rispetto a un Dominating Set.

\begin{definition}[Dominating Set (Problema di Decisione)]
Il problema di decisione \emph{Dominating Set} è definito come l'insieme delle coppie $\langle G, K \rangle$ tali che $G$ è un grafo non orientato, $K$ è un numero intero, ed esiste un Dominating Set in $G$ di taglia (cardinalità) al più $K$.
\end{definition}

\subsection{Membership in NP}

\begin{proposition}
Il problema \emph{Dominating Set} appartiene alla classe NP.
\end{proposition}

\begin{proof*}
\begin{enumerate}
    \item \textbf{Guess:} La MTND indovina un sottoinsieme $D'$ di nodi di $V$.
    \item \textbf{Check:} La MTND verifica deterministicamente:
    \begin{itemize}
        \item La cardinalità di $D'$ è al più $K$: $|D'| \le K$.
        \item Ogni nodo non in $D'$ è dominato: Per ogni nodo $v \in V \setminus D'$, verifica che esista un nodo $u \in D'$ tale che $(u, v) \in E$.
    \end{itemize}
\end{enumerate}
Entrambi i passaggi sono polinomiali. Quindi, Dominating Set $\in$ NP.
\end{proof*}

\subsection{Dimostrazione NP-Hardness: $VC \le_p DS$}

Per dimostrare che Dominating Set è NP-Hard, effettuiamo una riduzione polinomiale da Vertex Cover a Dominating Set.

\begin{theorem}
$VC \le_p DS$. Di conseguenza, \emph{Dominating Set} è NP-Hard.
\end{theorem}

\begin{proof*}
Sia $\langle G=(V,E), K \rangle$ un'istanza di Vertex Cover. Vogliamo costruire una coppia $\langle H=(V_H, E_H), L \rangle$ tale che $G$ ha un Vertex Cover di taglia al più $K$ se e solo se $H$ ha un Dominating Set di taglia al più $L$.

\textbf{Costruzione della Trasformazione ($f$):}
Il grafo $H$ viene costruito a partire da $G$ come segue:
\begin{enumerate}
    \item \textbf{Nodi ($V_H$):} $V_H$ contiene tutti i nodi di $V$ (i nodi originali). Per ogni arco $(u, v) \in E$ di $G$, aggiungiamo un nuovo nodo "ausiliario" $e_{uv}$ a $V_H$.
    \item \textbf{Archi ($E_H$):}
    \begin{itemize}
        \item Tutti gli archi originali di $G$ sono inclusi in $E_H$.
        \item Per ogni nodo ausiliario $e_{uv}$ (corrispondente all'arco $(u,v)$ in $G$), aggiungiamo archi che lo collegano ai suoi due endpoint originali: $(e_{uv}, u)$ e $(e_{uv}, v)$. Non aggiungiamo archi tra i nodi ausiliari.
    \end{itemize}
    \item \textbf{Valore $L$:} $L = K$. Il valore $K$ viene copiato.
\end{enumerate}
Questa costruzione è polinomiale. Se $G$ ha $N$ nodi e $M$ archi, $H$ avrà $N+M$ nodi e $M + 2M = 3M$ archi.

\textbf{Esempio di Trasformazione:}
Sia $G$ il grafo con $V=\{1,2,3,4\}$ ed $E=\{(1,2), (1,3), (2,3), (3,4)\}$.
\begin{center}
\begin{tikzpicture}[node distance=1.5cm, thick]
    % Original nodes
    \node[circle, draw] (1) at (0,2) {1};
    \node[circle, draw] (2) at (0,0) {2};
    \node[circle, draw] (3) at (2,0) {3};
    \node[circle, draw] (4) at (4,0) {4};

    % Original edges
    \draw (1) -- (2) node[midway, fill=white, inner sep=1pt] {};
    \draw (1) -- (3) node[midway, fill=white, inner sep=1pt] {};
    \draw (2) -- (3) node[midway, fill=white, inner sep=1pt] {};
    \draw (3) -- (4) node[midway, fill=white, inner sep=1pt] {};

    % New nodes (for edges) and their connections
    \node[circle, draw, scale=0.7] (e12) at (0,1) {$e_{12}$};
    \node[circle, draw, scale=0.7] (e13) at (1,1) {$e_{13}$};
    \node[circle, draw, scale=0.7] (e23) at (1,0) {$e_{23}$};
    \node[circle, draw, scale=0.7] (e34) at (3,0) {$e_{34}$};

    \draw (e12) -- (1); \draw (e12) -- (2);
    \draw (e13) -- (1); \draw (e13) -- (3);
    \draw (e23) -- (2); \draw (e23) -- (3);
    \draw (e34) -- (3); \draw (e34) -- (4);

\end{tikzpicture}
\end{center}
\captionof{figure}{Grafo $H$ costruito da $G$}
\label{fig:ds_reduction_graph}

\textbf{Correttezza della Riduzione:}

\subsubsection{$\implies$ (Se $G$ ha un VC di taglia al più $K$, allora $H$ ha un DS di taglia al più $L$)}
Supponiamo che $\langle G, K \rangle$ sia un'istanza "sì" di Vertex Cover. Questo significa che esiste un Vertex Cover $C$ in $G$ tale che $|C| \le K$.
Sosteniamo che $D=C$ (usando gli stessi nodi) è un Dominating Set di $H$. La taglia di $D$ è $|D|=|C| \le K = L$. Dobbiamo solo dimostrare che $D$ domina tutti i nodi in $H \setminus D$.
I nodi in $H$ sono di due tipi: nodi originali ($V$) e nodi ausiliari ($E_{uv}$).
\begin{itemize}
    \item \textbf{Nodi originali non in $D$ ($v \in V \setminus D$):} Poiché $C$ è un Vertex Cover di $G$, ogni arco $(v,u) \in E$ incidente a $v$ deve avere il suo altro endpoint $u \in C$ (dato che $v \notin C$). Quindi $v$ è adiacente a $u \in C$. Poiché $C \subseteq D$, $v$ è dominato da $u \in D$.
    \item \textbf{Nodi ausiliari ($e_{uv}$):} Per definizione di Vertex Cover, l'arco $(u,v)$ in $G$ è coperto da $C$. Questo significa che $u \in C$ oppure $v \in C$. Poiché $e_{uv}$ è collegato sia a $u$ che a $v$ in $H$, se $u \in C$ allora $e_{uv}$ è dominato da $u \in D$. Se $v \in C$ allora $e_{uv}$ è dominato da $v \in D$. In ogni caso, $e_{uv}$ è dominato da un nodo in $D$.
\end{itemize}
Quindi, $D=C$ è un Dominating Set di $H$ di taglia al più $L$. Perciò, $\langle H, L \rangle$ è un'istanza "sì" di Dominating Set.

\subsubsection{$\impliedby$ (Se $H$ ha un DS di taglia al più $L$, allora $G$ ha un VC di taglia al più $K$)}
Supponiamo che $\langle H, L \rangle$ sia un'istanza "sì" di Dominating Set. Questo significa che esiste un Dominating Set $D$ in $H$ tale che $|D| \le L$.
Costruiamo un insieme $C'$ di nodi originali a partire da $D$. L'idea è convertire i nodi ausiliari in $D$ in nodi originali, se necessario.
Sia $C' \subseteq V$ l'insieme dei nodi originali in $V$ che sono in $D$. Se un nodo ausiliario $e_{uv}$ è in $D$, allora aggiungiamo $u$ (o $v$, uno qualsiasi dei due) a $C'$. In altre parole:
\[ C' = (D \cap V) \cup \{ u \mid e_{uv} \in D \text{ per qualche } v \in V \text{ e } u \text{ è un endpoint di } (u,v) \} \]
La taglia di $C'$ sarà al più $|D|$ (poiché ogni $e_{uv}$ in $D$ è sostituito da un solo nodo originale in $C'$), e quindi $|C'| \le |D| \le L = K$.

Ora, dobbiamo dimostrare che $C'$ è un Vertex Cover di $G$.
Assumiamo per contraddizione che $C'$ non sia un Vertex Cover di $G$. Ciò significa che esiste almeno un arco $(x, y) \in E$ in $G$ tale che né $x$ né $y$ sono in $C'$.
Consideriamo il nodo ausiliario $e_{xy}$ in $H$ che corrisponde all'arco $(x, y)$ in $G$.
Per ipotesi, $D$ è un Dominating Set di $H$. Quindi, $e_{xy}$ deve essere dominato da un nodo in $D$.
Un nodo in $D$ che domina $e_{xy}$ può essere:
\begin{enumerate}
    \item $e_{xy}$ stesso: Se $e_{xy} \in D$, allora per costruzione di $C'$, uno dei suoi endpoint ($x$ o $y$) sarebbe stato aggiunto a $C'$. Ma abbiamo assunto che né $x$ né $y$ sono in $C'$. Questo è una contraddizione.
    \item Un nodo originale $v \in V \cap D$ adiacente a $e_{xy}$: Gli unici nodi originali adiacenti a $e_{xy}$ sono $x$ e $y$. Se $x \in D$ o $y \in D$, allora per costruzione di $C'$, $x \in C'$ o $y \in C'$. Questo contraddice la nostra assunzione che né $x$ né $y$ sono in $C'$.
\end{enumerate}
Entrambi i casi portano a una contraddizione. Pertanto, l'assunzione che $C'$ non sia un Vertex Cover è falsa.
Quindi, $C'$ è un Vertex Cover di $G$ di taglia al più $K$. Ciò significa che $\langle G, K \rangle$ è un'istanza "sì" di Vertex Cover.

Poiché la trasformazione è polinomiale e la dimostrazione di equivalenza è valida in entrambi i versi, abbiamo dimostrato che $VC \le_p DS$. Dato che Vertex Cover è NP-Hard, anche Dominating Set è NP-Hard. Con DS $\in$ NP (dimostrato sopra), concludiamo che \textbf{Dominating Set è NP-Completo}.
\end{proof*}

\section{Conclusioni}
Oggi abbiamo esaminato diversi problemi NP-Completi sui grafi, dimostrando la loro NP-Completezza tramite riduzioni polinomiali. Abbiamo visto come i problemi NP-Completi siano tutti interconnessi attraverso queste riduzioni, formando una "catena" di complessità. È fondamentale comprendere il metodo delle riduzioni, la costruzione di un'istanza e la dimostrazione della sua correttezza in entrambi i versi. Per padroneggiare questi concetti, è altamente consigliabile rivedere le dimostrazioni e provare a ricrearle autonomamente.


% =====================================================
% --- START LECTURE 20 ---
% =====================================================

\chapter{Nuove Definizioni di NP e NP-Completezza della Colorabilità dei Grafi}



\section{Introduzione e Ripasso}
Nella lezione precedente sono stati introdotti i concetti di classi di complessità P e NP, e le riduzioni polinomiali.
Il professor Malizia ha ripassato la struttura delle dimostrazioni di riduzione:
\begin{itemize}
    \item Si vuole dimostrare una riduzione da un problema A a un problema B ($A \le_p B$).
    \item Si deve costruire una funzione $f$ (algoritmo) che trasforma un'istanza $w$ di A in un'istanza $f(w)$ di B.
    \item Questa funzione $f$ deve essere calcolabile in tempo polinomiale.
    \item Si deve dimostrare l'equivalenza: $w \in A \iff f(w) \in B$. Questo richiede una dimostrazione in due direzioni (doppia implicazione logica).
    \item È fondamentale definire con precisione cosa sia un'istanza "sì" e un'istanza "no" per entrambi i problemi A e B. L'interpretazione vaga di questi termini può portare a confusioni.
\end{itemize}

\section{Una Nuova Caratterizzazione della Classe NP}

\subsection{Critica alla Definizione Classica di NP}
La definizione classica della classe NP è:
\[ NP = \bigcup_{c \ge 1} NTIME(n^c) \]
dove $NTIME(n^c)$ è l'insieme dei linguaggi decidibili da Macchine di Turing non deterministiche in tempo $O(n^c)$.

La critica a questa definizione risiede nel fatto che essa si basa su un modello di calcolo, la Macchina di Turing non deterministica, che non è realizzabile. Inoltre, il non determinismo, in base a questa definizione, può essere utilizzato in qualsiasi punto della computazione, non necessariamente all'inizio. Tuttavia, negli esempi pratici di problemi in NP (es. SAT, Independent Set, ecc.), si osserva un pattern comune: si "indovina" una soluzione (un "certificato") e poi la si "verifica" deterministicamente in tempo polinomiale.

\subsection{Il Concetto di Certificato}
L'intuizione alla base della nuova caratterizzazione di NP è che i problemi in NP sono quelli per i quali una soluzione (se esiste) può essere verificata efficientemente. La "soluzione indovinata" viene chiamata \textbf{certificato} o \textbf{testimone}.
Un certificato deve possedere due proprietà fondamentali:
\begin{itemize}
    \item \textbf{Conciso}: la sua lunghezza non deve essere troppo grande rispetto all'input, tipicamente polinomiale nella lunghezza dell'input.
    \item \textbf{Polinomialmente Verificabile}: la sua correttezza può essere controllata da un algoritmo deterministico in tempo polinomiale.
\end{itemize}

\subsection{Definizione di NP tramite Relazioni Binarie}

Sia $\Sigma^*$ l'insieme di tutte le stringhe sull'alfabeto $\Sigma$. Una \textbf{relazione binaria} $R$ su $\Sigma^*$ è semplicemente un sottoinsieme del prodotto cartesiano $\Sigma^* \times \Sigma^*$. Ovvero, $R$ è un insieme di coppie $(x, y)$ di stringhe.

\begin{definition}[Relazione Polinomialmente Bilanciata]
Una relazione binaria $R \subseteq \Sigma^* \times \Sigma^*$ si dice \textbf{polinomialmente bilanciata} se esiste una costante $c > 0$ tale che per ogni $(x, y) \in R$, la lunghezza di $y$ è limitata da un polinomio della lunghezza di $x$:
\[ |y| \le |x|^c \]
Questa proprietà assicura che il "certificato" $y$ (la seconda componente della coppia) non sia eccessivamente lungo rispetto all'istanza $x$ (la prima componente).
\end{definition}

\begin{definition}[Relazione Polinomialmente Decidibile]
Una relazione binaria $R \subseteq \Sigma^* \times \Sigma^*$ si dice \textbf{polinomialmente decidibile} se esiste una Macchina di Turing deterministica che, su input $(x, y)$ (codificato opportunamente come una singola stringa), decide in tempo polinomiale se $(x, y) \in R$ oppure no.
Questa proprietà assicura che la "verifica" del certificato sia efficiente.
\end{definition}

\begin{theorem}[Caratterizzazione di NP]
Un linguaggio $L$ appartiene alla classe NP se e solo se esiste una relazione binaria $R_L \subseteq \Sigma^* \times \Sigma^*$ che è sia \textbf{polinomialmente bilanciata} che \textbf{polinomialmente decidibile}, tale che:
\[ L = \{x \in \Sigma^* \mid \exists y \in \Sigma^* \text{ tale che } (x, y) \in R_L \} \]
In altre parole, un linguaggio $L$ è in NP se le sue istanze "sì" sono esattamente quelle $x$ per cui esiste un "certificato" $y$ (cioè la seconda componente della coppia $(x, y)$ nella relazione $R_L$) che è conciso e verificabile deterministicamente in tempo polinomiale.
\end{theorem}

\begin{proof}
\textbf{Parte 1: ($\impliedby$) Se esiste $R_L$ che è polinomialmente bilanciata e decidibile, allora $L \in NP$.}

Sia $L = \{x \mid \exists y \text{ t.c. } (x, y) \in R_L\}$, dove $R_L$ è polinomialmente bilanciata e polinomialmente decidibile.
Dobbiamo mostrare che $L$ può essere deciso da una Macchina di Turing non deterministica (NTM) $M'$ in tempo polinomiale.

L'NTM $M'$ per decidere $L$ funziona come segue:
\begin{enumerate}
    \item Su input $x$:
    \item $M'$ indovina non deterministicamente una stringa $y$. Poiché $R_L$ è polinomialmente bilanciata, sappiamo che la lunghezza di $y$ non eccede $|x|^c$ per una qualche costante $c$. Quindi, $M'$ può "scrivere" (ovvero indovinare i bit e scriverli sul nastro) $y$ in tempo polinomiale rispetto a $|x|$.
    \item $M'$ quindi controlla se la coppia $(x, y)$ appartiene a $R_L$. Poiché $R_L$ è polinomialmente decidibile, esiste una Macchina di Turing deterministica che esegue questo controllo in tempo polinomiale. $M'$ simula questa MT deterministica.
    \item Se il controllo restituisce "sì" (cioè $(x, y) \in R_L$), $M'$ accetta l'input $x$. Altrimenti, $M'$ rifiuta.
\end{enumerate}
Poiché sia la fase di indovinamento (scrittura di $y$) che la fase di verifica (decisione di $R_L$) avvengono in tempo polinomiale, l'intera computazione di $M'$ è in tempo polinomiale. Dunque, $L \in NP$.

\textbf{Parte 2: ($\implies$) Se $L \in NP$, allora esiste $R_L$ che è polinomialmente bilanciata e decidibile.}

Sia $L \in NP$. Per definizione, esiste una Macchina di Turing non deterministica $M$ che decide $L$ in tempo polinomiale. Sia $p(n)$ un polinomio tale che $M$ decide $L$ in tempo $p(n)$ (dove $n$ è la lunghezza dell'input).

Definiamo la relazione $R_L$ come segue:
\[ R_L = \{(x, y) \mid y \text{ è una sequenza accettante di configurazioni di } M \text{ su input } x \} \]
In altre parole, $(x, y) \in R_L$ se $x$ è l'input, e $y$ è una stringa che codifica una sequenza valida di descrizioni istantanee (ID) di $M$ che porta $M$ ad accettare $x$. Una sequenza accettante di configurazioni è del tipo $ID_0 \to ID_1 \to \dots \to ID_k$, dove $ID_0$ è la configurazione iniziale su $x$, $ID_k$ è una configurazione accettante, e ogni $ID_{i+1}$ è raggiungibile da $ID_i$ con una mossa legale di $M$.

Dobbiamo dimostrare che $R_L$ è polinomialmente bilanciata e polinomialmente decidibile.

\begin{itemize}
    \item \textbf{$R_L$ è polinomialmente bilanciata}:
    Poiché $M$ lavora in tempo $p(n)$, il numero di passi in una computazione accettante è al massimo $p(n)$. Questo significa che la sequenza $y$ contiene al massimo $p(n)$ descrizioni istantanee ($ID_0, \dots, ID_{p(n)-1}$).
    Ogni descrizione istantanea (ID) ha una lunghezza che dipende dalla lunghezza del nastro della macchina. Poiché $M$ esegue al più $p(n)$ passi, la sua testina può visitare al più $p(n)$ celle del nastro. Quindi, la lunghezza di ciascun $ID_i$ è $O(p(n))$.
    La lunghezza totale di $y$ (la sequenza di ID concatenati, opportunamente delimitati) sarà al più $p(n) \times O(p(n)) = O(p(n)^2)$. Poiché $p(n)$ è un polinomio, $p(n)^2$ è anch'esso un polinomio. Quindi, la lunghezza di $y$ è polinomiale nella lunghezza di $x$. Dunque, $R_L$ è polinomialmente bilanciata.

    \item \textbf{$R_L$ è polinomialmente decidibile}:
    Per decidere se $(x, y) \in R_L$ con una Macchina di Turing deterministica, dobbiamo verificare la validità della sequenza di configurazioni $y$. Questo può essere fatto come segue:
    \begin{enumerate}
        \item Verificare che $ID_0$ sia la configurazione iniziale corretta di $M$ su input $x$. (Tempo polinomiale).
        \item Verificare che $ID_k$ (l'ultima configurazione in $y$) sia uno stato accettante di $M$. (Tempo polinomiale).
        \item Per ogni $i$ da $0$ a $k-1$, verificare che $ID_{i+1}$ sia raggiungibile da $ID_i$ con una mossa legale di $M$. Questo implica controllare che la funzione di transizione di $M$ permetta il passaggio da $ID_i$ a $ID_{i+1}$. Questo controllo è una costante per ogni coppia di ID. Poiché ci sono al più $p(n)$ coppie di ID da controllare, il tempo totale per questa verifica è polinomiale.
    \end{enumerate}
    Tutte queste verifiche possono essere eseguite da una Macchina di Turing deterministica in tempo polinomiale. Dunque, $R_L$ è polinomialmente decidibile.
\end{itemize}
Abbiamo dimostrato entrambi i versi del teorema.
\end{proof}

\subsection{Implicazione della Nuova Caratterizzazione}
Questo teorema fornisce una prospettiva alternativa e molto intuitiva sulla classe NP:
\begin{quote}
    I linguaggi in \textbf{P} sono quelli per cui una soluzione può essere \emph{calcolata} in tempo polinomiale. \\
    I linguaggi in \textbf{NP} sono quelli per cui una soluzione può essere \emph{verificata} in tempo polinomiale (una volta fornito un certificato).
\end{quote}
La domanda P vs NP si riduce quindi a chiedersi se ogni problema la cui soluzione può essere facilmente verificata è anche un problema la cui soluzione può essere facilmente calcolata. La comune convinzione è che P $\ne$ NP.

\section{Problema della Colorabilità dei Grafi}
Il problema della colorabilità dei grafi è un altro esempio classico di problema NP-completo.

\begin{definition}[Colorazione di un Grafo]
Una \textbf{colorazione} di un grafo $G=(V, E)$ è una funzione $c: V \to \{1, \dots, k\}$ (dove $k$ è il numero di colori) tale che per ogni arco $(u, v) \in E$, si ha $c(u) \ne c(v)$.
\end{definition}

\begin{definition}[Problema della Colorabilità (GRAPH COLORING)]
Il problema della \textbf{Colorabilità dei Grafi} prende come input una coppia $(G, k)$, dove $G$ è un grafo e $k$ è un intero.
La domanda è: "È possibile colorare i nodi di $G$ con \emph{al più} $k$ colori tale che nessun nodo adiacente abbia lo stesso colore?"
\end{definition}

\subsection{Membership di GRAPH COLORING in NP}
Per dimostrare che GRAPH COLORING $\in NP$:
\begin{itemize}
    \item \textbf{Certificato}: Un certificato per un'istanza "sì" $(G, k)$ è una colorazione valida $c: V \to \{1, \dots, k\}$.
    \item \textbf{Concisezza del Certificato}: La colorazione è specificata assegnando un colore a ciascun nodo. Ci sono $|V|$ nodi, e ogni colore può essere rappresentato in $O(\log k)$ bit. La lunghezza del certificato è quindi $O(|V| \log k)$, che è polinomiale nella dimensione dell'input (che include $|V|$, $|E|$ e $k$).
    \item \textbf{Verificabilità Polinomiale}: Per verificare la correttezza di una data colorazione $c$:
        \begin{enumerate}
            \item Controllare che tutti i colori usati siano $\le k$.
            \item Per ogni arco $(u, v) \in E$, controllare che $c(u) \ne c(v)$.
        \end{enumerate}
    Entrambi i passi possono essere eseguiti in tempo polinomiale. Il secondo passo richiede di iterare su tutti gli archi, prendendo $O(|E|)$ tempo. Poiché $|E|$ è al più $O(|V|^2)$, la verifica è polinomiale.
\end{itemize}
Pertanto, GRAPH COLORING $\in NP$.

\subsection{NP-Hardness di GRAPH COLORING (Riduzione da 3SAT)}
Dimostriamo che GRAPH COLORING è NP-Hard riducendo 3SAT a GRAPH COLORING ($3SAT \le_p GRAPH\ COLORING$).

\subsubsection{Definizione dei Problemi per la Riduzione}
\begin{itemize}
    \item \textbf{Problema di Partenza (3SAT)}:
        \begin{itemize}
            \item \textbf{Input}: Una formula booleana $\phi$ in 3-CNF (congiunzione di clausole, ogni clausola è una disgiunzione di esattamente 3 letterali).
            \item \textbf{Istanza "Sì"}: Esiste un assegnamento di verità alle variabili di $\phi$ che rende la formula vera (cioè $\phi$ è soddisfacibile).
            \item \textbf{Istanza "No"}: Nessun assegnamento di verità rende $\phi$ vera (cioè $\phi$ è insoddisfacibile).
        \end{itemize}
    \item \textbf{Problema di Arrivo (GRAPH COLORING)}:
        \begin{itemize}
            \item \textbf{Input}: Una coppia $(G, k)$ dove $G$ è un grafo e $k$ è un intero.
            \item \textbf{Istanza "Sì"}: $G$ può essere colorato con al più $k$ colori in modo che nodi adiacenti abbiano colori diversi.
            \item \textbf{Istanza "No"}: $G$ non può essere colorato con al più $k$ colori senza violare la condizione.
        \end{itemize}
\end{itemize}

L'obiettivo è costruire una funzione polinomiale $f$ che prende una formula $\phi$ in 3-CNF e produce una coppia $(G, 3)$ (il valore $k$ sarà fissato a 3) tale che $\phi$ è soddisfacibile se e solo se $G$ è 3-colorabile.

\subsubsection{Costruzione del Grafo $G$}
Sia $\phi = C_1 \land C_2 \land \dots \land C_m$ una formula con $n$ variabili $x_1, \dots, x_n$ e $m$ clausole $C_1, \dots, C_m$.

Il grafo $G$ sarà costruito utilizzando diversi "gadget" (sottografi) che simulano la logica booleana. Il numero di colori $k$ sarà fissato a 3. I colori saranno identificati come "Vero" (verde), "Falso" (rosso) e "Base/Neutro" (blu).

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[node distance=1.5cm, thick]
        % Truth Gadget
        \node[circle, draw, fill=green!50!black] (T) {T};
        \node[circle, draw, fill=red!50!black, below right of=T] (F) {F};
        \node[circle, draw, fill=blue!50!black, below left of=F] (B) {B};

        \draw (T) -- (F);
        \draw (T) -- (B);
        \draw (F) -- (B);

        % Variable Gadgets
        \node[circle, draw, right=2cm of T] (x1) {$x_{1}$};
        \node[circle, draw, below=0.5cm of x1] (nx1) {$\neg x_{1}$};
        \draw (x1) -- (nx1);
        \draw (x1) -- (B);
        \draw (nx1) -- (B);
        
        \node[circle, draw, right=1.5cm of x1] (x2) {$x_{2}$};
        \node[circle, draw, below=0.5cm of x2] (nx2) {$\neg x_{2}$};
        \draw (x2) -- (nx2);
        \draw (x2) -- (B);
        \draw (nx2) -- (B);
        
        \node[circle, draw, right=1.5cm of x2] (x3) {$x_{3}$};
        \node[circle, draw, below=0.5cm of x3] (nx3) {$\neg x_{3}$};
        \draw (x3) -- (nx3);
        \draw (x3) -- (B);
        \draw (nx3) -- (B);
        
        \node[circle, draw, right=1.5cm of x3] (x4) {$x_{4}$};
        \node[circle, draw, below=0.5cm of x4] (nx4) {$\neg x_{4}$};
        \draw (x4) -- (nx4);
        \draw (x4) -- (B);
        \draw (nx4) -- (B);

        % Clause Gadgets (simplified representation for the diagram)
        % The professor's description of the OR gadget is:
        % A structure such that if two inputs are False, the output is False.
        % If at least one input is True, the output can be True.
        % The description uses 3 nodes in a triangle, with intermediate nodes connected to B.

        % Let's draw a generic OR gadget for a clause (L1 v L2 v L3)
        % L1, L2, L3 are the literal nodes (x_i or nx_i)
        % There's a structure that makes output node "True" if any literal is True,
        % and "False" if all literals are False.
        % The output node is then forced to be True by connecting it to B and F.

        % Assuming a specific structure for the OR gadget
        % This is a common way to draw it, but the professor described it verbally by showing
        % how coloring propagates.
        % Let's draw the abstract clause gadgets and their connection.

        \node[draw, rounded corners, right=2cm of x4] (C1_box) {Clause 1: $X_1 \lor X_2 \lor \neg X_3$};
        \node[draw, rounded corners, below=0.5cm of C1_box] (C2_box) {Clause 2: $\neg X_2 \lor X_3 \lor \neg X_4$};

        % Connections from literals to Clause Gadgets inputs (simplified for clarity)
        \draw[dotted] (x1) -- (C1_box);
        \draw[dotted] (x2) -- (C1_box);
        \draw[dotted] (nx3) -- (C1_box);
        \draw[dotted] (nx2) -- (C2_box);
        \draw[dotted] (x3) -- (C2_box);
        \draw[dotted] (nx4) -- (C2_box);

        % Output nodes of Clause Gadgets (implicitly within C1_box, C2_box)
        % These output nodes are forced to be True (green) by connecting them to B and F.
        % This is drawn from the box itself for simplicity, as the internal structure of the gadget is complex.
        \node[circle, draw, fill=yellow!50] (C1_out) [right=1cm of C1_box] {Output $C_1$};
        \node[circle, draw, fill=yellow!50] (C2_out) [right=1cm of C2_box] {Output $C_2$};

        \draw (C1_out) -- (B);
        \draw (C1_out) -- (F);
        \draw (C2_out) -- (B);
        \draw (C2_out) -- (F);

    \end{tikzpicture}
    \caption{Struttura Generale del Grafo $G$ per la Riduzione 3SAT $\to$ GRAPH COLORING}
    \label{fig:reduction_graph_structure}
\end{figure}

Il grafo $G$ è composto da:
\begin{enumerate}
    \item \textbf{Truth Gadget}: Una cricca di 3 nodi: $T$ (True), $F$ (False), $B$ (Base/Neutral). Questi 3 nodi devono necessariamente avere colori distinti. Assegneremo a $T$ il colore "Vero" (es. verde), a $F$ il colore "Falso" (es. rosso), e a $B$ il colore "Base" (es. blu). Questi colori serviranno da riferimento.

    \item \textbf{Variable Gadgets}: Per ogni variabile booleana $x_i$ nella formula $\phi$, creiamo due nodi: $x_i$ e $\neg x_i$.
    \begin{itemize}
        \item Colleghiamo $x_i$ e $\neg x_i$ con un arco. Questo assicura che debbano avere colori diversi.
        \item Colleghiamo sia $x_i$ che $\neg x_i$ al nodo $B$ del Truth Gadget. Questo implica che $x_i$ e $\neg x_i$ non possono avere il colore "Base". Quindi, possono solo assumere il colore "Vero" o "Falso".
    \end{itemize}
    In questo modo, se $x_i$ riceve il colore di $T$ (Vero), $\neg x_i$ deve ricevere il colore di $F$ (Falso), e viceversa. Questo simula un assegnamento di verità coerente.

    \item \textbf{Clause Gadgets}: Per ogni clausola $C_j = (L_{j1} \lor L_{j2} \lor L_{j3})$ (dove $L_{jk}$ è un letterale, $x_i$ o $\neg x_i$), si costruisce un sottografo che simula la funzione logica OR.
    \begin{itemize}
        \item Questo gadget riceve come "input" i nodi corrispondenti ai letterali $L_{j1}, L_{j2}, L_{j3}$.
        \item Il gadget è progettato in modo che se tutti e tre i nodi input ($L_{j1}, L_{j2}, L_{j3}$) sono colorati con il colore "Falso" (rosso), allora un nodo "output" all'interno del gadget deve essere colorato con il colore "Falso" (rosso).
        \item Se almeno uno dei nodi input è colorato con il colore "Vero" (verde), allora il nodo "output" può essere colorato con il colore "Vero" (verde).
        \item Il nodo "output" di ogni Clause Gadget è poi collegato ai nodi $B$ (Base) e $F$ (False) del Truth Gadget. Questo costringe il nodo "output" ad essere colorato con il colore "Vero" (verde), poiché non può essere "Base" (blu) né "Falso" (rosso) a causa dei suoi collegamenti.
    \end{itemize}
    L'intera costruzione del grafo $G$ è polinomiale nella dimensione della formula $\phi$. Il numero di nodi e archi è proporzionale al numero di variabili e clausole.
\end{enumerate}

\subsubsection{Dimostrazione dell'Equivalenza}

Dobbiamo mostrare che $\phi$ è soddisfacibile $\iff$ $G$ è 3-colorabile.

\textbf{Parte 1: ($\implies$) Se $\phi$ è soddisfacibile, allora $G$ è 3-colorabile.}

Supponiamo che $\phi$ sia soddisfacibile. Sia $\sigma$ un assegnamento di verità che soddisfa $\phi$. Costruiamo una 3-colorazione $C$ di $G$ come segue:
\begin{enumerate}
    \item Coloriamo il Truth Gadget: $C(T) = \text{verde}$, $C(F) = \text{rosso}$, $C(B) = \text{blu}$. Questo è valido in quanto sono una cricca di 3 nodi e usiamo 3 colori distinti.
    \item Coloriamo i Variable Gadgets: Per ogni variabile $x_i$:
    \begin{itemize}
        \item Se $\sigma(x_i) = \text{TRUE}$, allora $C(x_i) = \text{verde}$ e $C(\neg x_i) = \text{rosso}$.
        \item Se $\sigma(x_i) = \text{FALSE}$, allora $C(x_i) = \text{rosso}$ e $C(\neg x_i) = \text{verde}$.
    \end{itemize}
    Questa colorazione è valida perché $x_i$ e $\neg x_i$ sono collegati solo tra loro e a $B$. I colori "verde" e "rosso" sono distinti tra loro e distinti dal colore di $B$ ("blu").
    \item Coloriamo i Clause Gadgets: Per ogni clausola $C_j$:
    Poiché $\sigma$ soddisfa $\phi$, ogni clausola $C_j$ è vera sotto $\sigma$. Ciò significa che almeno uno dei suoi letterali $L_{j1}, L_{j2}, L_{j3}$ è vero.
    Per come abbiamo colorato i Variable Gadgets, questo implica che almeno uno dei nodi corrispondenti a $L_{j1}, L_{j2}, L_{j3}$ è colorato "verde".
    La proprietà del Clause Gadget (OR) assicura che se almeno uno dei suoi input è "verde", il suo nodo "output" può essere colorato "verde".
    Siccome il nodo "output" di ogni clausola è collegato ai nodi $B$ e $F$, ed è colorato "verde" (che è diverso da "blu" e "rosso"), la colorazione è valida.
\end{enumerate}
Tutti i nodi sono colorati con 3 colori, e nessuna adiacenza ha lo stesso colore. Quindi, $G$ è 3-colorabile.

\textbf{Parte 2: ($\impliedby$) Se $G$ è 3-colorabile, allora $\phi$ è soddisfacibile.}

Supponiamo che $G$ sia 3-colorabile. Sia $C$ una 3-colorazione valida di $G$.
Definiamo un assegnamento di verità $\sigma$ per le variabili di $\phi$ basandoci sui colori in $C$:
\begin{enumerate}
    \item Per ogni variabile $x_i$:
    \begin{itemize}
        \item Se $C(x_i)$ è lo stesso colore di $C(T)$ (cioè "verde"), poniamo $\sigma(x_i) = \text{TRUE}$.
        \item Se $C(x_i)$ è lo stesso colore di $C(F)$ (cioè "rosso"), poniamo $\sigma(x_i) = \text{FALSE}$.
    \end{itemize}
    (Nota: Per costruzione, $x_i$ e $\neg x_i$ sono collegati a $B$, quindi non possono avere il colore "blu". Essendo collegati tra loro, devono avere colori diversi, quindi se $x_i$ è "verde", $\neg x_i$ deve essere "rosso" e viceversa, garantendo coerenza nell'assegnamento).
\end{enumerate}
Dobbiamo dimostrare che questo assegnamento $\sigma$ soddisfa la formula $\phi$.
Supponiamo per contraddizione che $\sigma$ non soddisfi $\phi$. Questo significa che esiste almeno una clausola $C_j$ che è falsa sotto $\sigma$.
Se $C_j = (L_{j1} \lor L_{j2} \lor L_{j3})$ è falsa sotto $\sigma$, allora tutti i suoi letterali $L_{j1}, L_{j2}, L_{j3}$ sono falsi sotto $\sigma$.
Per come abbiamo definito $\sigma$, questo implica che i nodi corrispondenti a $L_{j1}, L_{j2}, L_{j3}$ nel grafo $G$ sono tutti colorati con il colore di $F$ (cioè "rosso").
Ma per la proprietà del Clause Gadget (OR): se tutti i suoi nodi input sono "rossi", allora il suo nodo "output" deve essere anch'esso "rosso".
Tuttavia, il nodo "output" di $C_j$ è collegato nel grafo ai nodi $B$ e $F$. Poiché $C(F)$ è "rosso", se il nodo "output" è anch'esso "rosso", si avrebbe una violazione della colorazione (due nodi adiacenti con lo stesso colore). Questa è una contraddizione, poiché abbiamo assunto che $C$ fosse una colorazione valida.
Pertanto, la nostra supposizione iniziale (che $\sigma$ non soddisfi $\phi$) deve essere falsa. Dunque, $\sigma$ soddisfa $\phi$.

Poiché la trasformazione è polinomiale e l'equivalenza è dimostrata, $3SAT \le_p GRAPH\ COLORING$.
Dato che 3SAT è NP-completo, e GRAPH COLORING è in NP e NP-hard, concludiamo che GRAPH COLORING è \textbf{NP-completo}.


% =====================================================
% --- START LECTURE 21 ---
% =====================================================

\chapter{NP-Completezza Exact Cover e Knapsack}



\section{Introduzione alle Classi di Complessità}

Questa lezione conclude l'introduzione esplicita alla classe NP, prima di esplorare altre classi di complessità come la complessità spaziale e le classi di funzioni (problemi di calcolo piuttosto che di decisione). Verrà inoltre dimostrato il Teorema di Cook nella prossima lezione, che stabilisce che SAT è NP-completo.

\subsection{Definizioni Fondamentali}

\begin{definition}[Problema NP-Completo]
Un problema di decisione $L$ è NP-completo se:
\begin{enumerate}
    \item $L \in \text{NP}$ (appartiene alla classe NP).
    \item $L$ è NP-hard (almeno "duro" quanto tutti i problemi in NP), ovvero ogni problema $L' \in \text{NP}$ è riducibile a $L$ in tempo polinomiale ($L' \le_P L$).
\end{enumerate}
\end{definition}

\begin{definition}[Problema NP-Hard]
Un problema $L$ è NP-hard se ogni problema $L' \in \text{NP}$ è riducibile a $L$ in tempo polinomiale ($L' \le_P L$).
\end{definition}

È importante sottolineare che le classi P e NP, e i concetti di NP-hard e NP-completo, si applicano esclusivamente ai \textbf{problemi di decisione}.

\begin{example}
\textbf{Problema di Somma:}
\begin{itemize}
    \item \textbf{Problema di calcolo:} "Sommare due numeri". Questo non è un problema di decisione e quindi non rientra nelle classi P o NP.
    \item \textbf{Problema di decisione:} "Dati tre numeri $a, b, c$, è vero che $c = a+b$?" Questo è un problema di decisione e, poiché risolvibile in tempo polinomiale, rientra nella classe P.
\end{itemize}
\end{example}

Oggi analizzeremo due problemi NP-completi: \emph{Exact Cover} e \emph{Knapsack}.

\section{Exact Cover}

\begin{definition}[Exact Cover]
\textbf{Input:}
Una coppia $(U, F)$ dove:
\begin{itemize}
    \item $U = \{u_1, u_2, \ldots, u_N\}$ è un insieme finito di oggetti, chiamato \emph{universo}.
    \item $F = \{S_1, S_2, \ldots, S_M\}$ è una famiglia di sottoinsiemi di $U$, ovvero $S_j \subseteq U$ per ogni $j \in \{1, \ldots, M\}$.
\end{itemize}
\textbf{Domanda:} Esiste un sottoinsieme $F' \subseteq F$ tale che gli insiemi in $F'$ formano una \emph{partizione} di $U$?
Una partizione di $U$ significa che gli insiemi in $F'$ sono a due a due disgiunti (cioè $S_a \cap S_b = \emptyset$ per ogni $S_a, S_b \in F', a \neq b$) e la loro unione è uguale a $U$ (cioè $\bigcup_{S \in F'} S = U$).
\end{definition}

\subsection{Membership in NP}
Exact Cover è in NP. Per dimostrarlo, è sufficiente mostrare che data un'istanza YES, possiamo verificare la soluzione in tempo polinomiale.
\begin{itemize}
    \item \textbf{Guess (Indovina):} Un certificato per un'istanza YES di Exact Cover è il sottoinsieme $F' \subseteq F$ che si afferma essere la partizione.
    \item \textbf{Check (Verifica):} Data $F'$, possiamo verificare in tempo polinomiale se:
        \begin{enumerate}
            \item Tutti gli insiemi in $F'$ sono a due a due disgiunti.
            \item L'unione di tutti gli insiemi in $F'$ è uguale all'universo $U$.
        \end{enumerate}
    Queste verifiche possono essere eseguite in tempo polinomiale rispetto alla dimensione dell'input.
\end{itemize}

\subsection{Hardness (Riduzione da 3-SAT)}
Dimostriamo che Exact Cover è NP-hard riducendolo da 3-SAT (che sappiamo essere NP-completo).
Sia $\phi$ un'istanza di 3-SAT. $\phi$ è una formula booleana in forma normale congiuntiva (CNF), composta da $L$ clausole $C_1, \ldots, C_L$, dove ogni clausola $C_j$ contiene esattamente 3 letterali: $C_j = (\lambda_{j,1} \lor \lambda_{j,2} \lor \lambda_{j,3})$. Sia $N$ il numero di variabili in $\phi$.
Dobbiamo costruire una funzione polinomiale $f$ che trasforma $\phi$ in un'istanza $(U_\phi, F_\phi)$ di Exact Cover tale che $\phi$ è soddisfacibile se e solo se $(U_\phi, F_\phi)$ è un'istanza YES di Exact Cover.

\subsubsection{Costruzione dell'Istanza $(U_\phi, F_\phi)$}
\paragraph{1. Universo $U_\phi$:}
L'universo $U_\phi$ è costruito per rappresentare le variabili, le clausole e i letterali della formula $\phi$.
$U_\phi = \{\text{var}_1, \ldots, \text{var}_N\} \cup \{c_1, \ldots, c_L\} \cup \{l_{j,k} \mid j \in [1, L], k \in [1, 3]\}$
\begin{itemize}
    \item $\text{var}_i$: Un oggetto per ogni variabile $x_i$ in $\phi$.
    \item $c_j$: Un oggetto per ogni clausola $C_j$ in $\phi$.
    \item $l_{j,k}$: Un oggetto per ogni letterale $\lambda_{j,k}$ in $\phi$.
\end{itemize}

\paragraph{2. Famiglia di Sottoinsiemi $F_\phi$:}
La famiglia $F_\phi$ contiene diversi tipi di insiemi, progettati per simulare l'assegnazione di verità e la soddisfazione delle clausole.
\begin{enumerate}
    \item \textbf{Insiemi di assegnamento variabile (Type 1):} Per ogni variabile $x_i$ ($i \in [1, N]$), creiamo due insiemi:
    \begin{itemize}
        \item $T_i^{true} = \{\text{var}_i\} \cup \{l_{j,k} \mid \text{il letterale } \lambda_{j,k} \text{ è } \neg x_i\}$
        (Questo insieme "copre" l'oggetto $\text{var}_i$ e tutti gli oggetti $l_{j,k}$ corrispondenti a letterali che diventerebbero falsi se $x_i$ fosse assegnata a TRUE).
        \item $T_i^{false} = \{\text{var}_i\} \cup \{l_{j,k} \mid \text{il letterale } \lambda_{j,k} \text{ è } x_i\}$
        (Questo insieme "copre" l'oggetto $\text{var}_i$ e tutti gli oggetti $l_{j,k}$ corrispondenti a letterali che diventerebbero falsi se $x_i$ fosse assegnata a FALSE).
    \end{itemize}
    \item \textbf{Insiemi di soddisfazione clausola (Type 2):} Per ogni clausola $C_j = (\lambda_{j,1} \lor \lambda_{j,2} \lor \lambda_{j,3})$ ($j \in [1, L]$), creiamo tre insiemi:
    \begin{itemize}
        \item $S_{j,1} = \{c_j, l_{j,1}\}$
        \item $S_{j,2} = \{c_j, l_{j,2}\}$
        \item $S_{j,3} = \{c_j, l_{j,3}\}$
    \end{itemize}
    (Questi insiemi rappresentano la soddisfazione della clausola $C_j$ tramite uno dei suoi letterali. Un solo insieme di questo tipo può essere scelto per ogni $c_j$ nella partizione).
    \item \textbf{Insiemi di pulizia letterale (Type 3):} Per ogni oggetto letterale $l_{j,k}$ in $U_\phi$, creiamo un insieme singleton:
    \begin{itemize}
        \item $\{l_{j,k}\}$
    \end{itemize}
    (Questi insiemi sono usati per "raccogliere le briciole", cioè per coprire gli oggetti $l_{j,k}$ che non sono stati coperti dagli insiemi di Tipo 1 o Tipo 2 selezionati per la partizione. Sono usati come elementi di "riserva").
\end{enumerate}

\begin{example}[Costruzione di $(U_\phi, F_\phi)$]
Sia $\phi = (x_1 \lor \neg x_2 \lor x_3) \land (\neg x_1 \lor x_2 \lor x_4)$.
Qui $N=4$ (variabili $x_1, \ldots, x_4$) e $L=2$ (clausole $C_1, C_2$).

\textbf{Universo $U_\phi$:}
\begin{itemize}
    \item Variabili: $\{\text{var}_1, \text{var}_2, \text{var}_3, \text{var}_4\}$
    \item Clausole: $\{c_1, c_2\}$
    \item Letterali: $\{l_{1,1}, l_{1,2}, l_{1,3}, l_{2,1}, l_{2,2}, l_{2,3}\}$ (corrispondenti a $x_1, \neg x_2, x_3$ per $C_1$ e $\neg x_1, x_2, x_4$ per $C_2$).
\end{itemize}
Quindi, $U_\phi = \{\text{var}_1, \text{var}_2, \text{var}_3, \text{var}_4, c_1, c_2, l_{1,1}, l_{1,2}, l_{1,3}, l_{2,1}, l_{2,2}, l_{2,3}\}$.

\textbf{Famiglia $F_\phi$:}
\begin{itemize}
    \item \textbf{Tipo 1 (assegnamento variabile):}
    \begin{itemize}
        \item $T_1^{true} = \{\text{var}_1\} \cup \{l_{2,1} \text{ (per } \neg x_1 \text{ in } C_2)\}$
        \item $T_1^{false} = \{\text{var}_1\} \cup \{l_{1,1} \text{ (per } x_1 \text{ in } C_1)\}$
        \item $T_2^{true} = \{\text{var}_2\} \cup \{l_{1,2} \text{ (per } \neg x_2 \text{ in } C_1)\}$
        \item $T_2^{false} = \{\text{var}_2\} \cup \{l_{2,2} \text{ (per } x_2 \text{ in } C_2)\}$
        \item $T_3^{true} = \{\text{var}_3\} \cup \emptyset$
        \item $T_3^{false} = \{\text{var}_3\} \cup \{l_{1,3} \text{ (per } x_3 \text{ in } C_1)\}$
        \item $T_4^{true} = \{\text{var}_4\} \cup \emptyset$
        \item $T_4^{false} = \{\text{var}_4\} \cup \{l_{2,3} \text{ (per } x_4 \text{ in } C_2)\}$
    \end{itemize}
    \item \textbf{Tipo 2 (soddisfazione clausola):}
    \begin{itemize}
        \item $S_{1,1} = \{c_1, l_{1,1}\}$
        \item $S_{1,2} = \{c_1, l_{1,2}\}$
        \item $S_{1,3} = \{c_1, l_{1,3}\}$
        \item $S_{2,1} = \{c_2, l_{2,1}\}$
        \item $S_{2,2} = \{c_2, l_{2,2}\}$
        \item $S_{2,3} = \{c_2, l_{2,3}\}$
    \end{itemize}
    \item \textbf{Tipo 3 (pulizia letterale):}
    \begin{itemize}
        \item $\{l_{1,1}\}, \{l_{1,2}\}, \{l_{1,3}\}, \{l_{2,1}\}, \{l_{2,2}\}, \{l_{2,3}\}$
    \end{itemize}
\end{itemize}
\end{example}

\subsubsection{Dimostrazione dell'Equivalenza}

\begin{theorem}
La formula $\phi$ è soddisfacibile se e solo se l'istanza di Exact Cover $(U_\phi, F_\phi)$ ha una partizione.
\end{theorem}
\begin{proof}
\textbf{Parte 1: Se $\phi$ è soddisfacibile $\implies (U_\phi, F_\phi)$ ha una partizione.}
Supponiamo che $\phi$ sia soddisfacibile. Allora esiste un assegnamento di verità $\sigma: \{x_1, \ldots, x_N\} \to \{\text{TRUE}, \text{FALSE}\}$ tale che $\phi$ è TRUE. Costruiamo una partizione $P \subseteq F_\phi$ come segue:
\begin{enumerate}
    \item Per ogni variabile $x_i$:
    \begin{itemize}
        \item Se $\sigma(x_i) = \text{TRUE}$, includiamo $T_i^{true}$ in $P$.
        \item Se $\sigma(x_i) = \text{FALSE}$, includiamo $T_i^{false}$ in $P$.
    \end{itemize}
    Questi insiemi coprono tutti gli oggetti $\text{var}_i$ esattamente una volta. Inoltre, coprono alcuni oggetti $l_{j,k}$ (quelli che vengono falsificati dall'assegnamento $\sigma$).
    \item Per ogni clausola $C_j$:
    Poiché $\phi$ è soddisfacibile, $C_j$ è soddisfatta da $\sigma$. Ciò significa che almeno uno dei letterali in $C_j$ è TRUE sotto $\sigma$. Scegliamo esattamente uno di questi letterali $\lambda_{j,k'}$ che rende $C_j$ vera, e includiamo l'insieme $S_{j,k'} = \{c_j, l_{j,k'}\}$ in $P$.
    Questi insiemi coprono tutti gli oggetti $c_j$ esattamente una volta. Essi coprono anche un oggetto $l_{j,k'}$ per ogni clausola. Questi $l_{j,k'}$ sono oggetti che corrispondono a letterali veri sotto $\sigma$, e quindi non sono stati coperti dagli insiemi di Tipo 1.
    \item Per tutti gli oggetti $l_{j,k}$ rimanenti (quelli non coperti dagli insiemi di Tipo 1 o Tipo 2), includiamo il loro singleton $\{l_{j,k}\}$ in $P$.
\end{enumerate}
Verifichiamo che $P$ è una partizione:
\begin{itemize}
    \item \textbf{Disgiunzione:}
    \begin{itemize}
        \item Gli insiemi di Tipo 1 (assegnamento variabile) sono disgiunti tra loro per lo stesso $\text{var}_i$ perché solo uno $T_i^{true}$ o $T_i^{false}$ è scelto. Per $\text{var}_i \ne \text{var}_k$, essi non condividono $\text{var}$ oggetti. Possono condividere oggetti $l_{j,k}$, ma non è un problema per la disgiunzione complessiva perché l'insieme selezionato sarà un sottoinsieme della partizione finale.
        \item Gli insiemi di Tipo 2 (soddisfazione clausola) sono disgiunti tra loro per la stessa $c_j$ perché solo uno $S_{j,k'}$ è scelto. Per $c_j \ne c_k$, non condividono $c$ oggetti. Possono condividere $l$ oggetti, ma questo è gestito dal processo di selezione e dalla gestione dei singleton.
        \item Gli insiemi di Tipo 1 e Tipo 2 non condividono oggetti $\text{var}_i$ o $c_j$. Possono condividere oggetti $l_{j,k}$. Per costruzione, gli $l_{j,k}$ in $T_i^{true}$ o $T_i^{false}$ sono quelli corrispondenti a letterali falsi sotto $\sigma$. Gli $l_{j,k'}$ in $S_{j,k'}$ sono quelli corrispondenti a letterali veri sotto $\sigma$. Dunque, un oggetto $l_{j,k}$ non può essere presente sia in un $T_i$ selezionato che in un $S_j$ selezionato.
        \item I singleton $\{l_{j,k}\}$ sono usati solo per coprire gli $l_{j,k}$ che non sono stati inclusi in nessun $T_i$ o $S_j$ selezionato.
    \end{itemize}
    \item \textbf{Unione:}
    \begin{itemize}
        \item Gli oggetti $\text{var}_i$ sono coperti esattamente una volta da $T_i^{true}$ o $T_i^{false}$.
        \item Gli oggetti $c_j$ sono coperti esattamente una volta da $S_{j,k'}$ (uno per clausola).
        \item Gli oggetti $l_{j,k}$ sono coperti: se falsificati da $\sigma$, sono inclusi in un $T_i$ selezionato; se rendono vera una clausola $C_j$ e vengono scelti per la partizione, sono inclusi in un $S_{j,k'}$ selezionato; altrimenti, sono coperti dal loro singleton $\{l_{j,k}\}$. Poiché ogni $l_{j,k}$ è o falsificato o vero (e se vero, può essere scelto per una clausola), tutti gli $l_{j,k}$ sono coperti.
    \end{itemize}
    Pertanto, $P$ è una partizione di $U_\phi$.

\textbf{Parte 2: Se $(U_\phi, F_\phi)$ ha una partizione $\implies \phi$ è soddisfacibile.}
Supponiamo che esista una partizione $P \subseteq F_\phi$ di $U_\phi$. Definiamo un assegnamento di verità $\sigma$ per le variabili di $\phi$:
Per ogni variabile $x_i$:
\begin{itemize}
    \item Se $T_i^{true} \in P$, allora $\sigma(x_i) = \text{TRUE}$.
    \item Se $T_i^{false} \in P$, allora $\sigma(x_i) = \text{FALSE}$.
\end{itemize}
\textbf{Verifica di $\sigma$:}
\begin{itemize}
    \item \textbf{Ogni variabile ha un assegnamento:} Ogni oggetto $\text{var}_i \in U_\phi$ deve essere coperto da un insieme in $P$. L'unico modo per coprire $\text{var}_i$ è includere o $T_i^{true}$ o $T_i^{false}$ in $P$. Poiché $P$ è una partizione, non possono essere entrambi in $P$ (condividerebbero $\text{var}_i$), quindi esattamente uno è in $P$. Ciò assicura che $\sigma$ assegna un valore a ogni variabile.
    \item \textbf{Assegnamento consistente:} Poiché solo uno tra $T_i^{true}$ e $T_i^{false}$ può essere in $P$, $\sigma$ non assegna contemporaneamente TRUE e FALSE a nessuna variabile.
\end{itemize}
\textbf{Soddisfazione di $\phi$:}
Per dimostrare che $\sigma$ soddisfa $\phi$, dobbiamo mostrare che ogni clausola $C_j$ è TRUE sotto $\sigma$.
Per ogni oggetto $c_j \in U_\phi$, esso deve essere coperto da un insieme in $P$. Gli unici insiemi in $F_\phi$ che contengono $c_j$ sono $S_{j,1}, S_{j,2}, S_{j,3}$. Dunque, esattamente uno di questi insiemi, diciamo $S_{j,k'} = \{c_j, l_{j,k'}\}$, deve essere in $P$.
Questo significa che il letterale $\lambda_{j,k'}$ corrispondente a $l_{j,k'}$ deve essere TRUE sotto l'assegnamento $\sigma$. Se $\lambda_{j,k'}$ fosse FALSE sotto $\sigma$, allora l'oggetto $l_{j,k'}$ sarebbe incluso in uno degli insiemi $T_i^{true}$ o $T_i^{false}$ selezionati (poiché $l_{j,k'}$ è falsificato). Ma se $l_{j,k'}$ fosse in un $T_i$ selezionato e anche in $S_{j,k'}$ (che è in $P$), ci sarebbe un'intersezione tra $T_i$ e $S_{j,k'}$, violando la proprietà di disgiunzione di una partizione.
Pertanto, $\lambda_{j,k'}$ deve essere TRUE sotto $\sigma$, il che significa che la clausola $C_j$ è soddisfatta. Poiché questo vale per ogni $C_j$, $\phi$ è soddisfacibile.
\end{itemize}
\end{proof}

Conclusione: Exact Cover è NP-completo.

\section{Knapsack (Problema della Bisaccia)}

\subsection{Definizione dell'ottimizzazione e della decisione}

\begin{definition}[Knapsack (Versione Ottimizzazione)]
\textbf{Input:}
Un insieme di $N$ oggetti, per ognuno dei quali:
\begin{itemize}
    \item $w_i$: un peso associato.
    \item $v_i$: un valore associato.
\end{itemize}
Una capacità massima $W$ (peso della bisaccia).
\textbf{Domanda:} Trovare un sottoinsieme di oggetti $S \subseteq \{1, \ldots, N\}$ tale che la somma dei pesi degli oggetti in $S$ non superi $W$ ($\sum_{i \in S} w_i \le W$) e la somma dei valori degli oggetti in $S$ sia massimizzata ($\sum_{i \in S} v_i$ sia massima).
\end{definition}

La versione di ottimizzazione di Knapsack non è direttamente in NP, poiché NP è una classe di problemi di decisione. Per studiarlo nella teoria della complessità, usiamo la versione di decisione:

\begin{definition}[Knapsack (Versione Decisione)]
\textbf{Input:}
Un insieme di $N$ oggetti, per ognuno dei quali:
\begin{itemize}
    \item $w_i$: un peso associato.
    \item $v_i$: un valore associato.
\end{itemize}
Una capacità massima $W$ (peso della bisaccia) e un valore soglia $K$.
\textbf{Domanda:} Esiste un sottoinsieme di oggetti $S \subseteq \{1, \ldots, N\}$ tale che la somma dei pesi degli oggetti in $S$ non superi $W$ ($\sum_{i \in S} w_i \le W$) e la somma dei valori degli oggetti in $S$ sia almeno $K$ ($\sum_{i \in S} v_i \ge K$)?
\end{definition}

\subsection{Membership in NP (Versione Decisione)}
Knapsack (versione decisione) è in NP.
\begin{itemize}
    \item \textbf{Guess (Indovina):} Un certificato per un'istanza YES di Knapsack è il sottoinsieme $S \subseteq \{1, \ldots, N\}$ degli oggetti scelti.
    \item \textbf{Check (Verifica):} Dati $S$, possiamo verificare in tempo polinomiale se:
        \begin{enumerate}
            \item La somma dei pesi $\sum_{i \in S} w_i \le W$.
            \item La somma dei valori $\sum_{i \in S} v_i \ge K$.
        \end{enumerate}
    Queste verifiche sono semplici sommatorie e confronti, eseguibili in tempo polinomiale.
\end{itemize}

\subsection{Hardness (Riduzione da Exact Cover)}
Dimostriamo che Knapsack (decisione) è NP-hard riducendolo da Exact Cover.
Sia $(U, F)$ un'istanza di Exact Cover, con $U = \{u_1, \ldots, u_N\}$ e $F = \{S_1, \ldots, S_M\}$.
Dobbiamo costruire una funzione polinomiale $f$ che trasforma $(U, F)$ in un'istanza di Knapsack (oggetti, pesi $w_i$, valori $v_i$, capacità $W$, soglia $K$) tale che $(U, F)$ è un'istanza YES di Exact Cover se e solo se l'istanza di Knapsack generata è un'istanza YES.

\subsubsection{Costruzione dell'Istanza di Knapsack}
La riduzione sfrutta una particolare forma dell'istanza di Knapsack.
\paragraph{1. Assunzione Specifiche:}
Costruiremo un'istanza di Knapsack in cui:
\begin{itemize}
    \item I valori sono uguali ai pesi: $v_i = w_i$ per ogni oggetto $i$.
    \item La soglia $K$ è uguale alla capacità $W$: $K = W$.
\end{itemize}
Con queste assunzioni, il problema di decisione del Knapsack diventa: "Esiste un sottoinsieme di oggetti $S$ tale che $\sum_{i \in S} w_i = W$?" (poiché $\sum w_i \le W$ e $\sum v_i \ge K$ con $v_i=w_i$ e $K=W$ implica $\sum w_i \ge W$, quindi uguaglianza).

\paragraph{2. Oggetti della Bisaccia:}
Ci sono $M$ oggetti nella bisaccia, uno per ogni insieme $S_j \in F$.

\paragraph{3. Pesi ($w_j$) e Valori ($v_j$):}
Per ogni oggetto $j$ (corrispondente al set $S_j \in F$), definiamo il suo peso $w_j$ (e valore $v_j=w_j$) in modo che codifichi la composizione del set $S_j$.
Per evitare i problemi di "riporto" che si avrebbero con la rappresentazione binaria standard, useremo una base numerica sufficientemente grande, in particolare $M+1$ (dove $M$ è il numero totale di insiemi in $F$). Questo assicura che la somma di $M$ cifre 0 o 1 in qualsiasi posizione non genererà un riporto.

Il peso $w_j$ (e valore $v_j$) per l'oggetto $j$ (che rappresenta l'insieme $S_j \in F$) è definito come:
\[ w_j = \sum_{k=1}^N \delta_{j,k} \cdot (M+1)^{N-k} \]
dove:
\begin{itemize}
    \item $N$ è il numero di elementi nell'universo $U$.
    \item $M$ è il numero di insiemi nella famiglia $F$.
    \item $\delta_{j,k} = 1$ se l'elemento $u_k \in U$ è contenuto nell'insieme $S_j$ (cioè $u_k \in S_j$).
    \item $\delta_{j,k} = 0$ se l'elemento $u_k \in U$ non è contenuto nell'insieme $S_j$ (cioè $u_k \notin S_j$).
\end{itemize}
Questa formula interpreta $w_j$ come un numero in base $(M+1)$, dove la $k$-esima cifra (da sinistra, corrispondente a $u_k$) è 1 se $u_k \in S_j$ e 0 altrimenti.

\paragraph{4. Capacità $W$ e Soglia $K$:}
La capacità della bisaccia $W$ (e la soglia $K$) è definita come il numero la cui rappresentazione in base $(M+1)$ è composta da tutti '1'. Questo significa che ogni elemento dell'universo deve essere coperto esattamente una volta.
\[ W = K = \sum_{k=1}^N 1 \cdot (M+1)^{N-k} \]

\begin{example}[Knapsack Reduction (Base $M+1$)]
Sia $U = \{u_1, u_2, u_3, u_4\}$ ($N=4$) e $F = \{S_1, S_2, S_3\}$ ($M=3$).
\begin{itemize}
    \item $S_1 = \{u_3, u_4\}$
    \item $S_2 = \{u_2, u_4\}$
    \item $S_3 = \{u_2, u_3, u_4\}$
\end{itemize}
La base sarà $M+1 = 3+1 = 4$.
I pesi $w_j$ (e valori $v_j$) sono:
\begin{itemize}
    \item $w_1 = (0011)_4 = 0 \cdot 4^3 + 0 \cdot 4^2 + 1 \cdot 4^1 + 1 \cdot 4^0 = 4+1 = 5$
    \item $w_2 = (0101)_4 = 0 \cdot 4^3 + 1 \cdot 4^2 + 0 \cdot 4^1 + 1 \cdot 4^0 = 16+1 = 17$
    \item $w_3 = (0111)_4 = 0 \cdot 4^3 + 1 \cdot 4^2 + 1 \cdot 4^1 + 1 \cdot 4^0 = 16+4+1 = 21$
\end{itemize}
La capacità e soglia $W=K$ sono:
\begin{itemize}
    \item $W = K = (1111)_4 = 1 \cdot 4^3 + 1 \cdot 4^2 + 1 \cdot 4^1 + 1 \cdot 4^0 = 64+16+4+1 = 85$
\end{itemize}
In questo esempio, l'istanza di Exact Cover è NO (l'elemento $u_1$ non è coperto da nessun set in $F$). Se la riduzione funziona correttamente, l'istanza di Knapsack generata dovrebbe essere anch'essa NO (non dovrebbe essere possibile sommare $w_j$ a 85).
\end{example}

\subsubsection{Dimostrazione dell'Equivalenza}

\begin{theorem}
L'istanza di Exact Cover $(U, F)$ ha una partizione se e solo se l'istanza di Knapsack costruita ha una soluzione con peso totale $W$ e valore totale $K$.
\end{theorem}
\begin{proof}
Ricordiamo che, per costruzione, il problema Knapsack si riduce a trovare un sottoinsieme di pesi che sommi esattamente a $W$, poiché $v_j=w_j$ e $K=W$.

\textbf{Parte 1: Se $(U, F)$ ha una partizione $\implies$ l'istanza di Knapsack ha una soluzione.}
Supponiamo che esista un sottoinsieme $F' \subseteq F$ tale che $F'$ è una partizione di $U$. Questo significa che:
\begin{enumerate}
    \item Gli insiemi in $F'$ sono a due a due disgiunti: $S_a \cap S_b = \emptyset$ per ogni $S_a, S_b \in F', a \neq b$.
    \item La loro unione è $U$: $\bigcup_{S \in F'} S = U$.
\end{enumerate}
Consideriamo il sottoinsieme di oggetti Knapsack corrispondente agli insiemi in $F'$. Siano questi oggetti $S'_{items} = \{j \mid S_j \in F'\}$.
Calcoliamo la somma dei pesi di questi oggetti: $\sum_{j \in S'_{items}} w_j$.
Per la definizione di $w_j$ e le proprietà di $F'$:
\[ \sum_{j \in S'_{items}} w_j = \sum_{j \in S'_{items}} \left( \sum_{k=1}^N \delta_{j,k} \cdot (M+1)^{N-k} \right) \]
Invertendo l'ordine delle sommatorie:
\[ \sum_{k=1}^N \left( \sum_{j \in S'_{items}} \delta_{j,k} \right) \cdot (M+1)^{N-k} \]
Poiché $F'$ è una partizione di $U$:
\begin{itemize}
    \item Ogni elemento $u_k \in U$ appartiene a \emph{esattamente uno} degli insiemi in $F'$. Questo significa che per ogni $k \in \{1, \ldots, N\}$, la somma $\sum_{j \in S'_{items}} \delta_{j,k}$ sarà esattamente 1. (Non può essere più di 1 perché i set sono disgiunti; non può essere meno di 1 perché la loro unione è $U$).
\end{itemize}
Quindi, la somma dei pesi diventa:
\[ \sum_{k=1}^N 1 \cdot (M+1)^{N-k} \]
Questa è esattamente la definizione di $W$.
\[ \sum_{j \in S'_{items}} w_j = W \]
Poiché per costruzione $v_j=w_j$ e $K=W$, anche $\sum_{j \in S'_{items}} v_j = K$.
Dunque, se $(U, F)$ ha una partizione, l'istanza di Knapsack ha una soluzione.

\textbf{Parte 2: Se l'istanza di Knapsack ha una soluzione $\implies (U, F)$ ha una partizione.}
Supponiamo che esista un sottoinsieme di oggetti Knapsack $S'_{items}$ tale che $\sum_{j \in S'_{items}} w_j = W$.
Consideriamo la somma $\sum_{j \in S'_{items}} w_j$ in base $(M+1)$. Ogni $w_j$ è un numero in base $(M+1)$ le cui cifre sono 0 o 1.
La somma delle cifre in una data posizione $k$ (corrispondente all'elemento $u_k \in U$) è data da $\sum_{j \in S'_{items}} \delta_{j,k}$. Poiché al massimo $M$ numeri sono sommati (corrispondenti agli $M$ insiemi in $F$), e la base è $(M+1)$, non ci saranno mai riporti. Questo è cruciale: la somma di $M$ cifre (0 o 1) in base $(M+1)$ non può generare un riporto. La somma massima possibile per una colonna è $M \times 1 = M$, che è una cifra valida in base $(M+1)$ (da 0 a $M$).
Dato che la somma totale è $W = \sum_{k=1}^N 1 \cdot (M+1)^{N-k}$ (il numero con tutte le cifre 1 in base $M+1$) e non ci sono riporti, questo implica che per ogni posizione $k$, la somma delle cifre in quella posizione deve essere esattamente 1.
\[ \forall k \in \{1, \ldots, N\}, \quad \sum_{j \in S'_{items}} \delta_{j,k} = 1 \]
Questa condizione significa che ogni elemento $u_k \in U$ è contenuto in esattamente uno degli insiemi $S_j$ il cui oggetto corrispondente è stato selezionato per la bisaccia.
Pertanto, gli insiemi $\{S_j \mid j \in S'_{items}\}$ formano una partizione di $U$.
Dunque, se l'istanza di Knapsack ha una soluzione, $(U, F)$ ha una partizione.
\end{proof}

\subsection{Conclusione}
Avendo dimostrato che Knapsack (decisione) è in NP e NP-hard (tramite riduzione da Exact Cover), concludiamo che \textbf{Knapsack è NP-completo}.
Il problema Knapsack può essere formulato come un problema di \textbf{Programmazione Lineare Intera (ILP)}. Poiché Knapsack è NP-completo, e ILP è un problema più generale che include Knapsack come caso speciale, si deduce che la \textbf{Programmazione Lineare Intera è NP-hard}.


% =====================================================
% --- START LECTURE 22 ---
% =====================================================

\chapter{Il Teorema di Cook}



\section{Introduzione e Richiami}

La lezione odierna chiude la parte dedicata alla classe di complessità \textbf{NP}, concentrandosi sul Teorema di Cook, fondamentale per la comprensione della teoria della NP-completezza.

\subsection{Richiami sulle Classi di Complessità}

\begin{definition}[Classe NP]
La classe \textbf{NP} (Nondeterministic Polynomial time) include tutti i problemi di decisione (linguaggi) che possono essere decisi da una \emph{Macchina di Turing Non Deterministiche (NDTM)} in tempo polinomiale.
\end{definition}

\begin{definition}[NP-Hard]
Un linguaggio $L$ è \textbf{NP-Hard} se per ogni linguaggio $L'$ appartenente a NP, $L'$ si riduce polinomialmente a $L$ ($L' \le_P L$). Intuitivamente, i problemi NP-Hard sono \textit{almeno difficili quanto} tutti i problemi in NP.
\end{definition}

\begin{definition}[NP-Complete]
Un linguaggio $L$ è \textbf{NP-Complete} se $L$ è sia in NP che NP-Hard.
\end{definition}

\subsection{Il Problema della Dimostrazione Iniziale}
Fino ad ora, per dimostrare che un problema $B$ è NP-Hard, abbiamo usato la \emph{transitività delle riduzioni polinomiali}:
\begin{theorem}[Proprietà di Transitività]
Se $A$ è NP-Hard e $A \le_P B$, allora $B$ è NP-Hard.
\end{theorem}
Questo metodo ci ha permesso di dimostrare che molti problemi (e.g., Knapsack, Exact Cover, 3-SAT, Vertex Cover, Click) sono NP-Complete o NP-Hard, partendo da un problema già noto come NP-Hard.
Tuttavia, questo approccio si basa sull'assunzione che esista almeno un problema NP-Hard conosciuto. Come abbiamo dimostrato che SAT è NP-Hard? In realtà, non l'abbiamo ancora fatto. Tutte le nostre dimostrazioni precedenti si fondano su questa assunzione.

Per dimostrare che \textbf{SAT} è NP-Hard, non possiamo usare la transitività, poiché non abbiamo un problema NP-Hard di partenza. Dobbiamo usare la definizione originale di NP-Hardness: mostrare che \emph{ogni linguaggio $L$ in NP si riduce polinomialmente a SAT}.
Il problema è che ci sono infiniti linguaggi in NP, quindi non possiamo fornire una riduzione specifica per ognuno di essi. È necessario un approccio generico. Questo è il contenuto del \textbf{Teorema di Cook}.

\section{Teorema di Cook: SAT è NP-Completo}

\begin{theorem}[Teorema di Cook]
Il problema SAT (Boolean Satisfiability Problem) è NP-Complete.
\end{theorem}

Per dimostrare che SAT è NP-Complete, dobbiamo mostrare due cose:
\begin{enumerate}
    \item SAT è in NP. (Questo è già noto: una soluzione (assegnamento di verità) può essere verificata in tempo polinomiale).
    \item SAT è NP-Hard. (Questo è l'obiettivo della lezione: mostrare che $L \le_P \text{SAT}$ per ogni $L \in \text{NP}$).
\end{enumerate}

\subsection{La Strategia della Riduzione}
L'idea chiave della dimostrazione di NP-Hardness di SAT è costruire una riduzione polinomiale da un \emph{generico} linguaggio $L \in \text{NP}$ a SAT.
Poiché $L \in \text{NP}$, per definizione, esiste una \emph{Macchina di Turing Non Deterministica (NDTM)} $M$ che decide $L$ in tempo polinomiale. Sia $P(n)$ il polinomio che definisce il tempo di esecuzione di $M$ per un input di lunghezza $n$.

La riduzione $f$ prenderà in input una stringa $W$ (istanza generica di $L$) e produrrà una \emph{formula booleana} $F(W)$ in forma congiuntiva normale (CNF). La formula $F(W)$ sarà soddisfacibile se e solo se la macchina $M$ accetta la stringa $W$.

\[ W \in L \iff M \text{ accetta } W \iff F(W) \text{ è soddisfacibile} \]

La funzione di riduzione $f$ (che costruisce $F(W)$) dovrà essere calcolabile in tempo polinomiale.

\subsubsection{Assunzioni semplificative sulla Macchina di Turing $M$}
Per facilitare la costruzione della formula $F(W)$, facciamo alcune assunzioni semplificative su $M$:
\begin{itemize}
    \item Il nastro di $M$ è \textbf{semi-infinito} (parte dalla cella 0, non va a sinistra di 0). Qualsiasi NDTM può essere convertita in una equivalente con nastro semi-infinito.
    \item $M$ esegue \textbf{esattamente $P(n)$ passi} per ogni ramo di computazione. Se un ramo termina prima (accetta o rifiuta), $M$ continua a ciclare in uno stato finale fittizio per riempire i restanti passi fino a $P(n)$. Questo semplifica la gestione del tempo.
    \item $M$ non scrive mai il simbolo "blank" (si può assumere che utilizzi un simbolo speciale diverso dal blank per marcare celle vuote o usate).
    \item Il numero di stati $R$ di $M$ e il numero di simboli dell'alfabeto del nastro $\Gamma$ sono costanti e indipendenti dalla lunghezza dell'input $n$.
\end{itemize}
Dato che $M$ compie al più $P(n)$ passi, l'unica parte del nastro che può essere letta o scritta è quella compresa tra la cella 0 e la cella $P(n)$ (inclusa).

\subsection{Variabili Proposizionali per la Simulazione}
Per simulare il comportamento di $M$ su $W$ all'interno di una formula booleana, definiamo un insieme di variabili proposizionali. Queste variabili rappresenteranno lo stato di $M$ in ogni istante di tempo e su ogni cella del nastro.
Sia $N_{max} = P(n)$ il numero massimo di passi e celle rilevanti.

\begin{enumerate}[label=\alph*)]
    \item \textbf{$Q_{i,k}$} (Stato): Variabile booleana vera se e solo se la macchina $M$ al passo $i$ si trova nello stato $q_k$.
    \begin{itemize}
        \item $i \in [0, N_{max}]$ (passi di computazione)
        \item $k \in [0, R-1]$ (indici degli stati, dove $R$ è il numero totale di stati di $M$)
        \item Numero di variabili: $O(N_{max} \cdot R) = O(P(n))$, che è polinomiale.
    \end{itemize}
    \item \textbf{$H_{i,j}$} (Head - Testina): Variabile booleana vera se e solo se la testina di $M$ al passo $i$ si trova sulla cella $j$ del nastro.
    \begin{itemize}
        \item $i \in [0, N_{max}]$
        \item $j \in [0, N_{max}]$ (indici delle celle del nastro)
        \item Numero di variabili: $O(N_{max}^2) = O(P(n)^2)$, che è polinomiale.
    \end{itemize}
    \item \textbf{$T_{i,j,l}$} (Tape - Nastro): Variabile booleana vera se e solo se al passo $i$ la cella $j$ del nastro contiene il simbolo $\alpha_l$.
    \begin{itemize}
        \item $i \in [0, N_{max}]$
        \item $j \in [0, N_{max}]$
        \item $l \in [0, |\Gamma|-1]$ (indici dei simboli dell'alfabeto del nastro $\Gamma$)
        \item Numero di variabili: $O(N_{max}^2 \cdot |\Gamma|) = O(P(n)^2)$, che è polinomiale.
    \end{itemize}
\end{enumerate}
Il numero totale di variabili proposizionali è polinomiale rispetto alla lunghezza dell'input $n$.

\subsection{Struttura della Formula $F(W)$}
La formula $F(W)$ sarà una congiunzione di quattro macro-clausole (o gruppi di clausole):
\[ F(W) = C \land S \land N \land F \]
Dove:
\begin{itemize}
    \item $C$ (Consistency): Assicura che l'assegnamento di verità alle variabili descriva una configurazione valida della macchina a ogni passo.
    \item $S$ (Start): Codifica la configurazione iniziale della macchina (stato, posizione testina, contenuto del nastro).
    \item $N$ (Next Step): Codifica come la configurazione della macchina evolve da un passo all'altro, seguendo la funzione di transizione di $M$.
    \item $F$ (Finish): Codifica che la macchina raggiunge uno stato di accettazione all'ultimo passo.
\end{itemize}

Prima di definire le macro-clausole, ripassiamo alcune proprietà utili per convertire le implicazioni in CNF:
\begin{itemize}
    \item \textbf{Implicazione}: $A \implies B$ è equivalente a $\neg A \lor B$.
    \item \textbf{Distributività}: $A \land (B \lor C)$ è equivalente a $(A \land B) \lor (A \land C)$.
    \item \textbf{De Morgan}: $\neg (A \land B)$ è equivalente a $\neg A \lor \neg B$.
    \item \textbf{Implicazione annidata}: $A \implies (B \land C)$ è equivalente a $(A \implies B) \land (A \implies C)$.
    \item \textbf{Implicazione annidata (OR)}: $A \implies (B \lor C)$ è equivalente a $(\neg A \lor B \lor C)$.
\end{itemize}

\begin{example}[Esempio di Riformulazione]
Consideriamo la formula: $A \land B \implies (C \land D) \lor (E \land F)$
\begin{enumerate}
    \item Applichiamo la distributività sulla parte destra dell'OR:
    $A \land B \implies (C \lor E) \land (C \lor F) \land (D \lor E) \land (D \lor F)$
    \item Applichiamo l'implicazione annidata: $X \implies Y \land Z$ è $(X \implies Y) \land (X \implies Z)$.
    Questo si traduce in una congiunzione di 4 implicazioni:
    $(A \land B \implies C \lor E) \land$ \\
    $(A \land B \implies C \lor F) \land$ \\
    $(A \land B \implies D \lor E) \land$ \\
    $(A \land B \implies D \lor F)$
    \item Ogni implicazione $X \implies Y$ può essere riscritta come $\neg X \lor Y$:
    $(\neg (A \land B) \lor C \lor E) \land$ \\
    $(\neg (A \land B) \lor C \lor F) \land$ \\
    $(\neg (A \land B) \lor D \lor E) \land$ \\
    $(\neg (A \land B) \lor D \lor F)$
    \item Infine, applichiamo De Morgan: $\neg (A \land B)$ è $\neg A \lor \neg B$:
    $(\neg A \lor \neg B \lor C \lor E) \land$ \\
    $(\neg A \lor \neg B \lor C \lor F) \land$ \\
    $(\neg A \lor \neg B \lor D \lor E) \land$ \\
    $(\neg A \lor \neg B \lor D \lor F)$
\end{enumerate}
Questa forma finale è una formula in CNF, composta da clausole di 4 letterali.
\end{example}

\subsubsection{1. Consistenza (C)}
La clausola $C$ impone vincoli affinché l'assegnamento di verità rappresenti una configurazione coerente della macchina di Turing ad ogni passo.

\begin{itemize}
    \item \textbf{Unicità dello stato per passo}: Ad ogni passo $i$, la macchina si trova in \emph{al più un} stato.
    \[ \bigwedge_{i=0}^{N_{max}} \bigwedge_{k=0}^{R-1} \bigwedge_{k'=k+1}^{R-1} (\neg Q_{i,k} \lor \neg Q_{i,k'}) \]
    Questo è equivalente a $Q_{i,k} \implies \neg Q_{i,k'}$ (se è nello stato $k$, non può essere nello stato $k'$).
    \item \textbf{Unicità della posizione della testina per passo}: Ad ogni passo $i$, la testina si trova in \emph{al più una} cella.
    \[ \bigwedge_{i=0}^{N_{max}} \bigwedge_{j=0}^{N_{max}} \bigwedge_{j'=j+1}^{N_{max}} (\neg H_{i,j} \lor \neg H_{i,j'}) \]
    Questo è equivalente a $H_{i,j} \implies \neg H_{i,j'}$ (se la testina è in $j$, non può essere in $j'$).
    \item \textbf{Unicità del simbolo per cella per passo}: Ad ogni passo $i$ e per ogni cella $j$, la cella contiene \emph{al più un} simbolo.
    \[ \bigwedge_{i=0}^{N_{max}} \bigwedge_{j=0}^{N_{max}} \bigwedge_{l=0}^{|\Gamma|-1} \bigwedge_{l'=l+1}^{|\Gamma|-1} (\neg T_{i,j,l} \lor \neg T_{i,j,l'}) \]
    Questo è equivalente a $T_{i,j,l} \implies \neg T_{i,j,l'}$ (se la cella $j$ contiene $\alpha_l$, non può contenere $\alpha_{l'}$).
\end{itemize}
Il numero totale di clausole in $C$ è polinomiale. Queste clausole assicurano che l'assegnamento di verità sia internamente consistente.

\subsubsection{2. Inizio (S)}
La clausola $S$ impone la configurazione iniziale della macchina $M$ all'istante $i=0$.
Assumiamo $q_0$ sia lo stato iniziale di $M$. La stringa input è $W = w_0 w_1 \dots w_{n-1}$.
\[ S = Q_{0,q_0} \land H_{0,0} \land \left( \bigwedge_{j=0}^{n-1} T_{0,j,w_j} \right) \land \left( \bigwedge_{j=n}^{N_{max}} T_{0,j,blank} \right) \]
\begin{itemize}
    \item $Q_{0,q_0}$: La macchina è nello stato iniziale $q_0$ al passo 0.
    \item $H_{0,0}$: La testina è sulla cella 0 al passo 0.
    \item $T_{0,j,w_j}$ per $j \in [0, n-1]$: Le prime $n$ celle del nastro contengono l'input $W$.
    \item $T_{0,j,blank}$ per $j \in [n, N_{max}]$: Tutte le celle successive sono blank.
\end{itemize}
Anche $S$ è composta da un numero polinomiale di clausole (tutte unitarie).

\subsubsection{3. Prossimo Passo (N)}
La clausola $N$ codifica il comportamento passo-passo della macchina. È divisa in tre sottocategorie:

\paragraph{3a. Inerzia ($N_I$): Celle distanti dalla testina}
Le celle del nastro che non sono sotto la testina al passo $i$ non cambiano il loro contenuto al passo $i+1$.
\[ N_I = \bigwedge_{i=0}^{N_{max}-1} \bigwedge_{j=0}^{N_{max}} \bigwedge_{l=0}^{|\Gamma|-1} ( (\neg H_{i,j} \land T_{i,j,l}) \implies T_{i+1,j,l} ) \]
Questa implicazione, convertita in CNF, diventa $\neg(\neg H_{i,j} \land T_{i,j,l}) \lor T_{i+1,j,l}$, che è equivalente a $(H_{i,j} \lor \neg T_{i,j,l} \lor T_{i+1,j,l})$.

\paragraph{3b. Transizione di stato/testina/simbolo ($N_H$): Cella sotto la testina}
Le celle sotto la testina cambiano il loro contenuto e la posizione della testina si sposta, in base alla funzione di transizione $\delta$ di $M$. Per ogni $i \in [0, N_{max}-1]$, $j \in [0, N_{max}]$ e per ogni $k \in [0, R-1]$, $l \in [0, |\Gamma|-1]$:
Se al passo $i$, la macchina è nello stato $q_k$, la testina è in posizione $j$, e la cella $j$ contiene $\alpha_l$, allora devono essere vere le condizioni sul prossimo passo ($i+1$) dettate dalla funzione di transizione.

Per ogni tupla $(q_k, \alpha_l)$, sia $\delta(q_k, \alpha_l) = \{ (q_{k_1}, \alpha_{l_1}, D_1), (q_{k_2}, \alpha_{l_2}, D_2), \dots \}$ dove $D_x \in \{L, R, S\}$ (Left, Right, Stay).
\begin{align*}
N_H = \bigwedge_{i=0}^{N_{max}-1} \bigwedge_{j=0}^{N_{max}} \bigwedge_{k=0}^{R-1} \bigwedge_{l=0}^{|\Gamma|-1} & ( (Q_{i,k} \land H_{i,j} \land T_{i,j,l}) \implies \\
& \bigvee_{(q_{k'}, \alpha_{l'}, D) \in \delta(q_k, \alpha_l)} (Q_{i+1,k'} \land T_{i+1,j,l'} \land H_{i+1,j'}) )
\end{align*}
Dove $j'$ è la nuova posizione della testina basata su $D$:
\begin{itemize}
    \item Se $D = R$, allora $j' = j+1$.
    \item Se $D = L$, allora $j' = j-1$.
    \item Se $D = S$, allora $j' = j$.
\end{itemize}
(Occorre gestire i casi limite di $j=0$ per $D=L$ e $j=N_{max}$ per $D=R$, assicurando che la testina non esca dai limiti del nastro rilevante o che tali transizioni non siano ammesse).
Questa formula, pur complessa, è polinomiale in dimensione e può essere trasformata in CNF.

\paragraph{3c. Padding ($N_P$): Ripetizione degli stati finali/bloccanti}
Questa clausola assicura che tutti i rami di computazione abbiano lunghezza $P(n)$, ripetendo la configurazione finale (accettante o bloccante) fino al passo $P(n)$.
\begin{itemize}
    \item \textbf{Se M entra in uno stato finale accettante $q_{acc}$}:
    \[ \bigwedge_{i=0}^{N_{max}-1} \bigwedge_{j=0}^{N_{max}} \bigwedge_{l=0}^{|\Gamma|-1} ( (Q_{i,q_{acc}} \land H_{i,j} \land T_{i,j,l}) \implies (Q_{i+1,q_{acc}} \land H_{i+1,j} \land T_{i+1,j,l}) ) \]
    Questa clausola afferma che se al passo $i$ la macchina è nello stato accettante $q_{acc}$, con testina in $j$ e simbolo $\alpha_l$, allora al passo $i+1$ la macchina rimane nello stesso stato, la testina non si muove e il contenuto del nastro rimane invariato.
    \item \textbf{Se M si blocca (nessuna transizione definita)}:
    Per ogni stato $q_k$ e simbolo $\alpha_l$ tale che $\delta(q_k, \alpha_l) = \emptyset$:
    \[ \bigwedge_{i=0}^{N_{max}-1} \bigwedge_{j=0}^{N_{max}} \bigwedge_{k \text{ s.t. } \delta(q_k, \alpha_l)=\emptyset} \bigwedge_{l=0}^{|\Gamma|-1} ( (Q_{i,k} \land H_{i,j} \land T_{i,j,l}) \implies (Q_{i+1,k} \land H_{i+1,j} \land T_{i+1,j,l}) ) \]
    Questa clausola assicura che se la macchina si trova in una configurazione in cui non sono definite transizioni, essa "stalla", ripetendo la stessa configurazione nel passo successivo.
\end{itemize}
Queste clausole garantiscono che tutte le computazioni, sia quelle accettanti che quelle bloccanti, vengano estese fino alla lunghezza $P(n)$.

\subsubsection{4. Finale (F)}
La clausola $F$ impone che la macchina $M$ debba terminare in uno stato accettante all'ultimo passo $N_{max} = P(n)$.
Assumiamo $q_{acc}$ sia lo stato accettante finale.
\[ F = Q_{N_{max}, q_{acc}} \]
Questa è una singola clausola unitaria, che forza l'assegnamento di verità a $Q_{P(n),q_{acc}}$ ad essere vero.

\section{Conclusione della Dimostrazione}

La formula completa $F(W) = C \land S \land N \land F$ ha le seguenti proprietà:
\begin{enumerate}
    \item \textbf{Dimensione Polinomiale}: Tutte le parti di $F(W)$ sono congiunzioni di un numero polinomiale di clausole, e ogni clausola ha una dimensione costante. Pertanto, la dimensione totale di $F(W)$ è polinomiale rispetto alla lunghezza dell'input $n$. Questo significa che la funzione di riduzione $f$ è calcolabile in tempo polinomiale.
    \item \textbf{Forma CNF}: Come dimostrato nell'esempio e per la natura delle implicazioni utilizzate, $F(W)$ può essere espressa in forma congiuntiva normale.
    \item \textbf{Correttezza}:
    \begin{itemize}
        \item Se $W \in L$: Per definizione di NP, esiste almeno un ramo di computazione accettante di $M$ su $W$ che termina entro $P(n)$ passi. Un assegnamento di verità che descrive fedelmente questo ramo di computazione renderà $F(W)$ vera, quindi $F(W)$ è soddisfacibile.
        \item Se $W \notin L$: Nessun ramo di computazione di $M$ su $W$ accetta. Qualsiasi assegnamento di verità che tenti di rendere $F(W)$ vera dovrà violare una delle clausole (es. una transizione non valida in $N$, o non raggiungendo $q_{acc}$ in $F$), rendendo $F(W)$ insoddisfacibile.
    \end{itemize}
\end{enumerate}

Quindi, abbiamo dimostrato che per ogni linguaggio $L \in \text{NP}$, $L \le_P \text{SAT}$. Per definizione, questo implica che SAT è \textbf{NP-Hard}.
Poiché SAT è anche in NP (come ricordato all'inizio), ne consegue che \textbf{SAT è NP-Complete}.

Questo risultato è di fondamentale importanza, in quanto stabilisce SAT come il "problema prototipo" della classe NP-Complete. Molti altri problemi NP-Complete vengono dimostrati tali attraverso riduzioni da SAT.
Inoltre, esistono programmi chiamati "SAT solvers" che cercano di determinare la soddisfacibilità di formule booleane. La capacità di ridurre problemi NP-Complete a SAT significa che questi SAT solvers possono essere usati per risolvere istanze di un'ampia varietà di problemi difficili, trasformandoli in problemi di soddisfacibilità booleana.


% =====================================================
% --- START LECTURE 23 ---
% =====================================================

\chapter{Classi di Complessità Co-NP, EXP, NEXP}



\section{Introduzione alle Classi di Complessità}

Riprendiamo il concetto di classe \textbf{NP}. Ricordiamo che un problema di decisione $L$ appartiene alla classe NP se esiste una Macchina di Turing non deterministica (NTM) che decide $L$ in tempo polinomiale. Intuitivamente, le istanze "sì" di un problema in NP ammettono un "certificato" conciso (polinomiale) che può essere verificato in tempo polinomiale da una Macchina di Turing deterministica.

\subsection{Problema UNSAT}

Consideriamo il problema \textbf{UNSAT}:
Data una formula booleana $f$ in forma normale congiuntiva (CNF), stabilire se $f$ \textit{non è soddisfacibile}.

\begin{definition}[Linguaggio UNSAT]
Il linguaggio UNSAT è l'insieme delle stringhe che codificano formule booleane in CNF che non sono soddisfacibili.
\[
\text{UNSAT} = \{ \langle f \rangle \mid f \text{ è una formula in CNF non soddisfacibile} \}
\]
Una formula non soddisfacibile è una formula per la quale non esiste alcuna assegnazione di verità alle variabili che la renda vera.
\end{definition}

\textbf{Domanda:} UNSAT appartiene alla classe NP?
\textbf{Risposta intuitiva:} No. Per rispondere "sì" (la formula non è soddisfacibile), dovremmo essere sicuri che \textit{nessuna} possibile assegnazione di verità la soddisfa. Non possiamo semplicemente "indovinare" un'assegnazione e verificare che non la soddisfa, perché un'assegnazione che non soddisfa la formula non prova che la formula sia non soddisfacibile; ne servirebbe una che la soddisfa per dire "no".

Se una macchina non deterministica "indovinasse" un'assegnazione di verità per le variabili di $f$, verificasse che tale assegnazione \textit{non} soddisfa $f$, e poi rispondesse "sì", tale macchina deciderebbe in realtà il complemento del linguaggio delle tautologie (formule sempre vere).

\section{La Classe Co-NP}

Introduciamo una nuova classe di complessità, \textbf{Co-NP}, che formalizza il tipo di problemi come UNSAT.

\begin{definition}[Classe Co-NP]
La classe Co-NP è l'insieme dei linguaggi $L$ tali che il loro complemento $\overline{L}$ appartiene alla classe NP.
\[
\text{Co-NP} = \{ L \mid \overline{L} \in \text{NP} \}
\]
\end{definition}

\textbf{Esempio 1: UNSAT}
UNSAT appartiene a Co-NP, poiché il suo complemento $\overline{\text{UNSAT}}$ è il problema SAT (Satisfiability), e SAT è un problema NP-completo, quindi appartiene a NP.

\textbf{Importante:} Co-NP \textit{non} è il complemento di NP. Il complemento di NP conterrebbe tutti i problemi fuori da NP, inclusi problemi indecidibili, problemi non ricorsivamente enumerabili, ecc. Co-NP è invece una classe ben definita di problemi decidibili.

\subsection{Intuizione per Co-NP}
In modo speculare all'intuizione per NP:
\begin{itemize}
    \item Un linguaggio $L$ è in \textbf{NP} se le sue istanze "sì" possono essere verificate efficientemente tramite un certificato conciso.
    \item Un linguaggio $L$ è in \textbf{Co-NP} se le sue istanze "no" possono essere verificate efficientemente tramite un certificato conciso.
\end{itemize}
Per i problemi in Co-NP, siamo in grado di rispondere "no" in modo efficiente, fornendo un "certificato di rifiuto".

\begin{example}[UNSAT - Intuizione Co-NP]
Per decidere UNSAT, se la formula $f$ \textit{non è} non soddisfacibile (cioè è soddisfacibile), possiamo fornire un certificato: un'assegnazione di verità che soddisfa $f$. Questo certificato permette di rispondere "no" in tempo polinomiale.
\end{example}

\begin{example}[TAUTOLOGY]
Il problema TAUTOLOGY è decidere se una data formula booleana in CNF è una tautologia (cioè sempre vera per ogni assegnazione di verità).
TAUTOLOGY appartiene a Co-NP. Per rispondere "no" a una formula che \textit{non è} una tautologia, basta fornire un'assegnazione di verità che la renda falsa. Questa verifica è polinomiale.
\end{example}

\section{Relazioni tra P, NP e Co-NP}

Mentre per le classi R e Co-R sappiamo che sono distinte e che la loro intersezione è R ($R \cap \text{Co-R} = R$), per P, NP e Co-NP la situazione è molto meno chiara. La maggior parte delle relazioni sono problemi aperti.

\begin{itemize}
    \item Non sappiamo se $\text{NP} = \text{Co-NP}$ o se $\text{NP} \neq \text{Co-NP}$. Questa è una questione aperta. L'ipotesi corrente è che siano distinte.
    \item Non sappiamo se $\text{P} = \text{NP} \cap \text{Co-NP}$. Sappiamo che $\text{P} \subseteq \text{NP} \cap \text{Co-NP}$.
\end{itemize}

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    scale=0.8,
    cirlce/.style={draw, circle, minimum size=2cm},
    set/.style={circle, draw, minimum size=4cm, fill=blue!10},
    subset/.style={circle, draw, minimum size=2cm, fill=red!10},
    label/.style={font=\bfseries}
]
    % Disegnare gli insiemi principali
    \node[set, label={NP}] (NP) {};
    \node[set, label={Co-NP}] (CoNP) at (2,0) {}; % Spostato per sovrapposizione

    % Aggiungere il sottoinsieme P
    \node[subset, label={P}] (P) at (1,0) {}; % Posizionato nell'intersezione

    % Etichette
    \node at (NP.center) {NP};
    \node at (CoNP.center) {Co-NP};
    \node at (P.center) {P};

    % Linee per indicare l'intersezione e inclusione
    \draw[dashed] (NP.north west) -- (NP.south east); % Rappresentazione schematica
    \draw[dashed] (CoNP.north east) -- (CoNP.south west); % Rappresentazione schematica

    % Rappresentazione di P come sottoinsieme dell'intersezione
    \node[below of=P, yshift=1cm, align=center] {Relazione ipotizzata: \\ $P \subseteq \text{NP} \cap \text{Co-NP}$};
\end{tikzpicture}
\caption{Relazioni ipotizzate tra P, NP e Co-NP.}
\end{figure}

\begin{theorem}[Relazione NP e Co-NP]
$\text{NP} = \text{Co-NP}$ se e solo se esiste un linguaggio $L$ NP-completo tale che $L \in \text{Co-NP}$.
\end{theorem}

\begin{proof}
\textbf{Parte 1: Se $\text{NP} = \text{Co-NP}$ allora esiste $L$ NP-completo tale che $L \in \text{Co-NP}$.}
Se $\text{NP} = \text{Co-NP}$, allora qualsiasi linguaggio $L$ NP-completo (che per definizione appartiene a NP) apparterrà anche a Co-NP. Questa direzione è banale.

\textbf{Parte 2: Se esiste $L$ NP-completo tale che $L \in \text{Co-NP}$ allora $\text{NP} = \text{Co-NP}$.}
Per dimostrare che $\text{NP} = \text{Co-NP}$, dobbiamo mostrare due inclusioni: $\text{NP} \subseteq \text{Co-NP}$ e $\text{Co-NP} \subseteq \text{NP}$.

\begin{enumerate}
    \item \textbf{Dimostriamo $\text{NP} \subseteq \text{Co-NP}$:}
    Sia $L'$ un linguaggio qualsiasi in NP. Il nostro obiettivo è mostrare che $L' \in \text{Co-NP}$, il che significa $\overline{L'} \in \text{NP}$.
    \begin{itemize}
        \item Poiché $L' \in \text{NP}$ e $L$ è NP-completo per ipotesi, sappiamo che $L'$ si riduce polinomialmente a $L$ (cioè $L' \le_P L$). Questo significa che esiste una funzione di trasformazione $f$ calcolabile in tempo polinomiale tale che per ogni stringa $w$:
        $w \in L' \iff f(w) \in L$
        \item Questa equivalenza può essere riscritta in termini di complementi:
        $w \notin L' \iff f(w) \notin L$
        Ciò implica:
        $w \in \overline{L'} \iff f(w) \in \overline{L}$
        Questa relazione mostra che $\overline{L'}$ si riduce polinomialmente a $\overline{L}$ (cioè $\overline{L'} \le_P \overline{L}$).
        \item Per ipotesi, $L \in \text{Co-NP}$. Per definizione di Co-NP, questo significa che $\overline{L} \in \text{NP}$.
        \item Poiché $\overline{L'}$ si riduce a $\overline{L}$ (che è in NP), e la classe NP è chiusa rispetto a riduzioni polinomiali (se $A \le_P B$ e $B \in \text{NP}$ allora $A \in \text{NP}$), allora $\overline{L'} \in \text{NP}$.
        \item Dato che $\overline{L'} \in \text{NP}$, per definizione di Co-NP, $L' \in \text{Co-NP}$.
    \end{itemize}
    Abbiamo quindi dimostrato che qualsiasi $L' \in \text{NP}$ è anche in $\text{Co-NP}$, da cui $\text{NP} \subseteq \text{Co-NP}$.

    \item \textbf{Dimostriamo $\text{Co-NP} \subseteq \text{NP}$:}
    Sia $L'$ un linguaggio qualsiasi in Co-NP. Il nostro obiettivo è mostrare che $L' \in \text{NP}$.
    \begin{itemize}
        \item Poiché $L' \in \text{Co-NP}$, per definizione, $\overline{L'} \in \text{NP}$.
        \item Per ipotesi, $L$ è un linguaggio NP-completo. Poiché $\overline{L'} \in \text{NP}$ e $L$ è NP-completo, sappiamo che $\overline{L'}$ si riduce polinomialmente a $L$ (cioè $\overline{L'} \le_P L$).
        \item Similmente al punto precedente, questa riduzione implica che $L'$ si riduce polinomialmente a $\overline{L}$ (cioè $L' \le_P \overline{L}$).
        \item Per ipotesi, $L \in \text{Co-NP}$, il che significa $\overline{L} \in \text{NP}$.
        \item Poiché $L'$ si riduce a $\overline{L}$ (che è in NP), e NP è chiusa rispetto a riduzioni polinomiali, allora $L' \in \text{NP}$.
    \end{itemize}
    Abbiamo quindi dimostrato che qualsiasi $L' \in \text{Co-NP}$ è anche in $\text{NP}$, da cui $\text{Co-NP} \subseteq \text{NP}$.
\end{enumerate}
Dato che $\text{NP} \subseteq \text{Co-NP}$ e $\text{Co-NP} \subseteq \text{NP}$, concludiamo che $\text{NP} = \text{Co-NP}$.
\end{proof}

\subsection{Problemi Co-NP-hard}
Analogamente a NP-hard, definiamo Co-NP-hard.
\begin{definition}[Co-NP-hard]
Un problema $L$ è \textbf{Co-NP-hard} se $\overline{L}$ è NP-hard.
\end{definition}
I problemi Co-NP-hard sono almeno difficili quanto tutti i problemi in Co-NP. UNSAT è un esempio di problema Co-NP-completo.

\section{Il Problema della Fattorizzazione (FACTOR)}

Il problema della fattorizzazione di numeri interi è un problema centrale in crittografia. Sebbene il problema di "produrre la fattorizzazione" non sia un problema di decisione, possiamo definirne una versione decisionale.

\begin{definition}[Linguaggio FACTOR]
Il linguaggio FACTOR è l'insieme delle coppie $\langle n, k \rangle$ tali che $n$ è un intero naturale e $n$ ha almeno un fattore primo $p$ tale che $p \le k$.
\[
\text{FACTOR} = \{ \langle n, k \rangle \mid n \in \mathbb{N}, \exists p \in \mathbb{P} \text{ t.c. } p \le k \text{ e } p \text{ divide } n \}
\]
\end{definition}

\begin{example}
\begin{itemize}
    \item $\langle 175, 6 \rangle \in \text{FACTOR}$ perché $175 = 5 \cdot 5 \cdot 7$. Il fattore primo $5 \le 6$, e $5$ divide $175$.
    \item $\langle 175, 4 \rangle \notin \text{FACTOR}$ perché nessuno dei fattori primi di $175$ ($5, 7$) è minore o uguale a $4$.
\end{itemize}
\end{example}

\begin{theorem}
$\text{FACTOR} \in \text{NP} \cap \text{Co-NP}$.
\end{theorem}

\begin{proof}
\textbf{Parte 1: $\text{FACTOR} \in \text{NP}$}
Per dimostrare che FACTOR è in NP, dobbiamo mostrare che esiste una NTM che lo decide in tempo polinomiale.
\begin{itemize}
    \item \textbf{Guess:} La NTM "indovina" un numero primo candidato $p$. Dato che $p \le k$ (e $k \le n$), $p$ non può essere "troppo grande" rispetto alla dimensione dell'input $\langle n, k \rangle$, quindi il guess ha una dimensione polinomiale rispetto all'input.
    \item \textbf{Verifica (Check):}
    \begin{enumerate}
        \item Verificare che $p \le k$. (Tempo polinomiale).
        \item Verificare che $p$ sia un numero primo. L'algoritmo AKS (Agrawal-Kayal-Saxena, 2003) testa la primalità in tempo polinomiale deterministico.
        \item Verificare che $p$ divida $n$ (cioè $n \pmod p = 0$). La divisione tra interi può essere eseguita in tempo polinomiale rispetto alla dimensione (numero di bit) degli operandi.
    \end{enumerate}
    Tutti i passaggi di verifica sono polinomiali, quindi $\text{FACTOR} \in \text{NP}$.
\end{itemize}

\textbf{Parte 2: $\text{FACTOR} \in \text{Co-NP}$}
Per dimostrare che FACTOR è in Co-NP, dobbiamo mostrare che il suo complemento $\overline{\text{FACTOR}}$ è in NP.

\begin{definition}[Linguaggio $\overline{\text{FACTOR}}$]
$\overline{\text{FACTOR}}$ è l'insieme delle coppie $\langle n, k \rangle$ tali che $n$ è un intero naturale e \textit{tutti} i fattori primi $p$ di $n$ sono maggiori di $k$.
\[
\overline{\text{FACTOR}} = \{ \langle n, k \rangle \mid n \in \mathbb{N}, \forall p \in \mathbb{P} \text{ t.c. } p \text{ divide } n \implies p > k \}
\]
\end{definition}

Per dimostrare che $\overline{\text{FACTOR}} \in \text{NP}$:
\begin{itemize}
    \item \textbf{Guess:} La NTM "indovina" l'intera fattorizzazione prima di $n$: $p_1, p_2, \dots, p_m$. Il numero di fattori $m$ è al più logaritmico rispetto a $n$ (poiché $2^m \le n \implies m \le \log_2 n$). Quindi la dimensione del guess è polinomiale nella dimensione dell'input.
    \item \textbf{Verifica (Check):}
    \begin{enumerate}
        \item Verificare che $p_i > k$ per ogni $i=1, \dots, m$. (Tempo polinomiale).
        \item Verificare che ogni $p_i$ sia un numero primo (con l'algoritmo AKS). Questo richiede $m$ test di primalità, per un costo totale polinomiale.
        \item Verificare che il prodotto $p_1 \cdot p_2 \cdot \dots \cdot p_m$ sia uguale a $n$. La moltiplicazione di $m$ numeri può essere eseguita in tempo polinomiale.
    \end{enumerate}
    Tutti i passaggi di verifica sono polinomiali, quindi $\overline{\text{FACTOR}} \in \text{NP}$.
\end{itemize}
Poiché $\overline{\text{FACTOR}} \in \text{NP}$, per definizione, $\text{FACTOR} \in \text{Co-NP}$.

Combinando le due parti, $\text{FACTOR} \in \text{NP} \cap \text{Co-NP}$.
\end{proof}

\subsection{Implicazioni del posizionamento di FACTOR}
Il problema della fattorizzazione è fondamentale per la crittografia (e.g., RSA si basa sulla sua presunta difficoltà).
\begin{itemize}
    \item $\text{FACTOR} \notin \text{P}$ (ipotesi): Nessuno è ancora riuscito a trovare un algoritmo polinomiale deterministico per la fattorizzazione. Se lo fosse, molti schemi crittografici sarebbero compromessi.
    \item $\text{FACTOR} \notin \text{NP-completo}$ (ipotesi): Poiché $\text{FACTOR} \in \text{NP} \cap \text{Co-NP}$, se fosse NP-completo, implicherebbe $\text{NP} = \text{Co-NP}$ (per il teorema precedente). Dato che si ipotizza $\text{NP} \neq \text{Co-NP}$, si ipotizza anche che $\text{FACTOR}$ non sia NP-completo.
\end{itemize}
Questo colloca FACTOR in una "terra di mezzo": un problema che si presume non sia in P (non semplice) ma nemmeno NP-completo (non il più difficile in NP).

\textbf{Nota sui computer quantistici:} L'algoritmo di Shor per macchine quantistiche risolve FACTOR in tempo polinomiale. Questo non implica che $\text{P} = \text{NP}$ o che i problemi NP-completi siano risolvibili in tempo polinomiale da macchine quantistiche. L'algoritmo di Shor dimostra che FACTOR non è "intrinsecamente" difficile per tutte le classi di calcolo (come quella quantistica), ma non confuta le congetture sulla difficoltà dei problemi NP-completi per i computer classici.

\section{Classi di Complessità Superiori: EXP e NEXP}

Oltre alle classi P, NP, e Co-NP, esistono classi di complessità che considerano tempi di esecuzione maggiori, in particolare esponenziali.

\subsection{Classe EXP (Exponential Time)}
\begin{definition}[Classe EXP]
La classe \textbf{EXP} (o \textbf{EXPTIME}) è l'insieme dei linguaggi che possono essere decisi da una Macchina di Turing deterministica in tempo esponenziale, ovvero $O(2^{n^c})$ per qualche costante $c \ge 1$.
\[
\text{EXP} = \bigcup_{c \ge 1} \text{DTIME}(2^{n^c})
\]
\end{definition}

\textbf{Relazioni:}
\begin{itemize}
    \item $\text{P} \subseteq \text{EXP}$: Ovvio, poiché un tempo polinomiale è anche un tempo esponenziale.
    \item $\text{P} \neq \text{EXP}$: Questo è un risultato dimostrato dal Teorema della Gerarchia Temporale (Time Hierarchy Theorem). Ci sono problemi risolvibili in tempo esponenziale che non sono risolvibili in tempo polinomiale.
    \item $\text{NP} \subseteq \text{EXP}$: Una Macchina di Turing non deterministica che opera in tempo polinomiale può essere simulata da una Macchina di Turing deterministica in tempo esponenziale (esplorando l'albero di computazione).
    \item $\text{Co-NP} \subseteq \text{EXP}$: Se $L \in \text{Co-NP}$, allora $\overline{L} \in \text{NP}$. Poiché $\text{NP} \subseteq \text{EXP}$, allora $\overline{L} \in \text{EXP}$. Le classi deterministiche come EXP sono chiuse sotto complemento (se un problema è in EXP, anche il suo complemento lo è), quindi $L \in \text{EXP}$.
    \item $\text{NP}$ vs $\text{EXP}$: È un problema aperto se $\text{NP} = \text{EXP}$ o $\text{NP} \neq \text{EXP}$. Si ipotizza che siano distinte.
\end{itemize}

\subsection{Classe NEXP (Non-deterministic Exponential Time)}
\begin{definition}[Classe NEXP]
La classe \textbf{NEXP} (o \textbf{NEXPTIME}) è l'insieme dei linguaggi che possono essere decisi da una Macchina di Turing non deterministica in tempo esponenziale, ovvero $O(2^{n^c})$ per qualche costante $c \ge 1$.
\[
\text{NEXP} = \bigcup_{c \ge 1} \text{NTIME}(2^{n^c})
\]
\end{definition}

\textbf{Relazioni:}
\begin{itemize}
    \item $\text{EXP} \subseteq \text{NEXP}$: Una DTM è un caso speciale di NTM.
    \item $\text{EXP}$ vs $\text{NEXP}$: È un problema aperto se $\text{EXP} = \text{NEXP}$ o $\text{EXP} \neq \text{NEXP}$.
    \item $\text{NP} \subsetneq \text{NEXP}$: Il Teorema della Gerarchia Temporale implica che le classi temporali non deterministiche con differenze esponenziali sono distinte.
\end{itemize}

\subsection{Caratterizzazione di NEXP basata sui Certificati}
Similmente a NP, NEXP può essere caratterizzata in termini di certificati.
\begin{definition}[Caratterizzazione Certificata di NEXP]
Un linguaggio $L$ appartiene a \textbf{NEXP} se le sue istanze "sì" sono caratterizzate da certificati di \textit{taglia esponenziale} (rispetto alla dimensione dell'input) che possono essere verificati da una Macchina di Turing deterministica in \textit{tempo polinomiale} nella taglia combinata dell'input e del certificato.
\end{definition}
Formalmente, per un linguaggio $L$, se $w \in L$, esiste un certificato $C$ tale che:
\begin{itemize}
    \item $|C| = O(2^{|w|^k})$ per qualche $k \ge 1$.
    \item Esiste una DTM che accetta $\langle w, C \rangle$ in tempo $P(|w| + |C|)$ per qualche polinomio $P$.
\end{itemize}
Questa è la ragione per cui la verifica è polinomiale nella taglia combinata: se fosse esponenziale nella taglia del certificato, il costo totale sarebbe doppiamente esponenziale rispetto all'input.

\section{Riepilogo delle Relazioni tra Classi di Complessità}

Le relazioni tra le classi di complessità sono in gran parte ancora problemi aperti, ma ci sono alcune inclusioni e separazioni note o ipotizzate.

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    scale=0.8,
    circle/.style={draw, circle, minimum size=2cm},
    set/.style={draw, minimum size=3cm, ellipse, fill=blue!10},
    subset/.style={draw, minimum size=2cm, ellipse, fill=red!10},
    label/.style={font=\bfseries}
]
    % Disegnare gli insiemi principali come ellissi
    \node[set, minimum size=6cm, label={NEXP}] (NEXP) at (0,0) {};
    \node[set, minimum size=5cm, label={EXP}] (EXP) at (0,0) {};
    \node[set, minimum size=4cm, label={NP}] (NP) at (-1,0) {}; % Spostato a sinistra
    \node[set, minimum size=4cm, label={Co-NP}] (CoNP) at (1,0) {}; % Spostato a destra

    % P nell'intersezione di NP e Co-NP
    \node[subset, minimum size=2.5cm, label={P}] (P) at (0,0) {};

    % Etichette
    \node at (NEXP.north west) {NEXP};
    \node at (EXP.north west) {EXP};
    \node at (NP.north west) {NP};
    \node at (CoNP.north east) {Co-NP};
    \node at (P.center) {P};

    % Illustrare inclusioni (schematico)
    \draw[->, dotted] (P.south) -- +(0,-0.5) node[below] {$P \subseteq \text{NP} \cap \text{Co-NP}$};
    \draw[->, dotted] (NP.south) -- +(0,-0.5) node[below] {$NP \subseteq EXP$};
    \draw[->, dotted] (CoNP.south) -- +(0,-0.5) node[below] {$CoNP \subseteq EXP$};
    \draw[->, dotted] (EXP.south) -- +(0,-0.5) node[below] {$EXP \subseteq NEXP$};

    % Linee per indicare sovrapposizione e aree aperte
    \draw[dashed] (NP.center) -- (CoNP.center); % Intersezione
    \node at (0,-2.5) {\tiny (Relazioni precise incerte, basate su congetture)};
\end{tikzpicture}
\caption{Panoramica delle relazioni tra le classi di complessità (ipotesi correnti).}
\end{figure}

\begin{itemize}
    \item $\text{P} \subseteq \text{NP} \subseteq \text{EXP} \subseteq \text{NEXP}$.
    \item $\text{P} \subseteq \text{NP} \cap \text{Co-NP}$.
    \item $\text{P} \neq \text{EXP}$ (confermato dal Teorema della Gerarchia Temporale).
    \item $\text{NP} \neq \text{NEXP}$ (confermato dal Teorema della Gerarchia Temporale).
    \item Le relazioni $\text{P}$ vs $\text{NP}$, $\text{NP}$ vs $\text{Co-NP}$, $\text{NP}$ vs $\text{EXP}$, $\text{EXP}$ vs $\text{NEXP}$ sono tutte problemi aperti. Le congetture più comuni sono che siano tutte inclusioni strette ($\neq$).
\end{itemize}

Le classi di complessità temporale deterministiche (come P, EXP) sono chiuse sotto complemento, mentre quelle non deterministiche (come NP, NEXP) non si sa con certezza.


% =====================================================
% --- START LECTURE 24 ---
% =====================================================

\chapter{Classi di Complessità Spaziale}



\section{Introduzione alla Complessità Spaziale}

Finora ci siamo concentrati sulla \emph{complessità temporale}, ovvero il tempo impiegato da un algoritmo. Ora introduciamo la \emph{complessità spaziale}, che misura la quantità di memoria necessaria per l'esecuzione di un algoritmo.
Intuitivamente, una macchina con tempo polinomiale è meno potente di una macchina con spazio polinomiale, perché lo spazio può essere riutilizzato, permettendo alla macchina di eseguire più a lungo.
Come vedremo, la classe \textbf{P} (problemi risolvibili in tempo polinomiale) è contenuta in \textbf{PSPACE} (problemi risolvibili in spazio polinomiale). Tuttavia, non sappiamo se \textbf{P} sia strettamente contenuta in \textbf{PSPACE}, un problema analogo a \textbf{P} vs \textbf{NP}.

\subsection{Modello di Macchina di Turing per la Complessità Spaziale}

Per definire la complessità spaziale, in particolare per classi con vincoli di spazio molto ridotti (es. logaritmico), utilizziamo un modello di Macchina di Turing (MT) leggermente modificato:
\begin{itemize}
    \item \textbf{Nastro di Input:} Un nastro di sola lettura. La testina può muoversi in entrambe le direzioni, ma non può modificare il contenuto. Questo nastro contiene l'input $W$.
    \item \textbf{Nastro di Lavoro (Work Tape):} Un nastro infinito di lettura-scrittura. La testina può muoversi in entrambe le direzioni e modificare il contenuto. Questo nastro è usato per i calcoli intermedi e lo spazio utilizzato su questo nastro sarà quello che misureremo.
    \item \textbf{Nastro di Output (solo per trasduttori):} Un nastro di sola scrittura. La testina può muoversi solo in una direzione (generalmente a destra). Questo nastro viene usato per produrre l'output finale.
\end{itemize}
La separazione dei nastri è cruciale per le classi a basso spazio. Se misurassimo lo spazio sull'input tape, qualsiasi algoritmo che legge l'intero input userebbe almeno spazio lineare, rendendo difficile definire classi come LogSpace.

\subsection{Definizioni Fondamentali}

\begin{definition}[Computation Space]
Il \textbf{Computation Space} di una macchina di Turing $M$ su un input $W$, denotato $Space_M(W)$, è il numero di celle \emph{distinte} visitate (anche solo lette, non necessariamente scritte) da $M$ sul suo \emph{nastro di lavoro} durante il processo di $W$.
\begin{itemize}
    \item Se $M$ è \textbf{deterministica}, $Space_M(W)$ è semplicemente il numero di celle distinte visitate.
    \item Se $M$ è \textbf{non deterministica}, $Space_M(W)$ è il \emph{massimo} numero di celle distinte visitate su work tape in qualsiasi dei suoi branch di computazione.
\end{itemize}
\end{definition}

\begin{definition}[Space Function]
Una funzione $S: \mathbb{N} \to \mathbb{N}$ è una \textbf{Space Function} se $S(n)$ è strettamente positiva per ogni $n$ ed è non-decreasing.
\end{definition}

\begin{definition}[Running Space]
Il \textbf{Running Space} di una macchina di Turing $M$ è $S(n)$ se per ogni stringa $W$ (a parte un numero finito di eccezioni), il $Computation Space$ di $M$ su $W$ è bounded da $S(|W|)$, ovvero $Space_M(W) \in O(S(|W|))$.
\end{definition}

\subsection{Classi di Complessità Spaziale}

Similmente alle classi temporali (DTIME, NTIME), definiamo le classi spaziali:

\begin{definition}[DSPACE($S(n)$)]
\textbf{DSPACE($S(n)$)} è l'insieme di tutti i linguaggi $L$ tali che esiste una Macchina di Turing deterministica $M$ che decide $L$ e il cui Running Space è in $O(S(n))$.
\[ \text{DSPACE}(S(n)) = \{ L \mid \exists M \text{ deterministica t.c. } L(M) = L \text{ e } Space_M(n) \in O(S(n)) \} \]
\end{definition}

\begin{definition}[NSPACE($S(n)$)]
\textbf{NSPACE($S(n)$)} è l'insieme di tutti i linguaggi $L$ tali che esiste una Macchina di Turing non deterministica $N$ che decide $L$ e il cui Running Space è in $O(S(n))$.
\[ \text{NSPACE}(S(n)) = \{ L \mid \exists N \text{ non-deterministica t.c. } L(N) = L \text{ e } Space_N(n) \in O(S(n)) \} \]
\end{definition}

\begin{definition}[Classi Logaritmiche]
Definiamo due classi di complessità spaziale fondamentali:
\begin{itemize}
    \item \textbf{L (LogSpace):} La classe di linguaggi decidibili da una MT deterministica in spazio logaritmico.
    \[ \text{L} = \text{DSPACE}(\log n) \]
    \item \textbf{NL (Nondeterministic LogSpace):} La classe di linguaggi decidibili da una MT non deterministica in spazio logaritmico.
    \[ \text{NL} = \text{NSPACE}(\log n) \]
\end{itemize}
\end{definition}

\begin{proposition}
Vale la relazione di contenimento: $\text{L} \subseteq \text{NL}$.
\end{proposition}
Analogamente a P vs NP, non sappiamo se $\text{L} = \text{NL}$ o se $\text{L} \subset \text{NL}$.

\section{Esempi di Problemi nelle Classi Spaziali}

\subsection{Linguaggio $0^n1^n$ in L}

Consideriamo il linguaggio $L_0 = \{0^n1^n \mid n > 0\}$.
Avevamo visto un algoritmo deterministico per questo linguaggio che marcava o cancellava i simboli sull'input tape, ma questo non è permesso con il nuovo modello di MT. Se copiassimo l'input sul work tape, useremmo spazio lineare, quindi $L_0 \notin \text{L}$ con quell'algoritmo.

\begin{example}[Decisore per $0^n1^n$ in LogSpace]
Per decidere $L_0$ in spazio logaritmico, la MT deterministica può fare quanto segue:
\begin{enumerate}
    \item Scorre l'input tape, contando il numero di $0$ e memorizzando il conteggio sul work tape in binario. La rappresentazione binaria di $n$ (numero di $0$s) occupa $O(\log n)$ spazio.
    \item Una volta terminati gli $0$s, riposiziona la testina dell'input tape all'inizio degli $1$s.
    \item Scorre gli $1$s, contando il loro numero e memorizzando il conteggio sul work tape in binario, sovrascrivendo il conteggio precedente.
    \item Confronta i due conteggi binari (uno dopo gli $0$s, uno dopo gli $1$s). Se sono uguali e la stringa è della forma corretta ($0...01...1$), la MT accetta; altrimenti, rigetta.
\end{enumerate}
Lo spazio utilizzato sul work tape è limitato alla memorizzazione di pochi numeri binari (il conteggio, eventuali puntatori, ecc.), ciascuno dei quali occupa $O(\log n)$ spazio. Pertanto, $L_0 \in \text{L}$.
\end{example}

\subsection{Problema della Raggiungibilità (REACHABILITY) in NL}

Il problema \textbf{REACHABILITY} (o \textbf{ST-Connectivity}) è un problema classico in teoria dei grafi.

\begin{definition}[REACHABILITY]
Sia $G=(V,E)$ un grafo orientato, e siano $s, t \in V$ due vertici. Il problema \textbf{REACHABILITY} consiste nel decidere se esiste un cammino (path) da $s$ a $t$ in $G$.
\[ \text{REACHABILITY} = \{ (G, s, t) \mid G \text{ è un grafo orientato e esiste un cammino da } s \text{ a } t \text{ in } G \} \]
\end{definition}

Algoritmi standard come BFS (Breadth-First Search) o DFS (Depth-First Search) risolvono REACHABILITY in tempo polinomiale, ma tipicamente richiedono spazio $O(|V| + |E|)$ o $O(|V|)$ per memorizzare i nodi visitati o le code/stack. Questo è spazio lineare, non logaritmico.

\begin{example}[Algoritmo non deterministico per REACHABILITY in LogSpace]
Sia $M$ una MT non deterministica. Per decidere se esiste un cammino da $s$ a $t$ in $G=(V,E)$, $M$ esegue il seguente algoritmo:

\begin{minted}[
    frame=lines,
    framesep=2mm,
    linenos,
    tabsize=2,
    obeytabs,
    mathescape=true
]{python}
Function NDTM_REACHABILITY(G, s, t):
    # G: grafo, s: nodo sorgente, t: nodo destinazione
    
    current_node = s  # Inizializza il nodo corrente con la sorgente
    path_length_counter = 1 # Contatore della lunghezza del cammino (numero di nodi visitati)
    
    # Se il nodo di partenza è già il target, accetta immediatamente
    if current_node == t:
        ACCEPT
        
    # La MT "guessta" i passi successivi
    while path_length_counter <= |V|:
        # Non deterministicamente, "guessta" un vertice successivo
        # Il guess non richiede di memorizzare tutti i possibili successori,
        # ma solo di selezionarne uno alla volta.
        next_node = GUESS_VERTEX_FROM_V() 
        
        # Verifica se esiste un arco da current_node a next_node in G
        # L'accesso al grafo G avviene tramite il nastro di input, senza occupare spazio sul work tape.
        if (current_node, next_node) in E:
            current_node = next_node
            path_length_counter = path_length_counter + 1
            
            # Se siamo arrivati al target, il cammino è stato trovato
            if current_node == t:
                ACCEPT
            # Altrimenti, continua a gueessare
        else:
            # Se il guess non porta a un arco valido, questo ramo di computazione RIGETTA
            REJECT
    
    # Se il contatore supera |V|, significa che siamo entrati in un ciclo
    # o abbiamo esplorato un cammino troppo lungo senza trovare t.
    # In ogni caso, questo ramo di computazione RIGETTA.
    REJECT
\end{minted}

\textbf{Analisi dello Spazio:}
\begin{itemize}
    \item \texttt{current\_node}: per memorizzare l'identificativo di un nodo in un grafo con $|V|$ nodi, sono necessari $O(\log |V|)$ bit.
    \item \texttt{next\_node}: stessa cosa, $O(\log |V|)$ bit.
    \item \texttt{path\_length\_counter}: un contatore fino a $|V|$, richiede $O(\log |V|)$ bit.
\end{itemize}
In totale, lo spazio utilizzato sul nastro di lavoro è $O(\log |V|)$, che è $O(\log n)$ dove $n$ è la dimensione dell'input (che include la rappresentazione del grafo, quindi $|V|$ è $O(n)$).

\textbf{Correttezza:}
\begin{itemize}
    \item \textbf{Completezza:} Se esiste un cammino da $s$ a $t$ in $G$, allora esiste un ramo di computazione non deterministica che sceglie correttamente i nodi successivi lungo quel cammino e accetta quando raggiunge $t$. Il vincolo $\texttt{path\_length\_counter} \leq |V|$ assicura che anche in presenza di cicli, il cammino non venga esplorato all'infinito e la MT termini. Se un cammino esiste, ne esiste uno senza cicli di lunghezza al più $|V|-1$.
    \item \textbf{Soundness:} Se la MT accetta, significa che ha trovato una sequenza di nodi validi che formano un cammino da $s$ a $t$.
\end{itemize}
Poiché l'algoritmo non deterministico decide correttamente REACHABILITY e usa spazio logaritmico, concludiamo che $\textbf{REACHABILITY} \in \text{NL}$.
\end{example}

\section{Riduzioni LogSpace e NL-Completezza}

Per studiare le relazioni tra le classi di complessità spaziale (in particolare tra L e NL), è necessario definire un tipo di riduzione più restrittivo rispetto alle riduzioni polinomiali usate per NP-Completezza. Le riduzioni polinomiali sono "troppo potenti" per le classi a basso spazio (es. $\text{L} \subseteq \text{P}$). Se un problema in L fosse NL-completo sotto riduzione polinomiale, non si otterrebbe una buona separazione.

\subsection{Trasduttore LogSpace}

Per definire le riduzioni LogSpace, introduciamo il concetto di trasduttore.

\begin{definition}[Trasduttore LogSpace]
Un \textbf{Trasduttore LogSpace} è una Macchina di Turing che calcola una funzione $f: \Sigma^* \to \Gamma^*$ e che soddisfa le seguenti proprietà:
\begin{itemize}
    \item Ha un nastro di input (sola lettura), un nastro di lavoro (lettura-scrittura) e un nastro di output (sola scrittura, unidirezionale).
    \item Il suo spazio di lavoro (sull'work tape) è limitato a $O(\log n)$ dove $n$ è la lunghezza dell'input.
\end{itemize}
\end{definition}
Il nastro di output è di sola scrittura e unidirezionale per assicurarsi che lo spazio speso per l'output non venga conteggiato come spazio di lavoro (non è riutilizzabile per calcoli intermedi) e che il trasduttore non possa usare l'output tape come work tape ausiliario.

\subsection{Riduzione LogSpace}

\begin{definition}[Riduzione LogSpace ($\le_L$)]
Siano $A$ e $B$ due linguaggi. Diciamo che $A$ si riduce in LogSpace a $B$, denotato $A \le_L B$, se esiste una funzione $f: \Sigma^* \to \Gamma^*$ calcolabile da un Trasduttore LogSpace, tale che per ogni stringa $w$:
\[ w \in A \iff f(w) \in B \]
\end{definition}
Le riduzioni LogSpace godono di proprietà di transitività, analoghe alle riduzioni polinomiali.

\subsection{NL-Completezza}

\begin{definition}[NL-Completo]
Un linguaggio $L_{NL}$ è \textbf{NL-Completo} se soddisfa due condizioni:
\begin{enumerate}
    \item \textbf{Appartenenza:} $L_{NL} \in \text{NL}$ (Membership).
    \item \textbf{Durezza (Hardness):} Per ogni linguaggio $L' \in \text{NL}$, si ha $L' \le_L L_{NL}$ (NL-Hardness).
\end{enumerate}
\end{definition}
Analogamente al Teorema di Cook-Levin per NP-completezza, un linguaggio NL-completo è considerato il più difficile nella classe NL. Un risultato importante afferma che:
\begin{theorem}
Se un linguaggio NL-Completo appartiene a \text{L}, allora $\text{L} = \text{NL}$.
\end{theorem}

\subsection{REACHABILITY è NL-Completo}

\begin{theorem}[NL-Completezza di REACHABILITY]
Il problema \textbf{REACHABILITY} è NL-Completo.
\end{theorem}
\begin{proof}
Dobbiamo dimostrare due cose:
\begin{enumerate}
    \item \textbf{REACHABILITY $\in$ NL:} Già dimostrato con l'algoritmo non deterministico in LogSpace.
    \item \textbf{REACHABILITY è NL-Hard:} Dobbiamo mostrare che per ogni linguaggio $L' \in \text{NL}$, si ha $L' \le_L \text{REACHABILITY}$.
\end{enumerate}

Sia $L'$ un linguaggio arbitrario in $\text{NL}$. Per definizione, esiste una Macchina di Turing non deterministica $M$ che decide $L'$ usando $O(\log n)$ spazio sul suo nastro di lavoro, dove $n$ è la lunghezza dell'input.
Per dimostrare che $L' \le_L \text{REACHABILITY}$, dobbiamo costruire una funzione $f$ (calcolabile da un trasduttore LogSpace) che prende in input una stringa $w$ e produce un'istanza $(G_w, s, t)$ di REACHABILITY, tale che $w \in L' \iff (G_w, s, t) \in \text{REACHABILITY}$.

L'idea chiave è rappresentare le computazioni di $M$ su $w$ come cammini in un grafo. I nodi del grafo $G_w$ saranno le \emph{configurazioni istantanee} (IDs) di $M$.

\textbf{Configurazione di una MT M:} Una configurazione istantanea (ID) di una MT $M$ in un dato momento della computazione è definita dallo stato corrente di $M$, dal contenuto del nastro di lavoro e dalla posizione di entrambe le testine (input e lavoro).
\begin{itemize}
    \item Stato corrente $q \in Q$ (insieme degli stati di $M$). Numero di stati è costante.
    \item Posizione testina input $h_1 \in \{1, \dots, |w|\}$. Richiede $O(\log |w|)$ bit.
    \item Posizione testina lavoro $h_2 \in \{1, \dots, S(|w|)\}$. Poiché $S(|w|) \in O(\log |w|)$, $h_2$ richiede $O(\log(\log |w|))$ bit.
    \item Contenuto nastro di lavoro $C \in \Gamma^{S(|w|)}$ (simboli dell'alfabeto del nastro). Poiché $S(|w|) \in O(\log |w|)$, il contenuto richiede $O(\log |w|)$ simboli.
\end{itemize}
Una configurazione può essere rappresentata da una tupla $(q, h_1, h_2, C)$. La lunghezza di questa tupla (come stringa binaria) è $O(\log |w|)$.
Il numero totale di configurazioni possibili è esponenziale in $O(\log |w|)$, il che è polinomiale in $|w|$ (es. $2^{c \log n} = n^c$).

\textbf{Costruzione del Grafo $G_w = (V,E)$:}
\begin{itemize}
    \item \textbf{Vertici $V$:} L'insieme dei vertici di $G_w$ è l'insieme di tutte le possibili configurazioni di $M$ su $w$.
    \item \textbf{Archi $E$:} Esiste un arco diretto da una configurazione $C_i$ a una configurazione $C_j$ se e solo se $M$ può passare da $C_i$ a $C_j$ in un singolo passo di computazione (secondo la funzione di transizione di $M$). Poiché $M$ è non deterministica, da una configurazione possono partire più archi.
    \item \textbf{Nodi speciali:}
        \begin{itemize}
            \item $s_{start}$: Rappresenta la configurazione iniziale di $M$ su $w$ (stato iniziale $q_0$, testine all'inizio, nastro di lavoro vuoto).
            \item $t_{final}$: Un nodo speciale aggiunto al grafo, non corrispondente a una configurazione di $M$. Tutti gli archi che provengono da configurazioni accettanti di $M$ (quelle in cui $M$ entra in uno stato di accettazione) puntano a $t_{final}$.
        \end{itemize}
\end{itemize}

\textbf{Funzionamento del Trasduttore $f$ (la riduzione LogSpace):}
Il trasduttore $f$ deve generare $(G_w, s_{start}, t_{final})$ sull'output tape usando solo spazio logaritmico sul work tape. Non può costruire l'intero grafo in memoria, dato che $G_w$ può avere un numero polinomiale di nodi e archi.
\begin{enumerate}
    \item \textbf{Generazione di $s_{start}$ e $t_{final}$:} Questi sono facili da generare, rappresentano configurazioni specifiche o un simbolo speciale. Richiedono spazio costante o logaritmico.
    \item \textbf{Generazione dei vertici di $G_w$:} Il trasduttore itera attraverso tutte le possibili combinazioni dei parametri di una configurazione $(q, h_1, h_2, C)$. Per ogni combinazione valida (cioè, che rispetta i limiti di dimensione e stati di $M$), il trasduttore la scrive sul nastro di output come un nodo del grafo. Questo processo richiede solo spazio logaritmico per memorizzare la configurazione corrente in fase di iterazione.
    \item \textbf{Generazione degli archi di $G_w$:} Il trasduttore itera attraverso tutte le possibili coppie di configurazioni $(C_i, C_j)$. Per ogni coppia, verifica se $M$ può passare da $C_i$ a $C_j$ in un passo. Se sì, scrive l'arco $(C_i, C_j)$ sul nastro di output. Questo richiede solo spazio logaritmico per memorizzare $C_i$ e $C_j$ e simulare il singolo passo di $M$. Gli archi dalle configurazioni accettanti a $t_{final}$ sono generati analogamente.
\end{enumerate}
La verifica della validità di una configurazione o della transizione tra due configurazioni può essere fatta determinismicamente in spazio logaritmico, poiché coinvolge solo la lettura di pochi bit (lo stato corrente, i simboli sotto le testine, le transizioni di $M$) e semplici calcoli.
Il trasduttore $f$ scrive l'output progressivamente, senza memorizzare l'intero grafo in memoria di lavoro.

\textbf{Equivalenza:}
\begin{itemize}
    \item Se $w \in L'$, allora esiste una sequenza di configurazioni $C_0, C_1, \dots, C_k$ tale che $C_0 = s_{start}$, $C_k$ è una configurazione accettante, e ogni $C_{i+1}$ è un successore legale di $C_i$. Questo cammino corrisponde a un cammino da $s_{start}$ a $t_{final}$ nel grafo $G_w$. Quindi $(G_w, s_{start}, t_{final}) \in \text{REACHABILITY}$.
    \item Se $(G_w, s_{start}, t_{final}) \in \text{REACHABILITY}$, allora esiste un cammino da $s_{start}$ a $t_{final}$ in $G_w$. Questo cammino rappresenta una sequenza di configurazioni $C_0, C_1, \dots, C_k$ di $M$ che parte da $s_{start}$ e arriva a una configurazione accettante (che poi si collega a $t_{final}$). Quindi $M$ accetta $w$, il che significa $w \in L'$.
\end{itemize}
Dato che $f$ è calcolabile da un trasduttore LogSpace, abbiamo dimostrato che $L' \le_L \text{REACHABILITY}$ per ogni $L' \in \text{NL}$.
Quindi, \textbf{REACHABILITY} è NL-Complete.
\end{proof}

Questo teorema ha implicazioni importanti: se qualcuno trovasse un algoritmo deterministico in LogSpace per \textbf{REACHABILITY}, allora $\text{L} = \text{NL}$. Dato che finora non è stato trovato tale algoritmo, si tende a credere che $\text{L} \ne \text{NL}$, ovvero che il non determinismo aggiunga un potere significativo quando lo spazio è limitato a $\log n$.


% =====================================================
% --- START LECTURE 25 ---
% =====================================================

\chapter{Space Complexity Avanzata}



\section{Introduzione e Richiami}
Oggi proseguiamo il discorso iniziato ieri sulla Complessità Spaziale, esplorando ulteriori classi di complessità e risultati importanti. Ieri abbiamo introdotto le classi \textbf{DSPACE} e \textbf{NSPACE}, e ci siamo focalizzati in particolare su \textbf{L} (\textbf{DSPACE}$(\log n)$) e \textbf{NL} (\textbf{NSPACE}$(\log n)$). Abbiamo visto che il problema \textbf{REACHABILITY} (raggiungibilità in un grafo orientato) è \textbf{NL}-completo.

Sebbene non sia dimostrato che \textbf{REACHABILITY} appartenga a \textbf{L} (il che implicherebbe \textbf{L} = \textbf{NL}), vedremo oggi un risultato interessante sulla sua complessità in spazio deterministico che sarà fondamentale per un teorema più generale. Non si ritiene che \textbf{REACHABILITY} sia in \textbf{L}.

\section{Complessità di REACHABILITY in DSPACE}
Nonostante non si sappia se \textbf{REACHABILITY} sia risolvibile in spazio logaritmico deterministico (\textbf{L}), siamo in grado di dimostrare che appartiene a \textbf{DSPACE}$(\log^2 n)$, ovvero spazio polinomiale nel logaritmo della dimensione dell'input $n$. Questo è un risultato significativo in quanto mostra che è comunque risolvibile in spazio polilogaritmico.

L'algoritmo che utilizzeremo per dimostrare questo risultato è di tipo ricorsivo e sfrutta una strategia di "ricerca binaria" sulla lunghezza dei cammini.

\subsection{Principio dell'Algoritmo}
L'osservazione chiave è la seguente: se esiste un cammino da un nodo sorgente $S$ a un nodo destinazione $T$ di lunghezza al più $K$, allora deve esistere un nodo intermedio $U$ tale che esista un cammino da $S$ a $U$ e un cammino da $U$ a $T$, entrambi di lunghezza al più $K/2$. Questa idea, combinata con il riuso dello spazio, ci permette di progettare un algoritmo efficiente in termini di memoria.

Intuitivamente, se vogliamo determinare se esiste un cammino da $S$ a $T$, possiamo iterare su tutti i possibili nodi intermedi $U$ e verificare ricorsivamente se esistono cammini da $S$ a $U$ e da $U$ a $T$ entrambi di lunghezza al più $K/2$.

\subsection{Algoritmo \texttt{exists-path}}
Definiamo un algoritmo ricorsivo \texttt{exists-path}$(G, S, T, K)$ che restituisce \texttt{vero} se nel grafo orientato $G=(V,A)$ esiste un cammino da $S$ a $T$ di lunghezza al più $K$.

\begin{minted}[
    frame=lines,
    framesep=2mm,
    linenos,
    tabsize=4,
    obeytabs,
    mathescape=true
]{python}
function exists-path(G, S, T, K):
    # Caso Base 1: Se K=0, S e T devono coincidere
    if K == 0:
        if S == T:
            return TRUE
        else:
            return FALSE

    # Caso Base 2: Se K=1, S e T devono essere collegati direttamente da un arco
    if K == 1:
        if (S, T) in G.A: # G.A è l'insieme degli archi di G
            return TRUE
        else:
            return FALSE
    
    # Passo Ricorsivo: K > 1
    # Iteriamo su tutti i possibili nodi intermedi U
    for each U in G.V:
        # Verifichiamo ricorsivamente l'esistenza di due cammini più corti
        # ceil(K/2) assicura che K si riduca correttamente anche per K dispari.
        if exists-path(G, S, U, ceil(K/2)) and \
           exists-path(G, U, T, ceil(K/2)):
            return TRUE # Trovato un cammino, possiamo terminare
            
    # Se nessuna U intermedia porta a un cammino, non esiste un cammino
    return FALSE
\end{minted}

\begin{example}
Consideriamo il seguente grafo orientato:
\begin{center}
\begin{tikzpicture}[
    node distance=1.5cm,
    every state/.style={fill=white, draw=black, thick, text=black},
    >=Stealth
]
    \node[state] (2) {2 (S)};
    \node[state, above right of=2] (1) {1};
    \node[state, right of=1] (4) {4};
    \node[state, below right of=4] (3) {3};
    \node[state, right of=3] (5) {5 (T)};

    \path[->] (2) edge (1);
    \path[->] (1) edge (4);
    \path[->] (4) edge (3);
    \path[->] (3) edge (5);
    \path[->] (4) edge[bend right=20] (5); % Freccia da 4 a 5
\end{tikzpicture}
\end{center}
Vogliamo sapere se esiste un cammino da $S=2$ a $T=5$. Inizialmente chiamiamo \texttt{exists-path}$(G, 2, 5, |V|)$ dove $|V|$ è il numero di vertici (5 in questo caso), per cercare un cammino di lunghezza arbitraria ma al massimo il numero di vertici.
Il cammino diretto è $2 \to 1 \to 4 \to 3 \to 5$.
\end{example}

\subsection{Analisi della Complessità Spaziale di \texttt{exists-path}}
Per analizzare la complessità spaziale, dobbiamo considerare lo spazio richiesto da una singola chiamata della funzione e l'altezza massima dello stack di chiamate ricorsive.

1.  \textbf{Spazio per una singola chiamata}:
    Una singola chiamata a \texttt{exists-path} deve memorizzare i parametri $S$, $T$, $K$ e la variabile $U$ nel ciclo \texttt{for}.
    *   $S$, $T$, $U$: sono identificativi di nodi. Per un grafo con $N$ nodi, un identificativo richiede $O(\log N)$ bit di spazio (ad esempio, un indice numerico).
    *   $K$: rappresenta la lunghezza massima del cammino. Inizialmente $K$ può essere al massimo $N$ (il numero di nodi). Quindi $K$ richiede $O(\log N)$ bit.
    *   In totale, una singola chiamata richiede $O(\log N)$ spazio sul \emph{work tape}. Questo spazio viene riutilizzato per ogni iterazione del ciclo \texttt{for} (cambiando $U$) e per ogni chiamata ricorsiva (passando nuovi $S, T, K$).

2.  \textbf{Altezza dello stack di chiamate ricorsive}:
    L'algoritmo ricorsivo dimezza il parametro $K$ ad ogni passo (da $K$ a $\lceil K/2 \rceil$). Questo significa che il numero di livelli di ricorsione è logaritmico rispetto al valore iniziale di $K$. Poiché $K$ può essere al massimo il numero di nodi $N$, la profondità massima di ricorsione è $O(\log N)$.
    Durante l'esecuzione, il ciclo \texttt{for each U} implica che le chiamate ricorsive per ogni $U$ non sono eseguite in parallelo. L'algoritmo esplora un ramo dell'albero di ricorsione (corrispondente a una scelta di $U$) e solo dopo aver completato quel ramo (o aver trovato un \texttt{TRUE}) passa al successivo $U$. Questo significa che, in memoria, lo stack contiene al più una sequenza di chiamate ricorsive in un singolo "percorso" attraverso l'albero di computazione (un ramo di ricerca in profondità).

Combinando questi due fattori, lo spazio totale richiesto è il prodotto dello spazio per una singola chiamata per l'altezza massima dello stack:
\[ \text{Spazio totale} = O(\text{Spazio per chiamata}) \times O(\text{Profondità stack}) \]
\[ \text{Spazio totale} = O(\log N) \times O(\log N) = O(\log^2 N) \]
Poiché in informatica teorica $N$ (numero di nodi) è solitamente uguale o proporzionale a $n$ (dimensione dell'input del problema), possiamo affermare che \textbf{REACHABILITY} può essere risolto in \textbf{DSPACE}$(\log^2 n)$.

\section{Il Teorema di Savitch}
Il Teorema di Savitch è uno dei risultati più importanti nella teoria della complessità spaziale, poiché stabilisce una relazione sorprendente tra le classi di complessità spaziali deterministiche e non deterministiche.

\begin{theorem}[Teorema di Savitch]
Sia $S(n)$ una funzione di spazio, tale che $S(n) \geq \Omega(\log n)$. Allora,
\[ \mathbf{NSPACE}(S(n)) \subseteq \mathbf{DSPACE}(S(n)^2) \]
\end{theorem}

Questo teorema implica che, per funzioni di spazio che crescono almeno logaritmicamente, qualsiasi problema risolvibile in spazio non deterministico $S(n)$ può essere risolto in spazio deterministico $S(n)^2$. Questo è un contrasto netto con le classi di complessità temporale, dove il passaggio dal non deterministico al deterministico (ad esempio da \textbf{NP} a \textbf{P}) è congetturato essere esponenziale (ovvero \textbf{P} $\ne$ \textbf{NP}). Per lo spazio, il "costo" della determinizzazione è solamente quadratico.

\subsection{Sketch della Dimostrazione del Teorema di Savitch}
L'idea della dimostrazione si basa su due concetti che abbiamo già introdotto:
1.  La riduzione di qualsiasi linguaggio in \textbf{NSPACE}$(S(n))$ al problema \textbf{REACHABILITY} su un grafo di configurazioni.
2.  L'algoritmo \texttt{exists-path} per \textbf{REACHABILITY} in \textbf{DSPACE}$(\log^2 n)$.

\begin{enumerate}
    \item \textbf{Linguaggio e Macchina Non Deterministica:} Sia $L$ un linguaggio appartenente a \textbf{NSPACE}$(S(n))$. Per definizione, esiste una macchina di Turing non deterministica $M$ che decide $L$ e utilizza $O(S(n))$ spazio sul nastro di lavoro.

    \item \textbf{Costruzione del Computation Graph:} Per simulare $M$ deterministicamente, possiamo costruire il suo \emph{computation graph} (o grafo delle configurazioni). I nodi di questo grafo rappresentano tutte le possibili \emph{configurazioni} di $M$, e un arco $(C_1, C_2)$ esiste se $M$ può passare da $C_1$ a $C_2$ in un singolo passo.

    \item \textbf{Dimensioni di una Configurazione:} Una configurazione di una macchina di Turing tipicamente è definita da:
    \begin{itemize}
        \item Il contenuto del nastro di lavoro: $O(S(n))$ simboli. Se $|\Gamma|$ è la cardinalità dell'alfabeto del nastro di lavoro, ci sono $|\Gamma|^{O(S(n))}$ possibili contenuti del nastro di lavoro.
        \item La posizione della testina sul nastro di input: $O(\log n)$ bit (dove $n$ è la lunghezza dell'input).
        \item La posizione della testina sul nastro di lavoro: $O(\log S(n))$ bit (poiché il nastro di lavoro ha lunghezza $O(S(n))$).
        \item Lo stato corrente della macchina: una costante $c$ di stati ($O(1)$ bit).
    \end{itemize}
    Il numero totale di possibili configurazioni $N_{conf}$ è il prodotto delle possibilità per ciascun componente. Poiché $S(n) \ge \Omega(\log n)$, il termine dominante è il contenuto del nastro di lavoro. Dunque, il numero totale di configurazioni (nodi del computation graph) $N_{conf}$ è $O(|\Gamma|^{S(n)} \cdot n \cdot S(n) \cdot c) = O(|\Gamma|^{S(n)} \cdot n \cdot S(n))$. Se consideriamo solo il termine dominante, $N_{conf} = O(|\Gamma|^{S(n)})$.

    \item \textbf{Applicazione di \texttt{exists-path}:} Per decidere se l'input $x$ è in $L$, dobbiamo determinare se esiste un cammino nel computation graph dalla configurazione iniziale $C_{start}$ (con input $x$) a una delle configurazioni accettanti $C_{accept}$ (possiamo far confluire tutte le configurazioni accettanti in un unico stato finale virtuale). Questo è esattamente il problema \textbf{REACHABILITY}.

    Utilizziamo l'algoritmo \texttt{exists-path} visto in precedenza. Lo spazio richiesto da \texttt{exists-path} per un grafo con $N'$ nodi è $O(\log^2 N')$.
    Nel nostro caso, $N'$ è il numero di configurazioni $N_{conf}$.
    Quindi, lo spazio richiesto è $O(\log^2 N_{conf})$.
    Sostituendo $N_{conf} = O(|\Gamma|^{S(n)} \cdot n \cdot S(n))$, abbiamo:
    \begin{align*} \text{Spazio} &= O(\log^2 (|\Gamma|^{S(n)} \cdot n \cdot S(n))) \\ &= O((\log (|\Gamma|^{S(n)}) + \log n + \log S(n))^2) \\ &= O((S(n)\log|\Gamma| + \log n + \log S(n))^2)\end{align*}
    Poiché $S(n) \ge \Omega(\log n)$, il termine dominante all'interno della parentesi è $S(n)\log|\Gamma|$.
    Il fattore $\log|\Gamma|$ è una costante (dipende dall'alfabeto della TM, non dalla dimensione dell'input).
    Quindi, lo spazio totale richiesto è $O((S(n))^2) = O(S(n)^2)$.
\end{enumerate}
Questo dimostra che qualsiasi linguaggio decidibile in spazio non deterministico $S(n)$ può essere deciso in spazio deterministico $S(n)^2$.

\section{Classi di Complessità Polinomiale Spaziale}
Basandosi sul Teorema di Savitch, possiamo definire le classi di complessità polinomiale in spazio.

\begin{definition}[Classe \textbf{PSPACE}]
La classe \textbf{PSPACE} (Polynomial Space) è l'insieme di tutti i linguaggi che possono essere decisi da una macchina di Turing deterministica che utilizza una quantità di spazio polinomiale rispetto alla dimensione dell'input.
\[ \mathbf{PSPACE} = \bigcup_{c \ge 1} \mathbf{DSPACE}(n^c) \]
\end{definition}

\begin{definition}[Classe \textbf{NPSPACE}]
La classe \textbf{NPSPACE} (Non-deterministic Polynomial Space) è l'insieme di tutti i linguaggi che possono essere decisi da una macchina di Turing non deterministica che utilizza una quantità di spazio polinomiale rispetto alla dimensione dell'input.
\[ \mathbf{NPSPACE} = \bigcup_{c \ge 1} \mathbf{NSPACE}(n^c) \]
\end{definition}

\subsection{Relazione tra PSPACE e NPSPACE}
Dalle definizioni è immediato che $\mathbf{PSPACE} \subseteq \mathbf{NPSPACE}$.
Tuttavia, applicando il Teorema di Savitch a funzioni di spazio polinomiali ($S(n) = n^c$):
Se un linguaggio $L \in \mathbf{NSPACE}(n^c)$, allora per il Teorema di Savitch $L \in \mathbf{DSPACE}((n^c)^2) = \mathbf{DSPACE}(n^{2c})$. Poiché $2c$ è ancora una costante, $n^{2c}$ è ancora un polinomio.
Quindi, $\mathbf{NPSPACE} \subseteq \mathbf{PSPACE}$.

Questo porta alla conclusione sorprendente:
\[ \mathbf{PSPACE} = \mathbf{NPSPACE} \]
Questo risultato è molto potente. Implica che la non-determinismo non aggiunge potere computazionale significativo quando si considera lo spazio polinomiale (e oltre). La ragione intuitiva è che, a differenza del tempo (che una volta speso è "andato"), lo spazio può essere riutilizzato. Una macchina deterministica può provare tutte le scelte possibili di una macchina non deterministica riutilizzando lo stesso spazio, pur impiegando un tempo esponenziale.

\section{Relazioni tra le Classi di Complessità}
Possiamo ora visualizzare le relazioni di inclusione tra alcune delle classi di complessità che abbiamo studiato:

\begin{itemize}
    \item \textbf{L}: Linguaggi decidibili in spazio logaritmico deterministico.
    \item \textbf{NL}: Linguaggi decidibili in spazio logaritmico non deterministico.
    Si congettura che $\mathbf{L} \ne \mathbf{NL}$, anche se è noto che $\mathbf{L} \subseteq \mathbf{NL}$. Inoltre, per il Teorema di Savitch, $\mathbf{NL} \subseteq \mathbf{DSPACE}((\log n)^2)$.

    \item \textbf{P}: Linguaggi decidibili in tempo polinomiale deterministico.
    \item \textbf{NP}: Linguaggi decidibili in tempo polinomiale non deterministico.
    \item \textbf{co-NP}: Linguaggi i cui complementi sono in \textbf{NP}.
    Si congettura che $\mathbf{P} \ne \mathbf{NP}$.

    \item \textbf{PSPACE}: Linguaggi decidibili in spazio polinomiale deterministico.
    \item \textbf{NPSPACE}: Linguaggi decidibili in spazio polinomiale non deterministico.
    Sappiamo che $\mathbf{PSPACE} = \mathbf{NPSPACE}$.

    \item \textbf{EXP}: Linguaggi decidibili in tempo esponenziale deterministico.
    \item \textbf{NEXP}: Linguaggi decidibili in tempo esponenziale non deterministico.
\end{itemize}

Le relazioni di inclusione note sono le seguenti:
\[ \mathbf{L} \subseteq \mathbf{NL} \subseteq \mathbf{P} \subseteq \mathbf{NP} \subseteq \mathbf{PSPACE} = \mathbf{NPSPACE} \subseteq \mathbf{EXP} \subseteq \mathbf{NEXP} \]

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
        scale=0.8,
        level distance=1.5cm,
        level 1/.style={sibling distance=2.5cm},
        level 2/.style={sibling distance=2.5cm},
        level 3/.style={sibling distance=3cm},
        level 4/.style={sibling distance=3.5cm}
    ]
    % Disegna i cerchi concentrici per le classi
    \draw[thick] (0,0) circle (0.5cm) node[below, yshift=-0.5em] {\textbf{L}};
    \draw[thick] (0,0) circle (1.0cm) node[below, yshift=-0.5em] {\textbf{NL}};
    \draw[thick] (0,0) circle (1.5cm) node[below, yshift=-0.5em] {\textbf{P}};
    \draw[thick] (0,0) circle (2.0cm) node[below, yshift=-0.5em] {\textbf{NP}};
    % Per co-NP si può posizionare a lato o implicare che PSPACE le contiene entrambe
    \draw[thick] (0,0) circle (2.5cm) node[below right=0.5cm, xshift=0.5em, yshift=-0.5em] {\textbf{co-NP}};
    \draw[thick] (0,0) circle (3.0cm) node[below, yshift=-0.5em] {\textbf{PSPACE}=\textbf{NPSPACE}};
    \draw[thick] (0,0) circle (3.5cm) node[below, yshift=-0.5em] {\textbf{EXP}};
    \draw[thick] (0,0) circle (4.0cm) node[below, yshift=-0.5em] {\textbf{NEXP}};
    \end{tikzpicture}
    \caption{Diagramma di inclusione delle principali classi di complessità.}
    \label{fig:complexity_classes}
\end{figure}

È noto che tutte le inclusioni sono strette, tranne possibilmente $\mathbf{P} \subseteq \mathbf{NP}$ e $\mathbf{L} \subseteq \mathbf{NL}$.
In particolare, sappiamo che $\mathbf{PSPACE}$ è strettamente più grande di $\mathbf{P}$ e $\mathbf{NL}$ è strettamente più grande di $\mathbf{L}$ (quest'ultima è una congettura ampiamente accettata, implicata dal fatto che si ritiene \textbf{REACHABILITY} non essere in \textbf{L}).

Questo conclude la discussione sulle classi di complessità spaziale.


% =====================================================
% --- START LECTURE 26 ---
% =====================================================

\chapter{Oracoli e Gerarchia Polinomiale}



\section{Introduzione: Ripasso e Problemi Difficili}

Ricapitoliamo il problema del \textbf{Vertex Cover (VC)}.
\begin{definition}[Vertex Cover]
Dato un grafo $G=(V,E)$, un Vertex Cover $VC$ è un sottoinsieme di vertici $V' \subseteq V$ tale che per ogni arco $(u,v) \in E$, almeno uno tra $u$ e $v$ appartiene a $V'$.
\end{definition}

Il problema decisionale associato, che denotiamo $VC$, è:
\begin{definition}[Problema Decisionale VC]
$VC = \{ \langle G, k \rangle \mid G \text{ è un grafo e ammette un Vertex Cover di taglia al più } k \}$.
\end{definition}
Il problema $VC$ è \textbf{NP-completo}. Questo significa che un'istanza $\langle G,k \rangle$ appartiene a $VC$ se e solo se esiste un certificato conciso (un $V'$ di taglia $\le k$) verificabile in tempo polinomiale.

\subsection{Richiamo sulle Classi NP e Co-NP}
\begin{definition}[NP]
La classe NP (Non-deterministic Polynomial time) contiene i linguaggi per cui le istanze "sì" hanno un certificato conciso e verificabile in tempo polinomiale.
\end{definition}
\begin{definition}[Co-NP]
La classe Co-NP contiene i linguaggi $L$ tali che il loro complemento $\overline{L}$ appartiene a NP. Intuitivamente, per i problemi in Co-NP, le istanze "no" hanno un certificato conciso e verificabile in tempo polinomiale.
\end{definition}

\subsection{Il Problema Min-Cover}
Introduciamo un problema decisionale leggermente diverso: \textbf{Min-Cover}.
\begin{definition}[Min-Cover]
$Min-Cover = \{ \langle G, k \rangle \mid G \text{ è un grafo e il Vertex Cover di taglia minima ha taglia esattamente } k \}$.
\end{definition}
A differenza di $VC$, $Min-Cover$ richiede che la taglia sia \emph{esattamente} $k$ e che sia la taglia \emph{minima}.

\subsubsection{Complessità di Min-Cover}
*   \textbf{Sta in NP?} Se abbiamo un certificato $V'$ di taglia $k$, possiamo verificare che sia un Vertex Cover. Ma come verifichiamo che sia \emph{minimo}? Non possiamo, a meno di testare tutte le possibilità più piccole, che porterebbe a un tempo esponenziale. Sembra non essere in NP.
*   \textbf{Sta in Co-NP?} Per dimostrare che un'istanza $\langle G,k \rangle$ è un "no" (ovvero che la taglia minima non è $k$), dovremmo fornire un certificato conciso. Se la taglia minima fosse $< k$, potremmo fornire un $V'$ di taglia inferiore. Ma se la taglia minima fosse $> k$, non avremmo un modo semplice per certificarlo in Co-NP. Sembra non essere in Co-NP.
*   \textbf{Sta in PSPACE?} Sì. Possiamo generare tutti i possibili sottoinsiemi di $V$, verificare per ciascuno se è un Vertex Cover e calcolarne la taglia. Teniamo traccia della taglia minima trovata. Poiché i sottoinsiemi possono essere generati e testati uno alla volta riutilizzando lo stesso spazio, questo può essere fatto in spazio polinomiale.

\subsubsection{Un Algoritmo per Min-Cover}
Osserviamo che $Min-Cover$ può essere riformulato in termini di $VC$:
$\langle G, k \rangle \in Min-Cover \iff \langle G, k \rangle \in VC \text{ AND } \langle G, k-1 \rangle \notin VC$.

Questo significa che per decidere $Min-Cover$, possiamo fare due domande a un decisore per $VC$.
Consideriamo il seguente pseudocodice:
\begin{minted}[mathescape, linenos, escapeinside=||]{python}
# `check_VC` è una subroutine che decide il problema VC
def solve_MinCover(G, k):
    # Domanda 1: Esiste un VC di taglia <= k?
    result1 = check_VC(G, k)

    # Domanda 2: Esiste un VC di taglia <= k-1?
    result2 = check_VC(G, k - 1)

    # La taglia minima è esattamente k se result1 è vero E result2 è falso
    return result1 and (not result2)
\end{minted}
Questo algoritmo deterministico in tempo polinomiale necessita di un "aiuto" esterno da una procedura capace di risolvere $VC$.

\section{Macchine ad Oracolo}
Per formalizzare il concetto di "aiuto" o "subroutine esterna", introduciamo il modello delle \textbf{macchine ad oracolo}.

\begin{definition}[Macchina ad Oracolo]
Una macchina ad oracolo $M^L$ è una Macchina di Turing deterministica o non deterministica estesa con i seguenti componenti:
\begin{itemize}
    \item \textbf{Oracle Tape (Nastro dell'Oracolo):} Un nastro di sola scrittura utilizzato per formulare le query all'oracolo.
    \item \textbf{Query State ($q_?$):} Uno stato speciale in cui la macchina può entrare dopo aver scritto una query sull'Oracle Tape.
    \item \textbf{Answer States ($q_{yes}$, $q_{no}$):} Due stati speciali in cui la macchina transita in un singolo passo dopo essere entrata in $q_?$. La transizione dipende dalla risposta dell'oracolo: a $q_{yes}$ se la stringa sulla query tape è un'istanza "sì" del linguaggio $L$, a $q_{no}$ se è un'istanza "no".
\end{itemize}
L'oracolo $L$ è un linguaggio (un problema decisionale). Quando la macchina entra in $q_?$, la stringa sul nastro dell'oracolo viene valutata istantaneamente dall'oracolo $L$. Dopo la transizione a $q_{yes}$ o $q_{no}$, il contenuto dell'Oracle Tape viene automaticamente cancellato. Ogni query all'oracolo conta come un singolo passo di computazione per la macchina chiamante.
\end{definition}
La notazione $M^L$ indica che la macchina $M$ ha accesso a un oracolo per il linguaggio $L$.

\section{Classi di Complessità con Oracolo}
Le macchine ad oracolo permettono di definire nuove classi di complessità.
\begin{definition}[$P^C$]
$P^C = \{ L \mid L \text{ può essere deciso in tempo polinomiale da una macchina ad oracolo deterministica } M^{\text{L'}} \text{ dove } L' \in C \}$.
\end{definition}
\begin{definition}[$NP^C$]
$NP^C = \{ L \mid L \text{ può essere deciso in tempo polinomiale da una macchina ad oracolo non deterministica } M^{\text{L'}} \text{ dove } L' \in C \}$.
\end{definition}

\subsection{Min-Cover in $P^{NP}$}
Basandoci sull'algoritmo precedente per $Min-Cover$:
Il problema $VC$ è in NP. L'algoritmo \texttt{solve\_MinCover} è deterministico e fa un numero costante (2) di query a un oracolo per $VC$ (che è un linguaggio in NP).
Quindi, $Min-Cover \in P^{NP}$.

\subsection{Relazioni tra le Classi con Oracolo}
\begin{proposition}[$NP \subseteq P^{NP}$]
Sia $L \in NP$. Per dimostrare che $L \in P^{NP}$, costruiamo una macchina $M$ che utilizza un oracolo per $L$. $M$ prende l'input $x$, lo scrive sul nastro dell'oracolo, effettua una query. Se l'oracolo risponde "sì", $M$ accetta; altrimenti, $M$ rifiuta. Questa macchina è deterministica e opera in tempo polinomiale (solo 1 query + tempo per copiare input). Quindi, $L \in P^{NP}$.
\end{proposition}

\begin{proposition}[$CoNP \subseteq P^{NP}$]
Sia $L \in CoNP$. Ciò implica che $\overline{L} \in NP$. Costruiamo una macchina $M$ che utilizza un oracolo per $\overline{L}$. $M$ prende l'input $x$, lo scrive sul nastro dell'oracolo, effettua una query. Se l'oracolo per $\overline{L}$ risponde "sì" (cioè $x \in \overline{L}$), $M$ rifiuta; altrimenti (cioè $x \notin \overline{L}$, quindi $x \in L$), $M$ accetta. Questa macchina è deterministica e opera in tempo polinomiale. Quindi, $L \in P^{NP}$.
\end{proposition}

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[node distance=1.5cm, auto, thick, scale=0.8,
        set/.style = {ellipse, draw, minimum width=2.5cm, minimum height=1.5cm}]

        % P
        \node (P) [set, label=below:$P$] at (0,0) {};
        
        % NP and CoNP
        \node (NP) [set, above right=0.8cm and 0.8cm of P, label=below:$NP$] {};
        \node (CoNP) [set, above left=0.8cm and 0.8cm of P, label=below:$CoNP$] {};
        
        % PNP
        \node (PNP) [set, minimum width=5cm, minimum height=3.5cm, draw, fit=(NP) (CoNP), label=above:$P^{NP}$] at (0,1.5) {};

        % Inclusions
        \draw[->] (P) -- (NP);
        \draw[->] (P) -- (CoNP);
        \draw[->] (NP) -- (PNP);
        \draw[->] (CoNP) -- (PNP);
    \end{tikzpicture}
    \caption{Relazioni iniziali tra $P$, $NP$, $CoNP$ e $P^{NP}$}
\end{figure}

\section{Gerarchia Polinomiale}
La nozione di classi ad oracolo può essere generalizzata, definendo livelli ricorsivamente. Questo porta alla \textbf{Gerarchia Polinomiale}.
La gerarchia polinomiale è un insieme di classi di complessità denotate da $\Sigma_i^P$, $\Pi_i^P$, e $\Delta_i^P$ per $i \ge 0$.

\begin{definition}[Gerarchia Polinomiale]
Le classi della Gerarchia Polinomiale sono definite come segue:
\begin{itemize}
    \item \textbf{Livello 0:}
    $\Sigma_0^P = P$
    $\Pi_0^P = P$
    $\Delta_0^P = P$
    \item \textbf{Livello $i+1$ (per $i \ge 0$):}
    $\Sigma_{i+1}^P = NP^{\Sigma_i^P}$
    $\Pi_{i+1}^P = co\Sigma_{i+1}^P$
    $\Delta_{i+1}^P = P^{\Sigma_i^P}$
\end{itemize}
\end{definition}

\subsubsection{Esempi dei Primi Livelli:}
\begin{itemize}
    \item $\Sigma_1^P = NP^{\Sigma_0^P} = NP^P = NP$ (poiché un oracolo in P non aggiunge potere computazionale a una macchina NP, che già può simulare P).
    \item $\Pi_1^P = co\Sigma_1^P = coNP$.
    \item $\Delta_1^P = P^{\Sigma_0^P} = P^P = P$.
    \item $\Delta_2^P = P^{\Sigma_1^P} = P^{NP}$. (Il problema $Min-Cover$ visto prima si colloca precisamente in $\Delta_2^P$).
    \item $\Sigma_2^P = NP^{\Sigma_1^P} = NP^{NP}$.
    \item $\Pi_2^P = co\Sigma_2^P = coNP^{NP}$.
\end{itemize}

\subsection{Relazioni di Inclusione nella Gerarchia Polinomiale}
\begin{proposition}[Inclusioni Fondamentali]
Per ogni $i \ge 0$:
\begin{enumerate}
    \item $\Sigma_i^P \subseteq \Delta_{i+1}^P$: Ogni linguaggio in $\Sigma_i^P$ può essere deciso da una macchina deterministica in tempo polinomiale che fa una sola query a un oracolo per un linguaggio in $\Sigma_i^P$ (la query è l'input stesso).
    \item $\Pi_i^P \subseteq \Delta_{i+1}^P$: Similmente, ogni linguaggio in $\Pi_i^P$ (il cui complemento è in $\Sigma_i^P$) può essere deciso da una macchina deterministica in tempo polinomiale con un oracolo in $\Sigma_i^P$ (query il complemento dell'input e inverte la risposta).
    \item $\Delta_i^P \subseteq \Sigma_i^P$: Una macchina deterministica in tempo polinomiale con oracolo $\Sigma_{i-1}^P$ è meno potente di una macchina non deterministica in tempo polinomiale con lo stesso oracolo.
    \item $\Delta_i^P \subseteq \Pi_i^P$: Simile al punto 3, una macchina deterministica è meno potente di una macchina non deterministica che accetta se il complemento rifiuta.
\end{enumerate}
\end{proposition}

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[node distance=1.5cm, auto, thick, scale=0.8]

        % Layer 0 (P)
        \node (P) [ellipse, draw, minimum width=2.5cm, minimum height=1.5cm, label=below:{$P (\Sigma_0^P, \Pi_0^P, \Delta_0^P)$}] at (0,0) {};

        % Layer 1 (NP, CoNP)
        \node (NP) [ellipse, draw, minimum width=2.5cm, minimum height=1.5cm, above right=1.2cm and 1.2cm of P, label=below:{$NP (\Sigma_1^P)$}] {};
        \node (CoNP) [ellipse, draw, minimum width=2.5cm, minimum height=1.5cm, above left=1.2cm and 1.2cm of P, label=below:{$CoNP (\Pi_1^P)$}] {};
        
        % Layer 2 (Delta_2^P)
        \node (Delta2) [ellipse, draw, minimum width=5cm, minimum height=2cm, fit=(NP) (CoNP), label=above:{$\Delta_2^P (P^{NP})$}] at (0, 3) {};

        % Layer 2 (Sigma_2^P, Pi_2^P)
        \node (Sigma2) [ellipse, draw, minimum width=2.5cm, minimum height=1.5cm, above right=1.2cm and 1.2cm of Delta2, label=below:{$\Sigma_2^P (NP^{NP})$}] {};
        \node (Pi2) [ellipse, draw, minimum width=2.5cm, minimum height=1.5cm, above left=1.2cm and 1.2cm of Delta2, label=below:{$\Pi_2^P (CoNP^{NP})$}] {};

        % Layer 3 (Delta_3^P)
        \node (Delta3) [ellipse, draw, minimum width=5cm, minimum height=2cm, fit=(Sigma2) (Pi2), label=above:{$\Delta_3^P (P^{NP^{NP}})$}] at (0, 6) {};

        % Layer 3 (Sigma_3^P, Pi_3^P)
        \node (Sigma3) [ellipse, draw, minimum width=2.5cm, minimum height=1.5cm, above right=1.2cm and 1.2cm of Delta3, label=below:{$\Sigma_3^P$}] {};
        \node (Pi3) [ellipse, draw, minimum width=2.5cm, minimum height=1.5cm, above left=1.2cm and 1.2cm of Delta3, label=below:{$\Pi_3^P$}] {};

        % Inclusions
        \draw[->] (P) -- (NP);
        \draw[->] (P) -- (CoNP);
        \draw[->] (NP) -- (Delta2);
        \draw[->] (CoNP) -- (Delta2);
        \draw[->] (Delta2) -- (Sigma2);
        \draw[->] (Delta2) -- (Pi2);
        \draw[->] (Sigma2) -- (Delta3);
        \draw[->] (Pi2) -- (Delta3);
        \draw[->] (Delta3) -- (Sigma3);
        \draw[->] (Delta3) -- (Pi3);
        
        % Vertical Ellipses for higher levels
        \node (dots1) at (0, 7.5) {$\vdots$};
        \node (dots2) at (0, 8) {$\vdots$};
        
        % PSPACE encompassing
        \node (PSPACE) [ellipse, draw, minimum width=10cm, minimum height=9cm, dashed, fit=(Delta3) (Sigma3) (Pi3), label=above right:{$PSPACE$}] at (0, 4.5) {};

        % EXPTIME encompassing
        \node (EXPTIME) [ellipse, draw, minimum width=12cm, minimum height=11cm, dashed, fit=(PSPACE), label=above right:{$EXPTIME$}] at (0, 5) {};
        
        % NEXPTIME encompassing
        \node (NEXPTIME) [ellipse, draw, minimum width=14cm, minimum height=13cm, dashed, fit=(EXPTIME), label=above right:{$NEXPTIME$}] at (0, 6) {};

        % R encompassing
        \node (R) [ellipse, draw, minimum width=16cm, minimum height=15cm, dashed, fit=(NEXPTIME), label=above right:{$R$}] at (0, 7) {};

    \end{tikzpicture}
    \caption{Struttura della Gerarchia Polinomiale e relazioni con altre classi.}
    \label{fig:polynomial_hierarchy}
\end{figure}

La gerarchia polinomiale contiene infiniti livelli, e la questione se questa gerarchia collassi (cioè se ci sia un livello $k$ tale che $\Sigma_k^P = PSPACE$) o se sia infinita è un problema aperto. Si ritiene che sia infinita. L'intera gerarchia polinomiale è contenuta in $PSPACE$. Al di sopra di $PSPACE$ troviamo $EXPTIME$, $NEXPTIME$, e infine la classe $R$ (Ricorsivamente enumerabile o Decidibile).

\subsection{Problemi Completi nella Gerarchia Polinomiale}
Le formule booleane quantificate (QBF) sono problemi naturali che si collocano ai vari livelli della gerarchia.

\begin{definition}[SAT Quantificato]
\begin{itemize}
    \item \textbf{Existential SAT ($SAT_{\exists}$):}
    $\{ \langle \Phi \rangle \mid \exists X_1, \dots, X_n \text{ tale che } \Phi(X_1, \dots, X_n) \text{ è vera} \}$
    Questo è il problema $SAT$ standard ed è NP-completo ($\Sigma_1^P$-completo).
    \item \textbf{Alternating SAT ($\Sigma_2^P$-complete):}
    $\{ \langle \Phi \rangle \mid \exists X_1, \dots, X_n \forall Y_1, \dots, Y_m \text{ tale che } \Phi(X, Y) \text{ è vera} \}$
    Questo problema è $\Sigma_2^P$-completo. Significa: "Esiste un'assegnazione per $X$ tale che per ogni assegnazione di $Y$, la formula $\Phi$ è vera".
    \item \textbf{Alternating SAT ($\Sigma_3^P$-complete):}
    $\{ \langle \Phi \rangle \mid \exists X \forall Y \exists Z \text{ tale che } \Phi(X, Y, Z) \text{ è vera} \}$
    Questo problema è $\Sigma_3^P$-completo. Significa: "Esiste un $X$ tale che per ogni $Y$, esiste un $Z$ tale che $\Phi$ è vera".
\end{itemize}
\end{definition}

L'alternanza dei quantificatori ("esiste", "per ogni", "esiste", ...) cattura la complessità dei giochi strategici (come gli scacchi): "Esiste una mossa che posso fare, tale che, qualunque mossa faccia il mio avversario, esiste una mossa che posso fare io, tale che... e alla fine vinco."

\section{Problemi Funzionali e Classi Funzionali}
Finora abbiamo parlato di problemi decisionali (risposta sì/no). Ora introduciamo i problemi funzionali, che richiedono il calcolo di un valore.

\begin{definition}[Functional Min-Cover (FMin-Cover)]
$FMin-Cover(G) = \min \{ |V'| \mid V' \subseteq V \text{ è un Vertex Cover di } G \}$.
\end{definition}
Questo problema richiede di calcolare la taglia del Vertex Cover minimo, non solo di decidere se esiste un VC di una certa taglia.

\subsection{La Classe FP}
\begin{definition}[FP (Functional Polynomial Time)]
FP è la classe delle funzioni che possono essere calcolate da trasduttori deterministici in tempo polinomiale. Un trasduttore è una Macchina di Turing con un nastro di output aggiuntivo.
\end{definition}
Le riduzioni polinomiali che usiamo spesso sono funzioni che appartengono a FP.

\subsection{FMin-Cover e le Classi con Oracolo}
Consideriamo nuovamente il problema $FMin-Cover$. Vogliamo calcolare la taglia del VC minimo. Abbiamo a disposizione un oracolo per $VC$ (il problema decisionale, che è NP-completo).

\subsubsection{Algoritmo per FMin-Cover usando un Oracolo VC:}
\textbf{1. Ricerca Lineare:}
\begin{enumerate}
    \item Si inizia con $k=1$.
    \item Si chiede all'oracolo $VC$: "Il grafo $G$ ha un Vertex Cover di taglia $\le k$?" (query $\langle G, k \rangle$).
    \item Se l'oracolo risponde "no", si incrementa $k$ e si ripete.
    \item Se l'oracolo risponde "sì", allora $k$ è la taglia minima. Si restituisce $k$.
\end{enumerate}
Il valore massimo di $k$ da testare è $|V|$ (poiché l'insieme di tutti i vertici $V$ è sempre un Vertex Cover). Quindi, si effettuano al più $|V|$ query all'oracolo. Poiché $|V|$ è polinomiale nella dimensione dell'input, questa procedura è in tempo polinomiale.
Quindi, $FMin-Cover \in FP^{NP}$.

\textbf{2. Ricerca Binaria (più efficiente):}
\begin{enumerate}
    \item Si stabilisce un intervallo di ricerca per la taglia minima, ad esempio $[0, |V|]$.
    \item Si seleziona il punto medio $m$ dell'intervallo.
    \item Si chiede all'oracolo $VC$: "Il grafo $G$ ha un Vertex Cover di taglia $\le m$?" (query $\langle G, m \rangle$).
    \item Se l'oracolo risponde "sì", significa che la taglia minima è $\le m$. Si restringe l'intervallo a $[0, m]$.
    \item Se l'oracolo risponde "no", significa che la taglia minima è $> m$. Si restringe l'intervallo a $[m+1, |V|]$.
    \item Si ripete fino a quando l'intervallo si riduce a un singolo valore.
\end{enumerate}
Con la ricerca binaria, il numero di query all'oracolo è $O(\log |V|)$.
Questa osservazione porta a una notazione più specifica per la classe di complessità:
$FMin-Cover \in FP^{NP[O(\log n)]}$. Questa notazione indica che la macchina deterministica in tempo polinomiale effettua solo un numero logaritmico di query all'oracolo in NP.

\subsection{Perché FMin-Cover non è in FP (se P $\neq$ NP)?}
La domanda finale è: $FMin-Cover \in FP$?
Se $FMin-Cover$ fosse in $FP$, significherebbe che esisterebbe un algoritmo deterministico in tempo polinomiale capace di \textbf{calcolare} la taglia del Vertex Cover minimo.
Se avessimo tale algoritmo, potremmo risolvere il problema decisionale $VC$ (che è NP-completo) in tempo polinomiale:
\begin{itemize}
    \item Input: $\langle G, k \rangle$.
    \item Calcola $size = FMin-Cover(G)$ usando l'algoritmo FP ipotizzato.
    \item Se $size \le k$, rispondi "sì"; altrimenti, rispondi "no".
\item Questo significherebbe che $VC \in P$. Ma poiché $VC$ è NP-completo, questo implicherebbe $P=NP$.
\end{itemize}
Poiché la maggior parte dei ricercatori ritiene che $P \neq NP$, si conclude che $FMin-Cover$ (e la maggior parte dei problemi di ottimizzazione NP-difficili) non è in FP.

Questo mostra la relazione fondamentale tra i problemi decisionali e i loro equivalenti funzionali/di ottimizzazione: la difficoltà di calcolare la soluzione ottimale è direttamente legata alla difficoltà di decidere una proprietà della soluzione.


% =====================================================
% --- START LECTURE 27 ---
% =====================================================

\chapter{Traveling Salesperson Problem (TSP)}



\section{Richiami: Macchine Oracolo e Gerarchia Polinomiale}

Riprendiamo il concetto di macchine oracolo e gerarchia polinomiale, già introdotto per problemi come il Vertex Cover.

\begin{definition}[Funzioni in $\text{FP}^{\text{NP}}$]
$\text{FP}^{\text{NP}}$ è l'insieme delle funzioni calcolabili da trasduttori deterministici in tempo polinomiale che hanno accesso a un oracolo in $\text{NP}$.
\end{definition}

\begin{remark}
Un \textbf{oracolo} è una subroutine che risponde a interrogazioni sull'appartenenza di stringhe a un linguaggio (es. un linguaggio in $\text{NP}$) in un singolo passo di calcolo. Un oracolo può essere molto potente (es. oracolo $\text{NP}$, $\text{NEXP}$).
\end{remark}

Le macchine oracolo permettono di definire gerarchie di complessità, come la Gerarchia Polinomiale ($\text{PH}$).
\begin{definition}[$\Sigma_k^P$]
$\Sigma_k^P$ è il $k$-esimo livello della Gerarchia Polinomiale. In particolare, $\Sigma_2^P$ è la classe dei linguaggi decidibili da macchine in $\text{NP}$ con accesso a un oracolo in $\text{NP}$.
\end{definition}

\begin{remark}
I problemi $\Sigma_k^P$-completi sono intrinsecamente più difficili dei problemi $\text{NP}$-completi. Un problema $\Sigma_2^P$-completo, ad esempio, richiede algoritmi con due livelli di "backtracking" annidati, suggerendo un tempo di esecuzione doppiamente esponenziale su macchine deterministiche (se P $\neq$ NP). Conoscere la classe di complessità di un problema aiuta a progettare algoritmi appropriati e a capire i limiti computazionali.
\end{remark}

\section{Traveling Salesperson Problem (TSP)}

Vediamo il Traveling Salesperson Problem (TSP) sia nella sua versione computazionale (funzionale) che decisionale.

\begin{definition}[Grafo Pesato]
Un grafo pesato è una tupla $G = (V, E, \lambda)$, dove $V$ è l'insieme dei vertici, $E$ è l'insieme degli archi (orientati o meno), e $\lambda: E \to \mathbb{N}^+$ è una funzione di pesatura che associa un peso (intero positivo) a ciascun arco.
\end{definition}

\begin{definition}[Ciclo Hamiltoniano]
Un ciclo Hamiltoniano in un grafo $G=(V, E)$ è un percorso che parte da un vertice, visita tutti gli altri vertici esattamente una volta, e termina nel vertice di partenza. Il peso di un percorso o ciclo è la somma dei pesi degli archi che lo compongono.
\end{definition}

\begin{definition}[FTSP - Functional Traveling Salesperson Problem]
Dato un grafo $G=(V, E, \lambda)$ non orientato e pesato, calcolare il peso del ciclo Hamiltoniano di peso minimo.
Formalmente, $\text{FTSP}(G) = \min \{ \lambda(\pi) \mid \pi \text{ è un ciclo Hamiltoniano di } G \}$.
\end{definition}

\begin{definition}[TSP - Decision Traveling Salesperson Problem]
Dato un grafo $G=(V, E, \lambda)$ non orientato e pesato, e un intero $K$, decidere se esiste un ciclo Hamiltoniano in $G$ di peso al più $K$.
Le istanze di TSP sono coppie $(G, K)$.
\end{definition}

\subsection{Risoluzione di FTSP usando un Oracolo per TSP}

Supponiamo di avere un oracolo per TSP. Vogliamo risolvere FTSP usando chiamate a questo oracolo.

\subsubsection{Prima Soluzione (Naive)}
1.  Inizializza $K=1$.
2.  Chiedi all'oracolo: "Esiste un ciclo Hamiltoniano di peso al più $K$ in $G$?"
3.  Se l'oracolo risponde "Sì", allora $K$ è il peso minimo. Termina.
4.  Se l'oracolo risponde "No", incrementa $K$ e ripeti dal passo 2.
5.  Se $G$ non ammette alcun ciclo Hamiltoniano, l'oracolo risponderà sempre "No". Per garantire la terminazione, il valore massimo di $K$ da testare può essere la somma di tutti i pesi degli archi in $G$ (chiamiamo questo valore $S_{total}$). Se l'oracolo risponde "No" anche per $S_{total}$, allora non esiste un ciclo Hamiltoniano.

\begin{remark}
Questa procedura non colloca FTSP in $\text{FP}^{\text{NP}}$. Il numero di chiamate all'oracolo è proporzionale a $S_{total}$. Poiché $S_{total}$ può essere esponenziale nella dimensione dell'input (se i pesi sono grandi e rappresentati in binario), il trasduttore impiega un tempo esponenziale per effettuare le chiamate. La complessità è una funzione della taglia dell'input, non del suo valore numerico.
\end{remark}

\subsubsection{Seconda Soluzione (Ricerca Binaria)}
Questa procedura è quella corretta per collocare FTSP in $\text{FP}^{\text{NP}}$.

1.  Calcola $S_{total}$, la somma di tutti i pesi degli archi in $G$.
2.  Chiedi all'oracolo: "Esiste un ciclo Hamiltoniano di peso al più $S_{total}$ in $G$?"
    *   Se l'oracolo risponde "No", allora $G$ non ammette alcun ciclo Hamiltoniano. Termina.
    *   Se l'oracolo risponde "Sì", allora esiste almeno un ciclo Hamiltoniano. Procedi con una ricerca binaria nell'intervallo $[0, S_{total}]$ per trovare il peso minimo.
3.  Esegui una ricerca binaria sull'intervallo $[L, R]$, inizialmente $[0, S_{total}]$:
    *   Sia $M = \lfloor (L+R)/2 \rfloor$.
    *   Chiedi all'oracolo: "Esiste un ciclo Hamiltoniano di peso al più $M$ in $G$?"
    *   Se l'oracolo risponde "Sì", allora il peso minimo è $\le M$, quindi imposta $R = M$.
    *   Se l'oracolo risponde "No", allora il peso minimo è $> M$, quindi imposta $L = M+1$.
    *   Ripeti finché $L=R$. Il valore finale di $L$ (o $R$) è il peso minimo.

\begin{theorem}
$\text{FTSP} \in \text{FP}^{\text{NP}}$.
\end{theorem}
\begin{proof}
Il numero di chiamate all'oracolo effettuate dalla procedura di ricerca binaria è logaritmico rispetto alla dimensione del dominio di ricerca $[0, S_{total}]$.
La dimensione del dominio è $S_{total}$, che può essere esponenziale nella taglia dell'input (numero di bit per rappresentare $S_{total}$).
Tuttavia, $\log(S_{total})$ è polinomiale nella taglia della rappresentazione binaria di $S_{total}$ (e quindi polinomiale nella taglia dell'input).
Ad esempio, se $S_{total} \approx 2^N$ dove $N$ è la taglia dell'input, allora $\log(S_{total}) \approx N$, che è polinomiale.
Il trasduttore deterministico esegue un numero polinomiale di chiamate all'oracolo e le operazioni ausiliarie (calcolo $S_{total}$, calcolo $M$, aggiornamento $L, R$) sono polinomiali. Pertanto, FTSP appartiene a $\text{FP}^{\text{NP}}$.
\end{proof}

\subsection{È FTSP in FP?}

Ci chiediamo se $\text{FTSP} \in \text{FP}$, ovvero se può essere risolto da un algoritmo deterministico in tempo polinomiale senza l'ausilio di un oracolo.

\begin{proposition}
Se $\text{FTSP} \in \text{FP}$, allora $\text{TSP} \in \text{P}$.
\end{proposition}
\begin{proof}
Supponiamo che esista un algoritmo deterministico polinomiale per FTSP. Per risolvere un'istanza $(G, K)$ di TSP:
1.  Usa l'algoritmo per FTSP per calcolare il peso minimo di un ciclo Hamiltoniano in $G$, sia esso $W_{min}$.
2.  Confronta $W_{min}$ con $K$. Se $W_{min} \le K$, la risposta è "Sì"; altrimenti, la risposta è "No".
Poiché sia il calcolo di $W_{min}$ che il confronto sono polinomiali, anche TSP sarebbe risolvibile in tempo polinomiale.
\end{proof}

Dato che P $\neq$ NP è un problema aperto e si ritiene improbabile che P = NP, la proposizione precedente implica che è altamente improbabile che FTSP sia in FP, a meno che NP non collassi su P. Questo sposta il nostro focus sulla complessità di TSP.

\section{TSP è NP-Completo}

Dimostriamo ora che TSP è NP-completo. Per fare ciò, dobbiamo mostrare due cose:
1.  $\text{TSP} \in \text{NP}$ (membership).
2.  $\text{TSP}$ è $\text{NP}$-arduo (hardness).

\subsection{Membership: TSP $\in$ NP}

\begin{theorem}
$\text{TSP} \in \text{NP}$.
\end{theorem}
\begin{proof}
Per mostrare che TSP è in NP, dobbiamo esibire un certificato di dimensione polinomiale che possa essere verificato in tempo polinomiale da una macchina deterministica.
Sia $(G, K)$ un'istanza di TSP.
\begin{itemize}
    \item \textbf{Guess (Certificato):} La macchina non deterministica "indovina" una sequenza di vertici $v_1, v_2, \dots, v_{|V|}, v_1$ che rappresenta un potenziale ciclo Hamiltoniano. Questa sequenza ha lunghezza $|V|+1$, quindi è polinomiale nella dimensione dell'input.
    \item \textbf{Check (Verifica):} Una macchina deterministica può verificare in tempo polinomiale che la sequenza:
    \begin{enumerate}
        \item Formi un ciclo: $v_1 \to v_2 \to \dots \to v_{|V|} \to v_1$. Questo significa controllare che per ogni $i \in \{1, \dots, |V|-1\}$, esista un arco $(v_i, v_{i+1})$ e un arco $(v_{|V|}, v_1)$ in $E$.
        \item Visiti tutti i vertici: Controllare che ogni vertice in $V$ appaia esattamente una volta nella sequenza $v_1, \dots, v_{|V|}$.
        \item Abbia peso al più $K$: Calcolare la somma dei pesi degli archi lungo il ciclo $\sum_{i=1}^{|V|-1} \lambda(v_i, v_{i+1}) + \lambda(v_{|V|}, v_1)$ e verificare che sia $\le K$.
    \end{enumerate}
Tutti questi passaggi possono essere eseguiti in tempo polinomiale rispetto a $|V|$ e $|E|$.
Pertanto, TSP è in NP.
\end{itemize}
\end{proof}

\subsection{Hardness: TSP è NP-arduo}

Per dimostrare che TSP è NP-arduo, mostreremo una catena di riduzioni polinomiali da un problema noto $\text{NP}$-completo (3SAT) a TSP:
$$ \text{3SAT} \le_P \text{DHC} \le_P \text{HC} \le_P \text{TSP} $$
dove:
\begin{itemize}
    \item \textbf{DHC (Directed Hamiltonian Cycle):} Dato un grafo orientato $G$, decidere se $G$ ammette un ciclo Hamiltoniano.
    \item \textbf{HC (Undirected Hamiltonian Cycle):} Dato un grafo non orientato $G$, decidere se $G$ ammette un ciclo Hamiltoniano.
\end{itemize}

\subsubsection{Riduzione: 3SAT $\le_P$ DHC}

Questa è la riduzione più complessa. Dobbiamo trasformare una formula booleana in 3CNF in un grafo orientato $G$ tale che $G$ ammetta un ciclo Hamiltoniano se e solo se la formula è soddisfacibile.

Sia $\Phi = C_1 \land C_2 \land \dots \land C_m$ una formula 3CNF con $m$ clausole e $n$ variabili $X = \{x_1, x_2, \dots, x_n\}$. Assumiamo che nessuna clausola contenga un letterale e la sua negazione (es. $x \lor \neg x$).

\textbf{Costruzione del Grafo $G_\Phi$:}
Il grafo $G_\Phi$ sarà costruito in modo da codificare sia gli assegnamenti di verità delle variabili che la soddisfazione delle clausole.
\begin{itemize}
    \item \textbf{Sezioni Variabili:} Per ogni variabile $x_i \in X$, creiamo una "catena" di nodi composta da $m$ coppie di nodi, una per ogni clausola. Ogni coppia è $(x_{i,j}', x_{i,j}'')$ per $j=1, \dots, m$. Aggiungiamo nodi intermedi ausiliari (piccoli pallini) tra le coppie e nodi diamante all'inizio e alla fine di ciascuna catena.

    \begin{figure}[h]
    \centering
    \begin{tikzpicture}[node distance=1.5cm, auto, thick, scale=0.8, every node/.style={transform shape}]
        \node[diamond_node_style] (start_xi) at (0,0) {};
        \node[small_node_style] (aux_0) at (1.5,0) {};
        \node[node_style] (xi_1_prime) at (3,0.5) {$x_{i,1}'$};
        \node[node_style] (xi_1_second) at (3,-0.5) {$x_{i,1}''$};
        \node[small_node_style] (aux_1) at (4.5,0) {};
        \node[node_style] (xi_2_prime) at (6,0.5) {$x_{i,2}'$};
        \node[node_style] (xi_2_second) at (6,-0.5) {$x_{i,2}''$};
        \node[small_node_style] (aux_2) at (7.5,0) {};
        \node (dots) at (9,0) {\dots};
        \node[small_node_style] (aux_m_minus_1) at (10.5,0) {};
        \node[node_style] (xi_m_prime) at (12,0.5) {$x_{i,m}'$};
        \node[node_style] (xi_m_second) at (12,-0.5) {$x_{i,m}''$};
        \node[small_node_style] (aux_m) at (13.5,0) {};
        \node[diamond_node_style] (end_xi) at (15,0) {};

        % Green path (True)
        \draw[green_arrow] (start_xi) -- (aux_0);
        \draw[green_arrow] (aux_0) -- (xi_1_prime);
        \draw[green_arrow] (xi_1_prime) -- (aux_1);
        \draw[green_arrow] (aux_1) -- (xi_2_prime);
        \draw[green_arrow] (xi_2_prime) -- (aux_2);
        \draw[green_arrow] (aux_2) -- (dots);
        \draw[green_arrow] (dots) -- (aux_m_minus_1);
        \draw[green_arrow] (aux_m_minus_1) -- (xi_m_prime);
        \draw[green_arrow] (xi_m_prime) -- (aux_m);
        \draw[green_arrow] (aux_m) -- (end_xi);

        % Red path (False)
        \draw[red_arrow] (start_xi) -- (aux_0);
        \draw[red_arrow] (aux_0) -- (xi_1_second);
        \draw[red_arrow] (xi_1_second) -- (aux_1);
        \draw[red_arrow] (aux_1) -- (xi_2_second);
        \draw[red_arrow] (xi_2_second) -- (aux_2);
        \draw[red_arrow] (aux_2) -- (dots);
        \draw[red_arrow] (dots) -- (aux_m_minus_1);
        \draw[red_arrow] (aux_m_minus_1) -- (xi_m_second);
        \draw[red_arrow] (xi_m_second) -- (aux_m);
        \draw[red_arrow] (aux_m) -- (end_xi);
    \end{tikzpicture}
    \caption{Struttura della Catena per la Variabile $x_i$}
    \label{fig:xi_chain}
    \end{figure}

    \item \textbf{Struttura Generale del Grafo:} Connettiamo le $n$ sezioni di variabili in serie. Ci sarà un nodo di inizio globale $S$ e un nodo di fine globale $T$.
    \begin{figure}[h]
    \centering
    \begin{tikzpicture}[node distance=2cm, auto, thick, scale=0.8, every node/.style={transform shape}]
        \node[node_style] (S) {$S$};
        \node[diamond_node_style] (start_x1) [right=of S] {};
        \node[small_node_style] (mid_x1) [right=1.5cm of start_x1] {}; % Placeholder for variable chain
        \node[diamond_node_style] (end_x1) [right=1.5cm of mid_x1] {};
        \node[diamond_node_style] (start_x2) [right=of end_x1] {};
        \node[small_node_style] (mid_x2) [right=1.5cm of start_x2] {}; % Placeholder for variable chain
        \node[diamond_node_style] (end_x2) [right=1.5cm of mid_x2] {};
        \node (dots) [right=of end_x2] {\dots};
        \node[diamond_node_style] (start_xn) [right=of dots] {};
        \node[small_node_style] (mid_xn) [right=1.5cm of start_xn] {}; % Placeholder for variable chain
        \node[diamond_node_style] (end_xn) [right=1.5cm of mid_xn] {};
        \node[node_style] (T) [right=of end_xn] {$T$};

        \draw[black_arrow] (S) -- (start_x1);
        \draw[black_arrow] (start_x1) -- (mid_x1); % Represents traversing the x1 chain
        \draw[black_arrow] (mid_x1) -- (end_x1); % Represents traversing the x1 chain
        \draw[black_arrow] (end_x1) -- (start_x2);
        \draw[black_arrow] (start_x2) -- (mid_x2); % Represents traversing the x2 chain
        \draw[black_arrow] (mid_x2) -- (end_x2); % Represents traversing the x2 chain
        \draw[black_arrow] (end_x2) -- (dots);
        \draw[black_arrow] (dots) -- (start_xn);
        \draw[black_arrow] (start_xn) -- (mid_xn); % Represents traversing the xn chain
        \draw[black_arrow] (mid_xn) -- (end_xn); % Represents traversing the xn chain
        \draw[black_arrow] (end_xn) -- (T);
        \draw[black_arrow] (T) .. controls +(0,2cm) and +(0,2cm) .. (S); % Arc from T to S to close the cycle

        % Indicate choices for each variable chain (simplified)
        \node[align=center] at ($(start_x1)!0.5!(mid_x1) + (0, 0.8cm)$) {Verde (True)};
        \node[align=center] at ($(start_x1)!0.5!(mid_x1) + (0, -0.8cm)$) {Rosso (False)};
    \end{tikzpicture}
    \caption{Struttura Generale del Grafo $G_\Phi$}
    \label{fig:general_graph}
    \end{figure}

    In ogni sezione di variabile (es. tra $\texttt{start\_x1}$ e $\texttt{end\_x1}$), un ciclo Hamiltoniano dovrà scegliere di percorrere o tutti gli archi "verdi" (associati a $x_i = \text{TRUE}$) o tutti gli archi "rossi" (associati a $x_i = \text{FALSE}$) per passare tutti i nodi interni di quella sezione. Questo crea $2^n$ percorsi distinti da $S$ a $T$, uno per ogni possibile assegnamento di verità.

    \item \textbf{Nodi Clausola:} Per ogni clausola $C_j$, aggiungiamo un nodo $c_j$. Questi nodi devono essere visitati da qualsiasi ciclo Hamiltoniano. I nodi clausola sono agganciati alle catene delle variabili.
    \begin{itemize}
        \item Se il letterale $x_i$ appare positivamente in $C_j$: Aggiungi archi orientati da $x_{i,j}'$ a $c_j$ e da $c_j$ a $x_{i,j}''$. Questo crea un "detour" dalla catena "verde" (True) di $x_i$.
        \item Se il letterale $\neg x_i$ appare negativamente in $C_j$: Aggiungi archi orientati da $x_{i,j}''$ a $c_j$ e da $c_j$ a $x_{i,j}'$. Questo crea un "detour" dalla catena "rossa" (False) di $x_i$.
    \end{itemize}

    \begin{figure}[h]
    \centering
    \begin{tikzpicture}[node distance=1.5cm, auto, thick, scale=0.8, every node/.style={transform shape}]
        % Common nodes for context
        \node[small_node_style] (prev_aux) at (0,0) {};
        \node[node_style] (xi_j_prime) at (1.5,0.5) {$x_{i,j}'$};
        \node[node_style] (xi_j_second) at (1.5,-0.5) {$x_{i,j}''$};
        \node[small_node_style] (next_aux) at (3,0) {};
        \node[clause_node_style] (cj_node) at (4.5,0) {$C_j$};

        % Standard path
        \draw[green_arrow] (prev_aux) -- (xi_j_prime);
        \draw[green_arrow] (xi_j_prime) -- (next_aux);
        \draw[red_arrow] (prev_aux) -- (xi_j_second);
        \draw[red_arrow] (xi_j_second) -- (next_aux);

        % Detour for positive literal xi in Cj
        \node[align=center] at (1.5, 1.5) {\small $x_i \in C_j$};
        \draw[green_arrow, dashed] (xi_j_prime) to[out=45,in=135] (cj_node);
        \draw[red_arrow, dashed] (cj_node) to[out=-135,in=-45] (xi_j_second);

        % Detour for negative literal not xi in Cj
        \node[align=center] at (1.5, -1.5) {\small $\neg x_i \in C_j$};
        \draw[red_arrow, dashed] (xi_j_second) to[out=-45,in=-135] (cj_node);
        \draw[green_arrow, dashed] (cj_node) to[out=135,in=45] (xi_j_prime);
    \end{tikzpicture}
    \caption{Agganci tra Nodi Variabile e Nodi Clausola}
    \label{fig:clause_attachment}
    \end{figure}
\end{itemize}

\textbf{Dimostrazione:}
\begin{theorem}
$\Phi$ è soddisfacibile se e solo se $G_\Phi$ ammette un ciclo Hamiltoniano.
\end{theorem}
\begin{proof}
\textbf{($\Rightarrow$) Se $\Phi$ è soddisfacibile:}
Sia $\sigma$ un assegnamento di verità che soddisfa $\Phi$. Costruiamo un ciclo Hamiltoniano $\pi$ in $G_\Phi$ come segue:
1.  Inizia da $S$.
2.  Per ogni variabile $x_i$, se $\sigma(x_i) = \text{TRUE}$, attraversa la catena della variabile $x_i$ (ovvero, da $\texttt{start\_xi} a \texttt{end\_xi}$) seguendo gli archi "verdi" (da $x_{i,j}'$ a $x_{i,j+1}'$ tramite nodi ausiliari). Se $\sigma(x_i) = \text{FALSE}$, segui gli archi "rossi" (da $x_{i,j}''$ a $x_{i,j+1}''$).
3.  Quando si arriva a un nodo $x_{i,j}'$ (se stiamo sul percorso verde) o $x_{i,j}''$ (se stiamo sul percorso rosso), controlla se la clausola $C_j$ è soddisfatta da questo assegnamento di $x_i$. Poiché $\sigma$ è un assegnamento che soddisfa $\Phi$, ogni clausola $C_j$ è soddisfatta da almeno un letterale.
    *   Se $x_i \in C_j$ e $\sigma(x_i) = \text{TRUE}$, allora mentre si passa per $x_{i,j}'$, possiamo deviare verso $c_j$ (arco da $x_{i,j}'$ a $c_j$) e poi tornare a $x_{i,j}''$ (arco da $c_j$ a $x_{i,j}''$), e poi proseguire il percorso principale.
    *   Se $\neg x_i \in C_j$ e $\sigma(x_i) = \text{FALSE}$, allora mentre si passa per $x_{i,j}''$, possiamo deviare verso $c_j$ (arco da $x_{i,j}''$ a $c_j$) e poi tornare a $x_{i,j}'$ (arco da $c_j$ a $x_{i,j}'$), e poi proseguire il percorso principale.
4.  Questo meccanismo assicura che tutti i nodi clausola $c_j$ vengano visitati (dato che ogni $C_j$ è soddisfatta da almeno un letterale $x_i$ o $\neg x_i$ il cui percorso si sta già attraversando). I nodi intermedi (pallini) e i nodi $x_{i,j}', x_{i,j}''$ vengono tutti visitati.
5.  Infine, da $T$ si torna a $S$ chiudendo il ciclo.
Questo percorso $\pi$ visita tutti i nodi esattamente una volta (inclusi $S, T$, tutti i nodi delle catene $x_i$, e tutti i nodi clausola $c_j$) e forma un ciclo. Quindi $G_\Phi$ ammette un ciclo Hamiltoniano.

\textbf{($\Leftarrow$) Se $G_\Phi$ ammette un ciclo Hamiltoniano:}
Sia $\pi$ un ciclo Hamiltoniano in $G_\Phi$. Dobbiamo dimostrare che da $\pi$ si può derivare un assegnamento di verità $\sigma$ che soddisfa $\Phi$.

\begin{lemma}
In un ciclo Hamiltoniano $\pi$ di $G_\Phi$:
\begin{itemize}
    \item Il percorso attraverso ciascuna sezione di variabile $x_i$ (da $\texttt{start\_xi} a \texttt{end\_xi}$) deve seguire esclusivamente gli archi "verdi" (da $x_{i,j}'$ a $x_{i,j+1}'$) o esclusivamente gli archi "rossi" (da $x_{i,j}''$ a $x_{i,j+1}''$). Non è possibile alternare tra percorsi verdi e rossi all'interno della stessa sezione, altrimenti si lascerebbero nodi non visitati o si violerebbe la proprietà di ciclo.
    \item Se $\pi$ devia in un nodo clausola $c_j$ da $x_{i,j}'$, deve tornare a $x_{i,j}''$. Simmetricamente, se devia da $x_{i,j}''$, deve tornare a $x_{i,j}'$. Non sono possibili "salti" a nodi di altre sezioni di variabili o altre clausole.
\end{itemize}
\end{lemma}
\begin{proof}[Dimostrazione del Lemma (Sketch)]
La prima proprietà è assicurata dalla struttura delle catene: se si alterna, si lascerebbe un "lato" della catena non visitato (es. tutti i nodi $x_{i,k}''$ se si sceglie il percorso verde e poi si cambia). Per la seconda proprietà, consideriamo il nodo $c_j$. Ha solo due archi in ingresso e due in uscita, provenienti dalle sezioni variabili. Se si entra in $c_j$ da $x_{i,j}'$, l'unica via per visitare $x_{i,j}''$ (che deve essere visitato se non lo è già) è tornare a $x_{i,j}''$ direttamente da $c_j$. Un salto verso un altro $x_{k,l}$ impedirebbe di visitare correttamente i nodi rimanenti della catena di $x_i$ o violerebbe il ciclo Hamiltoniano.
\end{proof}

Grazie al Lemma, un ciclo Hamiltoniano $\pi$ ha una struttura ben definita:
1.  Per ogni variabile $x_i$, $\pi$ o attraversa tutti i nodi $x_{i,j}'$ (usando gli archi "verdi") o tutti i nodi $x_{i,j}''$ (usando gli archi "rossi"). Questo ci permette di definire un assegnamento $\sigma$:
    *   Se $\pi$ attraversa la sezione $x_i$ sul percorso verde, poniamo $\sigma(x_i) = \text{TRUE}$.
    *   Se $\pi$ attraversa la sezione $x_i$ sul percorso rosso, poniamo $\sigma(x_i) = \text{FALSE}$.
2.  Poiché $\pi$ è un ciclo Hamiltoniano, deve visitare tutti i nodi del grafo, inclusi tutti i nodi clausola $c_j$.
3.  Per ogni nodo $c_j$ visitato, $\pi$ deve aver eseguito un detour da un nodo $x_{i,j}'$ o $x_{i,j}''$.
    *   Se il detour è da $x_{i,j}'$ (percorso verde), significa che $\sigma(x_i)=\text{TRUE}$ e $x_i \in C_j$. Questo rende la clausola $C_j$ vera.
    *   Se il detour è da $x_{i,j}''$ (percorso rosso), significa che $\sigma(x_i)=\text{FALSE}$ e $\neg x_i \in C_j$. Questo rende la clausola $C_j$ vera.
Poiché tutti i nodi $c_j$ sono visitati, tutte le clausole $C_j$ devono essere soddisfatte dall'assegnamento $\sigma$.
Quindi $\Phi$ è soddisfacibile.
\end{proof}

Questo conclude la riduzione $\text{3SAT} \le_P \text{DHC}$. La trasformazione è polinomiale perché il numero di nodi e archi nel grafo $G_\Phi$ è polinomiale rispetto a $n$ e $m$ (circa $O(nm)$ nodi e $O(nm)$ archi).

\subsubsection{Riduzione: DHC $\le_P$ HC}

Dobbiamo trasformare un grafo orientato $G=(V, E)$ in un grafo non orientato $H=(V', E')$ tale che $G$ ammette un ciclo Hamiltoniano se e solo se $H$ ammette un ciclo Hamiltoniano.

\textbf{Costruzione del Grafo $H$:}
Per ogni vertice $v \in V$ in $G$, creiamo tre vertici in $H$: $v_{in}$, $v_{mid}$, $v_{out}$.
\begin{figure}[h]
\centering
\begin{tikzpicture}[node distance=1.5cm, auto, thick, scale=0.8, every node/.style={transform shape}]
    % Original nodes in G
    \node[node_style] (a) at (0,0) {$a$};
    \node[node_style] (b) at (3,0) {$b$};

    % Corresponding nodes in H
    \node[node_style] (a_in) at (0,-2) {$a_{in}$};
    \node[node_style] (a_mid) at (1,-2) {$a_{mid}$};
    \node[node_style] (a_out) at (2,-2) {$a_{out}$};

    \node[node_style] (b_in) at (3,-2) {$b_{in}$};
    \node[node_style] (b_mid) at (4,-2) {$b_{mid}$};
    \node[node_style] (b_out) at (5,-2) {$b_{out}$};

    % Edges within triplets in H (undirected)
    \draw[thick, black] (a_in) -- (a_mid);
    \draw[thick, black] (a_mid) -- (a_out);
    \draw[thick, black] (b_in) -- (b_mid);
    \draw[thick, black] (b_mid) -- (b_out);

    % Directed edge in G
    \draw[black_arrow] (a) -- (b) node[midway, above] {$G$};

    % Corresponding edge in H (undirected)
    \draw[thick, black] (a_out) -- (b_in) node[midway, below] {$H$};

    \node at (-1,-2) {\dots};
    \node at (6,-2) {\dots};

    % Pi in G
    \node at (0,1) {$\pi$};
    \node at (1.5,1) {$v_1, v_2, v_3 \dots$};

    % Gamma in H
    \node at (0,-3) {$\gamma$};
    \node at (1.5,-3) {$v_1^{in}, v_1^{mid}, v_1^{out}$};
    \node at (4.5,-3) {$v_2^{in}, v_2^{mid}, v_2^{out}$};
    \node at (7.5,-3) {$v_3^{in}, v_3^{mid}, v_3^{out}$};

\end{tikzpicture}
\caption{Struttura della Riduzione DHC $\le_P$ HC}
\label{fig:dhc_to_hc}
\end{figure}

\begin{itemize}
    \item \textbf{Nodi in $H$:} $V' = \{v_{in}, v_{mid}, v_{out} \mid v \in V\}$. Quindi $|V'| = 3|V|$.
    \item \textbf{Archi in $H$:}
    \begin{enumerate}
        \item Per ogni $v \in V$, aggiungi archi non orientati $(v_{in}, v_{mid})$ e $(v_{mid}, v_{out})$. Questi archi "forzano" l'attraversamento in sequenza.
        \item Per ogni arco orientato $(u, v) \in E$ in $G$, aggiungi un arco non orientato $(u_{out}, v_{in})$ in $H$.
    \end{enumerate}
\end{itemize}

\textbf{Dimostrazione:}
\begin{theorem}
$G$ ammette un ciclo Hamiltoniano se e solo se $H$ ammette un ciclo Hamiltoniano.
\end{theorem}
\begin{proof}
\textbf{($\Rightarrow$) Se $G$ ammette un ciclo Hamiltoniano:}
Sia $\pi = v_1 \to v_2 \to \dots \to v_k \to v_1$ un ciclo Hamiltoniano in $G$ (dove $k=|V|$).
Costruiamo un ciclo in $H$ come segue:
$\gamma = v_{1,in} \to v_{1,mid} \to v_{1,out} \to v_{2,in} \to v_{2,mid} \to v_{2,out} \to \dots \to v_{k,in} \to v_{k,mid} \to v_{k,out} \to v_{1,in}$.
Questo percorso visita tutti i $3|V|$ nodi di $H$ esattamente una volta:
\begin{itemize}
    \item Le transizioni $v_{i,in} \to v_{i,mid} \to v_{i,out}$ sono garantite dalla costruzione degli archi all'interno di ogni tripletto.
    \item Le transizioni $v_{i,out} \to v_{i+1,in}$ (e $v_{k,out} \to v_{1,in}$ per chiudere il ciclo) sono garantite dall'esistenza degli archi $(v_i, v_{i+1})$ in $G$, che corrispondono agli archi $(v_{i,out}, v_{i+1,in})$ in $H$.
\end{itemize}
Pertanto, $\gamma$ è un ciclo Hamiltoniano in $H$.

\textbf{($\Leftarrow$) Se $H$ ammette un ciclo Hamiltoniano:}
Sia $\gamma$ un ciclo Hamiltoniano in $H$. Tutti i nodi $v_{mid}$ hanno solo due vicini: $v_{in}$ e $v_{out}$. Pertanto, qualsiasi ciclo Hamiltoniano che passi per $v_{mid}$ deve per forza percorrere gli archi $(v_{in}, v_{mid})$ e $(v_{mid}, v_{out})$ in sequenza (o viceversa). Questo implica che i nodi di ogni tripletto $(v_{in}, v_{mid}, v_{out})$ devono essere visitati consecutivamente in $\gamma$ in una delle due direzioni.
Quindi $\gamma$ deve essere della forma:
$\dots \to u_{out} \to v_{in} \to v_{mid} \to v_{out} \to w_{in} \to \dots$
Da questa sequenza, possiamo costruire un ciclo Hamiltoniano in $G$: $u \to v \to w \to \dots \to u$.
Ogni volta che $\gamma$ si sposta da $u_{out}$ a $v_{in}$, ciò implica l'esistenza dell'arco $(u_{out}, v_{in})$ in $H$, che a sua volta significa che esisteva l'arco orientato $(u, v)$ in $G$.
Poiché $\gamma$ visita tutti i nodi di $H$, ogni vertice di $G$ (rappresentato dal suo tripletto di nodi in $H$) deve essere visitato. Poiché ogni tripletto è visitato esattamente una volta, ogni vertice in $G$ è visitato esattamente una volta. Questo forma un ciclo Hamiltoniano in $G$.
\end{proof}
Questa riduzione è polinomiale poiché $|V'|=3|V|$ e il numero di archi in $H$ è $2|V| + |E|$, il che è polinomiale in $|V|$ e $|E|$.

\subsubsection{Riduzione: HC $\le_P$ TSP}

Dobbiamo trasformare un grafo non orientato $G=(V, E)$ in un'istanza $(H, K)$ di TSP.

\textbf{Costruzione dell'Istanza $(H, K)$ per TSP:}
1.  Il grafo $H$ è uguale a $G$: $H=G$. Tutti i vertici e gli archi di $G$ sono i vertici e gli archi di $H$.
2.  Assegna un peso a tutti gli archi di $H$: Per ogni arco $e \in E$, imposta il peso $\lambda(e)=1$.
3.  Imposta il valore limite $K$: $K=|V|$ (il numero di vertici in $G$).

\textbf{Dimostrazione:}
\begin{theorem}
$G$ ammette un ciclo Hamiltoniano se e solo se $H$ ammette un ciclo Hamiltoniano di peso al più $K$.
\end{theorem}
\begin{proof}
\textbf{($\Rightarrow$) Se $G$ ammette un ciclo Hamiltoniano:}
Sia $\pi$ un ciclo Hamiltoniano in $G$. Per definizione, $\pi$ visita tutti i $|V|$ vertici esattamente una volta e consiste di esattamente $|V|$ archi.
Poiché tutti gli archi in $H$ hanno peso 1, il peso del ciclo $\pi$ in $H$ è $1 \times |V| = |V|$.
Dato che $K=|V|$, il ciclo $\pi$ in $H$ ha peso esattamente $K$, che è al più $K$.
Pertanto, $H$ ammette un ciclo Hamiltoniano di peso al più $K$.

\textbf{($\Leftarrow$) Se $H$ ammette un ciclo Hamiltoniano di peso al più $K$:}
Sia $\gamma$ un ciclo Hamiltoniano in $H$ con peso $\lambda(\gamma) \le K$.
Per definizione di ciclo Hamiltoniano, $\gamma$ visita tutti i $|V|$ vertici di $H$ (e quindi di $G$) esattamente una volta e consiste di esattamente $|V|$ archi.
Il peso di $\gamma$ è la somma dei pesi di questi $|V|$ archi. Dato che tutti i pesi degli archi in $H$ sono $1$ (e sono positivi), l'unico modo per avere un peso $\le K=|V|$ per un ciclo di $|V|$ archi è che ogni arco nel ciclo abbia peso esattamente 1.
Questo significa che $\gamma$ è un ciclo Hamiltoniano in $G$ (che non ha pesi o ha pesi unitari).
\end{proof}

Questa riduzione è chiaramente polinomiale poiché la trasformazione consiste solo nell'aggiungere pesi unitari e definire $K$, operazioni che richiedono tempo polinomiale.

\subsection{Conclusione della NP-Completezza di TSP}

Avendo dimostrato che:
\begin{itemize}
    \item TSP $\in$ NP.
    \item 3SAT $\le_P$ DHC $\le_P$ HC $\le_P$ TSP.
\end{itemize}
E sapendo che 3SAT è NP-completo, per la transitività delle riduzioni polinomiali, concludiamo che TSP è NP-completo.

\begin{theorem}
$\text{TSP}$ è $\text{NP}$-completo.
\end{theorem}

Ciò rafforza l'idea che $\text{FTSP}$ è improbabile che sia in $\text{FP}$, a meno di una sorprendente risoluzione di $\text{P}$ vs $\text{NP}$.


% =====================================================
% --- START LECTURE 28 ---
% =====================================================

\chapter{Preparazione Esame}



\section{Organizzazione Esame}
Il voto massimo totale ottenibile dalla prova scritta è 32 punti (potrebbe passare a 31).
La prova è lunga, si stima una durata di 3 ore e mezza.

\subsection{Politica di Registrazione del Voto}
\begin{itemize}
    \item Se il punteggio allo scritto è $\geq 18$: è possibile registrare il voto così com'è.
    \item Se il punteggio allo scritto è $\geq 29$:
    \begin{itemize}
        \item È possibile scegliere di \textbf{non} fare l'orale e registrare 28. (Il voto massimo senza orale è 28 per garantire flessibilità a chi ottiene voti molto alti).
        \item È possibile sostenere l'orale: il voto dello scritto è perso, e il voto finale può salire o scendere. L'orale è l'unico modo per ottenere la lode.
    \end{itemize}
    \item La decisione di accettare o rifiutare il voto dello scritto deve essere comunicata entro le date dell'orale. Il silenzio assenso implica l'accettazione del voto dello scritto.
\end{itemize}

\subsection{Struttura dell'Orale}
L'orale è riservato principalmente a chi ambisce ai voti più alti ($\geq 29$).
\begin{itemize}
    \item Domande di teoria e dimostrazioni.
    \item Quesiti che collegano concetti diversi: "qual è la differenza tra X e Y?", "quale relazione c'è tra A e B?".
    \item Domande su argomenti non espressamente trattati a lezione, per valutare la capacità di ragionamento (es. Teorema di Cook, NP-completezza di problemi specifici come Hamilton Cycle o Tiling).
\end{itemize}

\section{Struttura e Tipologia di Domande della Prova Scritta}
La prova è divisa in diverse sezioni, con un totale di 32 punti.
\begin{itemize}
    \item \textbf{Macchina di Turing (Domanda 1)}: circa 8 punti. Sarà un esercizio di complessità alta, simile a quelli visti alla fine del corso (e.g., controllo del non determinismo, manipolazione di stringhe complesse).
    \item \textbf{Domande Teoriche (Domanda 2)}: 3-4 domande da 2 punti ciascuna (totale 6-8 punti). Definizione formale di concetti chiave, domande sul "perché" certi aspetti della teoria esistono.
    \item \textbf{Dimostrazioni (Domande 3 e 4)}: 1-2 dimostrazioni su concetti visti a lezione (es. proprietà di linguaggi decidibili, transitività delle riduzioni polinomiali). Circa 2-3 punti ciascuna.
    \item \textbf{Esercizi Creativi (Domanda 5 e 6)}:
    \begin{itemize}
        \item Un esercizio sull'indecidibilità (Domanda 5): dato un linguaggio, stabilirne la decidibilità/indecidibilità, eventualmente usando il Teorema di Rice o una riduzione diretta.
        \item Un esercizio sull'NP-completezza (Domanda 6): dato un nuovo problema, dimostrarne l'NP-completezza tramite riduzione da un problema NP-completo noto (la sorgente della riduzione sarà suggerita).
    \end{itemize}
\end{itemize}

\subsection{Criteri di Valutazione per le Dimostrazioni e gli Esercizi Creativi}
Sarà valutata l'abilità di argomentazione:
\begin{itemize}
    \item \textbf{Chiarezza e Coerenza Logica}: le frasi devono essere collegate, il flusso di ragionamento deve essere guidato e comprensibile per il lettore.
    \item \textbf{Formalità}: riferimento corretto a oggetti e concetti definiti.
    \item \textbf{Correttezza}: anche se l'idea è corretta, una dimostrazione mal strutturata non otterrà il massimo dei punti.
\end{itemize}

\section{Esercizi d'Esame: Esempio e Soluzioni}

\subsection{Domanda 1: Macchina di Turing}
Si definisca una Macchina di Turing (eventualmente non-deterministica e/o multi-nastro) per il seguente linguaggio $L$:
\[ L = \{ A \# B \# W_1 \# \dots \# W_N \mid A, B, W_i \in \{a,b,c,d\}^+, N = |A| + |B|, \forall i \in [1, N]: W_i = R_i S_i R_i^R S_i, |R_i| \ge 1, |S_i| \ge 1 \} \]
\textbf{Esempio dato:} $A = \text{\texttt{bc}}$, $B = \text{\texttt{c}}$. $|A|=2$, $|B|=1 \Rightarrow N = 3$.
Quindi la stringa sarà della forma \texttt{bc\#c\#W1\#W2\#W3}.
Un esempio di $W_i$ è \texttt{ab c ba c}, dove $R_i=\text{\texttt{ab}}$, $S_i=\text{\texttt{c}}$. Allora $R_i^R = \text{\texttt{ba}}$. La struttura è $R_i S_i R_i^R S_i$.

\textit{Nota}: Per la soluzione, qualsiasi approccio funzionante è accettato. È preferito un approccio non-deterministico se applicabile, per dimostrare la comprensione del non determinismo. Si richiede il grafo di transizione della macchina e un breve commento sul funzionamento (e.g., utilizzo dei nastri).

\subsection{Domanda 2: Domande Teoriche}
\begin{itemize}
    \item \textbf{A. Perché ci focalizziamo sullo studio dei problemi di decisione?}
    \textbf{Risposta:} I problemi di decisione sono concettualmente più semplici da affrontare e da studiare formalmente. Un problema di ricerca o di ottimizzazione può spesso essere ricondotto a una serie di problemi di decisione.

    \item \textbf{B. Definizione formale di Big-O notation.}
    \textbf{Risposta:} Una funzione $f(n)$ è $O(g(n))$ se esistono costanti positive $c$ e $n_0$ tali che $0 \le f(n) \le c \cdot g(n)$ per ogni $n \ge n_0$.

    \item \textbf{C. Cos'è un trasduttore?}
    \textbf{Risposta:} Un trasduttore è una macchina di Turing con almeno tre nastri: un nastro di input, un nastro di lavoro (work tape) e un nastro di output. Un trasduttore calcola una funzione $f$ se, data una stringa $w$ sul nastro di input, al termine della sua esecuzione, il nastro di output contiene $f(w)$.

    \item \textbf{D. Dimostrare che se un linguaggio $L$ appartiene a R, allora il suo complemento $\bar{L}$ appartiene anch'esso a R.}
    \textbf{Risposta:} Se $L \in R$, esiste una MT $M$ che decide $L$. $M$ si ferma e accetta per $w \in L$ e si ferma e rifiuta per $w \notin L$. Per costruire una MT $M'$ che decide $\bar{L}$, basta scambiare gli stati di accettazione e rifiuto di $M$. Se $M$ accetta $w$, $M'$ rifiuta $w$. Se $M$ rifiuta $w$, $M'$ accetta $w$. Poiché $M$ si ferma sempre, anche $M'$ si fermerà sempre. Quindi $M'$ decide $\bar{L}$, e $\bar{L} \in R$.
\end{itemize}

\subsection{Domanda 5: Decidibilità/Indecidibilità}
\subsubsection{Problema}
Discutere la decidibilità del seguente linguaggio: $L_{cambio}$ è l'insieme dei codici di Macchine di Turing $M$ tali per cui $M$, quando computa su qualsiasi input, cambia stato ad ogni passo.
\begin{itemize}
    \item \textbf{Passo 1: È una proprietà semantica?}
    Una proprietà è semantica se, date due macchine $M_1$ e $M_2$ che riconoscono lo stesso linguaggio ($L(M_1) = L(M_2)$), l'appartenenza di $M_1$ alla proprietà implica l'appartenenza di $M_2$ alla proprietà.
    Per dimostrare che $L_{cambio}$ \textbf{non} è una proprietà semantica, forniamo un controesempio:
    \begin{example}[Controesempio per proprietà semantica]
        Siano $M_1$ e $M_2$ due macchine di Turing.
        \begin{itemize}
            \item \textbf{Macchina $M_1$}:
            Parte dallo stato $q_0$. Legge il primo simbolo $\alpha$, lo riscrive $\alpha$, mantiene la testina ferma (o si muove a destra/sinistra), e passa allo stato $q_1$. Poi si ferma.
            Questa macchina fa un solo passo e cambia stato ($q_0 \to q_1$). Il suo linguaggio è $\emptyset$ (non accetta nulla). $M_1 \in L_{cambio}$ (perché in ogni passo che fa, cambia stato).
            \item \textbf{Macchina $M_2$}:
            Parte dallo stato $q_0$. Legge il primo simbolo $\alpha$, lo riscrive $\alpha$, mantiene la testina ferma, e rimane nello stato $q_0$. Non accetta mai.
            Questa macchina non cambia stato (rimane sempre in $q_0$). Il suo linguaggio è $\emptyset$ (non accetta nulla). $M_2 \notin L_{cambio}$.
        \end{itemize}
        Poiché $L(M_1) = L(M_2) = \emptyset$, ma $M_1 \in L_{cambio}$ e $M_2 \notin L_{cambio}$, la proprietà $L_{cambio}$ non è semantica. Di conseguenza, il Teorema di Rice \textbf{non può} essere applicato per determinarne la decidibilità.
    \end{example}

    \item \textbf{Passo 2: Collocazione in R/co-R}
    \begin{itemize}
        \item \textbf{È $L_{cambio} \in R$?}
        Per $L_{cambio} \in R$, dovrebbe esistere una MT che per ogni input $M$ decide se $M$ cambia stato ad ogni passo su \textbf{tutti} gli input. Questo è difficile, perché simulare $M$ su tutti gli input è impossibile.

        \item \textbf{È $L_{cambio} \in co-R$?}
        Questo significa che il complemento $\overline{L_{cambio}}$ (macchine che \textbf{non} cambiano stato ad ogni passo, cioè esiste almeno un passo in cui rimangono nello stesso stato o almeno un input su cui si bloccano in uno stato) è in $R$.
        Per $\overline{L_{cambio}}$, possiamo costruire una macchina che risponde "sì" in tempo finito. Dato un codice $M$:
        \begin{itemize}
            \item Iteriamo su tutte le possibili configurazioni raggiungibili da $M$ (o simuliamo $M$ su un insieme di input).
            \item Per ogni configurazione, verifichiamo la transizione. Se in un qualsiasi momento $M$ transisce da uno stato $q$ a $q$ (cioè $Q_{next} = Q_{current}$), allora possiamo dire "sì, questa macchina non cambia stato ad ogni passo".
            \item Questo implica che $\overline{L_{cambio}} \in R$, quindi $L_{cambio} \in co-R$.
        \end{itemize}
        \textit{Osservazione}: Una semplice analisi sintattica della funzione di transizione (cercare loop $q \to q$) non è sufficiente, perché potrebbero esserci loop che la macchina non raggiunge mai, o transizioni che implicano un cambio di stato ma finiscono per essere bloccate in uno stato che non avanza.

    \end{itemize}

    \item \textbf{Passo 3: Dimostrazione di indecidibilità (riduzione)}
    Per dimostrare che $L_{cambio} \notin R$, riduciamo un problema noto indecidibile a $L_{cambio}$. Useremo il linguaggio $L_u = \{ \langle M, w \rangle \mid M \text{ accetta } w \}$, che è noto essere indecidibile.
    Costruiamo una riduzione $f$ tale che $\langle M, w \rangle \in L_u \iff f(\langle M, w \rangle) \in L_{cambio}$.
    La funzione $f$ prende $\langle M, w \rangle$ e costruisce una nuova macchina $N = f(\langle M, w \rangle)$ nel modo seguente:
    \begin{itemize}
        \item $N$ ignora il proprio input.
        \item $N$ scrive $w$ su un nastro di lavoro.
        \item $N$ simula la macchina $M$ su $w$ usando una versione modificata di $M$, che chiamiamo $M'$.
    \end{itemize}
    La macchina $M'$ è costruita da $M$ in modo che:
    \begin{itemize}
        \item Tutte le transizioni di $M$ che portano ad un loop su se stesso (es. $q \xrightarrow{\alpha \to \beta, D} q$) vengono modificate. Invece di rimanere nello stato $q$, $M'$ transisce a un nuovo stato $q'$ e da $q'$ transisce a $q$ (creando un'oscillazione $q \leftrightarrow q'$). Questo garantisce che $M'$ cambi sempre stato, simulando comunque il comportamento di $M$.
        \item Se $M$ accetta $w$, $M'$ entra in un ciclo infinito di oscillazione tra due stati distinti (es. $q_{acc} \leftrightarrow q'_{acc}$), garantendo un cambio di stato continuo.
        \item Se $M$ rifiuta $w$ o entra in un loop infinito (non accettando), $M'$ entra in un unico stato di "blocco" (es. $q_{loop}$), dove rimane indefinitamente senza cambiare stato.
    \end{itemize}

    \textbf{Analisi della riduzione:}
    \begin{itemize}
        \item Se $\langle M, w \rangle \in L_u$ (cioè $M$ accetta $w$):
        $N$ (ignorando il suo input) simulerà $M'$ su $w$. Poiché $M'$ è progettata per oscillare continuamente se $M$ accetta, $N$ cambierà sempre stato durante la sua computazione su qualsiasi input. Quindi $N \in L_{cambio}$.

        \item Se $\langle M, w \rangle \notin L_u$ (cioè $M$ non accetta $w$):
        $N$ (ignorando il suo input) simulerà $M'$ su $w$. Poiché $M'$ è progettata per bloccarsi in un singolo stato se $M$ non accetta, $N$ a un certo punto non cambierà più stato durante la sua computazione. Quindi $N \notin L_{cambio}$.
    \end{itemize}
    La riduzione è polinomiale in tempo. Dato che $L_u$ è indecidibile, e abbiamo una riduzione da $L_u$ a $L_{cambio}$, allora $L_{cambio}$ è indecidibile.
    In conclusione, $L_{cambio}$ è un linguaggio indecidibile e appartiene a $co-R$ (ma non a $R$).
\end{itemize}

\subsection{Domanda 6: NP-Completezza}
\subsubsection{Problema: Transazioni}
In un sistema di basi di dati, si vuole gestire l'esecuzione di transazioni concorrenti. Si vuole accettare l'esecuzione del maggior numero possibile di transazioni, avendo cura di non accettare contemporaneamente due transazioni che chiedono di scrivere sullo stesso dato.

\begin{definition}[Istanza di Transazioni]
Un'istanza $I_{TD}$ è una quadrupla $(T, D, A, K)$, dove:
\begin{itemize}
    \item $T = \{t_1, \dots, t_m\}$ è l'insieme delle transazioni.
    \item $D = \{d_1, \dots, d_p\}$ è l'insieme dei dati.
    \item $A$ è un insieme di triple $(t_x, d_y, \alpha)$, dove $\alpha \in \{R, W\}$, indicante che la transazione $t_x$ accede al dato $d_y$ in lettura (R) o in scrittura (W).
    \item $K$ è un intero positivo.
\end{itemize}
Un'istanza $I_{TD}$ è \textbf{SÌ} se e solo se è possibile accettare un insieme $X \subseteq T$ di transazioni tale che:
\begin{itemize}
    \item $|X| \ge K$.
    \item Per ogni coppia di transazioni $t_i, t_j \in X$ ($t_i \neq t_j$), se $t_i$ e $t_j$ accedono allo stesso dato $d_k$, almeno una delle due non accede in scrittura (W). Formalmente: non esiste $d_k \in D$ tale che $(t_i, d_k, W) \in A$ e $(t_j, d_k, W) \in A$.
\end{itemize}
\end{definition}

\subsubsection{Discutere la complessità del problema Transazioni}
\textbf{Suggerimento:} Riduzione da Independent Set.

\textbf{Passo 1: Membership in NP}
Per dimostrare che il problema delle Transazioni è in NP, dobbiamo mostrare che esiste una macchina non-deterministica che può verificarne una soluzione in tempo polinomiale.
\begin{enumerate}
    \item \textbf{Guess (Fase non-deterministica):} La macchina non-deterministica "indovina" un sottoinsieme $X \subseteq T$ di transazioni. Questo guess è polinomiale poiché la cardinalità di $T$ è finita e il numero di transazioni nel sottoinsieme non può superare $|T|$.
    \item \textbf{Check (Fase deterministica polinomiale):} La macchina verifica i due vincoli:
    \begin{itemize}
        \item \textbf{Cardinalità:} Controlla se $|X| \ge K$. Questo è un controllo in tempo polinomiale ($O(|X|)$).
        \item \textbf{Conflitti di Scrittura:} Per ogni coppia di transazioni $(t_i, t_j)$ in $X$:
        \begin{itemize}
            \item Si scorre l'insieme $A$ delle richieste.
            \item Per ogni dato $d_k$, si verifica se sia $t_i$ che $t_j$ hanno una richiesta di scrittura su $d_k$.
        \end{itemize}
        Questo controllo può essere fatto efficientemente. Per esempio, si possono creare liste di accesso per ogni dato: per ogni $d_k$, una lista di transazioni che scrivono su $d_k$. Poi si itera su ogni $d_k$. Se una lista contiene più di una transazione presente in $X$, e tutte queste transazioni scrivono su $d_k$, allora c'è un conflitto. Questo check è polinomiale, $O(|X|^2 \cdot |D|)$ o $O(|D| \cdot |T|^2)$ a seconda dell'implementazione.
    \end{itemize}
    Poiché sia la fase di guess che quella di check sono polinomiali, il problema delle Transazioni appartiene a NP.
\end{enumerate}

\textbf{Passo 2: NP-Hardness (Riduzione da Independent Set)}
Dobbiamo mostrare una riduzione polinomiale da Independent Set (IS) al problema delle Transazioni.
\begin{definition}[Independent Set (IS)]
Un'istanza di IS è una coppia $(G, k)$, dove $G=(V, E)$ è un grafo non orientato e $k$ è un intero. L'istanza è \textbf{SÌ} se esiste un sottoinsieme $V' \subseteq V$ di nodi tale che $|V'| \ge k$ e nessun paio di nodi in $V'$ sono collegati da un arco (cioè, $V'$ è un insieme indipendente).
\end{definition}

\textbf{Costruzione della Riduzione $f: (G, k) \to (T, D, A, L)$:}
Data un'istanza $(G, k)$ del problema Independent Set, costruiamo un'istanza $(T, D, A, L)$ per il problema delle Transazioni come segue:
\begin{enumerate}
    \item \textbf{Insieme delle Transazioni $T$:}
    Per ogni nodo $v_i \in V$ del grafo $G$, creiamo una transazione $t_i \in T$.
    Quindi, $T = \{ t_i \mid v_i \in V \}$. In questo modo, ogni transazione $t_i$ corrisponde univocamente a un nodo $v_i$.

    \item \textbf{Insieme dei Dati $D$:}
    Per ogni arco $e_j \in E$ del grafo $G$, creiamo un dato $d_j \in D$.
    Quindi, $D = \{ d_j \mid e_j \in E \}$. In questo modo, ogni dato $d_j$ corrisponde univocamente a un arco $e_j$.

    \item \textbf{Insieme delle Richieste $A$:}
    Per ogni arco $e_j = (v_a, v_b) \in E$ in $G$:
    \begin{itemize}
        \item Aggiungiamo la tripla $(t_a, d_j, W)$ all'insieme $A$, indicando che la transazione $t_a$ (corrispondente al nodo $v_a$) vuole scrivere sul dato $d_j$ (corrispondente all'arco $e_j$).
        \item Aggiungiamo la tripla $(t_b, d_j, W)$ all'insieme $A$, indicando che la transazione $t_b$ (corrispondente al nodo $v_b$) vuole scrivere sul dato $d_j$ (corrispondente all'arco $e_j$).
    \end{itemize}
    Non ci sono richieste di lettura o altre richieste di scrittura. L'obiettivo è che le transazioni corrispondenti a nodi collegati da un arco entrino in conflitto.

    \item \textbf{Valore $L$:}
    Impostiamo $L = k$. Questo valore rappresenta la cardinalità minima richiesta per l'insieme indipendente, che ora si traduce nella cardinalità minima per l'insieme di transazioni accettate.
\end{enumerate}

\textbf{Correttezza della Riduzione:}
Dobbiamo dimostrare che $(G, k)$ è un'istanza SÌ di IS se e solo se $(T, D, A, L)$ è un'istanza SÌ di Transazioni.

\textbf{Parte 1: Se $(G, k)$ è SÌ $\implies$ $(T, D, A, L)$ è SÌ}
Supponiamo che $(G, k)$ sia un'istanza SÌ di IS. Questo significa che esiste un insieme indipendente $V' \subseteq V$ tale che $|V'| \ge k$ e per ogni $v_a, v_b \in V'$ ($v_a \neq v_b$), $(v_a, v_b) \notin E$.
Costruiamo un insieme di transazioni $X \subseteq T$ come segue: $X = \{ t_i \mid v_i \in V' \}$.
\begin{itemize}
    \item \textbf{Cardinalità:} Per costruzione, $|X| = |V'| \ge k$. Quindi il primo vincolo è soddisfatto.
    \item \textbf{Conflitti di Scrittura:} Supponiamo per assurdo che esistano due transazioni $t_a, t_b \in X$ ($t_a \neq t_b$) che accedono in scrittura allo stesso dato $d_j$. Per costruzione della riduzione, se $t_a$ e $t_b$ scrivono su $d_j$, allora $d_j$ deve corrispondere a un arco $e_j = (v_a, v_b) \in E$ che collega i nodi $v_a$ e $v_b$ (corrispondenti a $t_a$ e $t_b$). Ma $v_a, v_b \in V'$, e $V'$ è un insieme indipendente, il che implica che $(v_a, v_b) \notin E$. Questo è una contraddizione.
    Pertanto, non esistono conflitti di scrittura tra le transazioni in $X$.
\end{itemize}
Dato che entrambi i vincoli sono soddisfatti, $(T, D, A, L)$ è un'istanza SÌ del problema delle Transazioni.

\textbf{Parte 2: Se $(T, D, A, L)$ è SÌ $\implies$ $(G, k)$ è SÌ}
Supponiamo che $(T, D, A, L)$ sia un'istanza SÌ di Transazioni. Questo significa che esiste un insieme $X \subseteq T$ di transazioni tale che $|X| \ge L$ (e $L=k$) e per ogni $t_a, t_b \in X$ ($t_a \neq t_b$), non accedono in scrittura allo stesso dato.
Costruiamo un sottoinsieme di nodi $V' \subseteq V$ come segue: $V' = \{ v_i \mid t_i \in X \}$.
\begin{itemize}
    \item \textbf{Cardinalità:} Per costruzione, $|V'| = |X| \ge L = k$. Quindi il primo vincolo è soddisfatto.
    \item \textbf{Assenza di Archi:} Supponiamo per assurdo che esistano due nodi $v_a, v_b \in V'$ ($v_a \neq v_b$) collegati da un arco $(v_a, v_b) \in E$. Per costruzione della riduzione, se $(v_a, v_b) \in E$, allora esiste un dato $d_j$ corrispondente a questo arco, e l'insieme $A$ contiene le richieste $(t_a, d_j, W)$ e $(t_b, d_j, W)$. Ma $t_a, t_b \in X$, e per ipotesi $X$ è un insieme di transazioni senza conflitti di scrittura. Questo è una contraddizione, poiché $t_a$ e $t_b$ entrerebbero in conflitto su $d_j$.
    Pertanto, nessun paio di nodi in $V'$ è collegato da un arco, e $V'$ è un insieme indipendente.
\end{itemize}
Dato che entrambi i vincoli sono soddisfatti, $(G, k)$ è un'istanza SÌ del problema Independent Set.

\textbf{Complessità della Riduzione:}
La costruzione di $T$, $D$, $A$ è polinomiale. $|T| = |V|$, $|D| = |E|$. La costruzione di $A$ richiede di iterare sugli archi di $G$, e per ogni arco si aggiungono due triple. Questo può essere fatto in tempo $O(|V| + |E|)$, che è polinomiale rispetto alla dimensione dell'input $(G, k)$.

\textbf{Conclusione:}
Poiché il problema Independent Set è NP-completo, e abbiamo dimostrato una riduzione polinomiale da Independent Set al problema delle Transazioni, allora il problema delle Transazioni è NP-Hard. Dato che è anche in NP, il problema delle Transazioni è \textbf{NP-Completo}.

\subsubsection{Complessità del Problema di Ottimizzazione}
Qual è la complessità di calcolare il numero più grande di transazioni che si possono eseguire contemporaneamente?
Questo è un problema di ottimizzazione. Per risolverlo, possiamo utilizzare un oracolo per il problema decisionale NP-completo (Transazioni).

\textbf{Algoritmo di Ricerca Binaria con Oracolo NP:}
Sia $N_{max}$ la dimensione massima possibile dell'insieme di transazioni che possiamo accettare (che è al massimo $|T|$).
\begin{enumerate}
    \item Inizializziamo un intervallo di ricerca: $min = 0$, $max = |T|$.
    \item Finché $min \le max$:
    \begin{itemize}
        \item Calcoliamo $mid = (min + max) / 2$.
        \item Chiediamo all'oracolo per Transazioni se è possibile accettare $mid$ transazioni (cioè, l'istanza $(T, D, A, mid)$ è SÌ?).
        \item Se l'oracolo risponde SÌ: significa che possiamo accettare almeno $mid$ transazioni. Tentiamo valori più grandi: $best\_count = mid$, $min = mid + 1$.
        \item Se l'oracolo risponde NO: significa che non possiamo accettare $mid$ transazioni. Dobbiamo cercare valori più piccoli: $max = mid - 1$.
    \end{itemize}
    \item Il valore finale di $best\_count$ sarà il numero più grande di transazioni che si possono eseguire contemporaneamente.
\end{enumerate}
Il numero di chiamate all'oracolo è $O(\log |T|)$. Poiché ogni chiamata all'oracolo risolve un problema in NP, la classe di complessità di questo problema di ottimizzazione è $FP^{NP}$, che denota le funzioni calcolabili in tempo polinomiale con accesso a un oracolo NP. Più precisamente, è $FP_{log n}^{NP}$ o $FP_{log|T|}^{NP}$.

\end{document}
